
\documentclass[a4paper,11pt,twoside]{article}
\include{formatAndDefs_FR}

\begin{document}
%\chapter*{Résumé}
\section{Introduction}
\subsection{Contexte}


Donner du sens aux textes joue un rôle essentiel dans l'évolution de l'intelligence artificielle générale. Compte tenu de la génération croissante de données textuelles, il existe un besoin de systèmes de calcul capables d'extraire des informations utiles à partir de grandes quantités de collections textuelles pour faciliter nos activités quotidiennes de trouver des informations latentes utiles cachées derrière ces grandes quantités de données.
%
En effet, faire l'apprentissage des ordinateurs est l'objectif général de la recherche sur l'intelligence artificielle \cite{Sugiyama2015}. Issu de ce domaine multidisciplinaire, le traitement automatique du langage naturel (TALN) est le domaine qui vise à faire comprendre aux machines notre langage \cite{JurafskyM09}.



Les solutions aux tâches TALN suivent généralement trois étapes pour atteindre leurs objectifs respectifs \cite{JurafskyM09,mining12Book}. Ces étapes sont: premièrement, en pré-traitement, un corpus d'entrée est "normalisé" de sorte qu'il sera plus facile à traiter par la machine dans les étapes suivantes. Deuxièmement, dans la représentation des caractéristiques, de nombreuses caractéristiques sont extraites du texte prétraité. Troisièmement, dans {Knowledge Discovery}, une technique d'apprentissage automatique est utilisée pour apprendre un modèle capable de fournir un aperçu intéressant au sein des données existantes ainsi que sur de nouvelles instances futures. La sortie de ce système est généralement le modèle ou la connaissance du langage qui révèle une information intéressante contenue dans le corpus d'entrée.

\subsection{Problématiques et contributions}

Il y a plusieurs défis de recherche qui découlent des choix effectués dans chacune des étapes composant le flux du système PNL. Dans cette thèse, nous nous concentrons sur trois défis qui se posent à la fois dans les phases de représentation des caractéristiques et de découverte des connaissances. Ces défis sont: (1) la modélisation, l'extraction et le stockage de différents types d'éléments linguistiques à partir de textes bruts, (2) le traitement de la parcimonie inhérente aux données textuelles et leur combinaison pour obtenir de meilleures représentations; entre les mots et ensuite les exploiter afin de découvrir leur parenté latente et être capable de résoudre les tâches de la PNL.


Afin de répondre à ces défis, nous proposons trois contributions:
\begin{itemize}
\item un modèle de réseau basé sur hypergraphs pour contenir des données linguistiques hétérogènes
\item une méthode pour combiner des représentations hétérogènes provenant du
modèle hypergraphique, tout en atténuant le problème de parcimonie, commun en traitant des fonctions de texte.
\item un algorithme basé sur le réseau ci-dessus pour découvrir la relation sémantique entre les mots liés.
\end{itemize}


Ces contributions sont evaluèes et validèes en utilisant deux deux taches semantiqiues du TALN~: l'induction et disambiguisation de mots (Word Sense Induction and Disambiguation ou WSDI/WSD) et la reconnaisance d'entitées nommées (ou Named Entity Recongiontion NER). Nous attaquons ces tâches etant donée qu'elles sont des pièces centrales lors de la construction de systemes TALN plus avancés.

\section{Antecedents}

\subsection{Hypothèse Dsitributionelle}
Le travail nous presentons dans cette thèse est basè principalement sur l'hypothèse distributionelle (HD). C'est aussi le cas pour la majorité des approches sémantiques actuelles dans le domain du TALN. Cet aperçu de l'analyse du contexte est credité primordialement à \cite{harris1954}. L'hypothese est simple mais puissante~: elle affirme que la similarité de la significance des mots est correlée avec la similarite de leur contextes. Consequenment, le significat d'un mot peut etre determiné par l'ensemble de contextes dans lequels ce mot participe.
%
Dans ce travail nous nous focalisons exclusivement sur deux contextes~: la cooccurrence lexicale et la coocurrence syntaxique des contextes. L'occurrence lexicale est basée sur ces mots qui cooccurent avec un autre mot dasn un voisinage predefini. Les cooccurrences syntaxiques sont basees sur une analyse (ou parsing) plus profonde du texte afin d'obtenir 


%
In this work we will exclusively focus on those two contexts: lexical co-occurrence and syntactic co-occurrence contexts. Lexical co-occurrences consist on those  words that co-occur with a given word in a predetermined neighborhood. Syntactic contexts are based on the analysis (or parsing) of text in order to obtain sense from them.

\subsection{Representation Models}
 The Vector Space Model (VSM) consists in representing textual units in a multi-dimensional space. The textual units represented are not constrained to words themselves. We may describe co-occurrent features for documents, phrases, paragraphs, or other types \cite{manning1999foundations}. A matrix is used as the structure that holds each object and its context features. Distance metrics (or similarity measures) are used over the vectors of the matrix to determine a level of dissimilarity or similarity between these objects.
 Other type of representation models, based on graph structures are commonly used in the literature. Indeed, network based models have been studied deeply during the last years in the TALN field  \cite{Mihalcea2011}. While we can represent a graph as a matrix, and thus as a vector space model, graphs are useful representation formalism that can be applied to a large set of linguistic characteristics, from the relation between words in a text or between the features that describe them. Indeed, language being a dynamic complex system, networks provide an adequate model to represent and study the structure and evolution of linguistic systems \cite{Choudhury2009}.
In this thesis we base our linguistic model proposition on a graph-based structure.

\subsection{Data Sparsity}
Without regards to their type, network-based structures are ultimately transformed into matrices before being treated computationally. Therefore, given that we are still modeling language (words), graphs suffer from sparsity just as vector space models. Indeed, data sparsity is an issue that affects the performance of knowledge discovery approaches \cite{mining12Book,PerinetH15} applied to textual data.

 A sparse data matrix has most of its entries equal to zero. Thus, the majority of the words (rows) in the corpus are described by very few contexts (columns). This is a significant problem as on the knowledge discovery phase of any TALN system we aim to train a learning model that will eventually predict, classify, group our words in one way ot another. If the words are represented by a limited number of contexts, the learning algorithms will not be able to generalize properly. 
%\subsection{Conclusion}

Whether its vector based or graph-based, a textual, explicit, and distributional representation will be sparse. There are too many words in a text and its assured that, while they could occur in other texts, they will not occur in a single text. This becomes an important problem with TALN systems: words are described by only a small set of features. In the following describe our two first propositions which address the issue of using heterogeneous information to represent a term and alleviating the data sparsity that comes with such types of textual representations.


\section{Modèle linguistique basé sur des hypergraphs enrichi par la fusion}
The first two contributions of this thesis are contained in a fusion enriched hypergraph linguistic model proposition. The model  consists on two components which address two research questions each: the issue of making sparsity less severe and leveraging different types of features  by using a single feature representation space. 

The model we present here entails three  important characteristics: firstly, the possibility to leverage different types of information.  Secondly, as the words will be linked together, there is an inner structure that will emerge from the model and which we exploit in our experiments. Thirdly, given that we treat unstructured text data, the relations (or features)  between words are sparse, this is alleviated by combining features via fusion techniques.  The three of them are addressed with our propositions. 

Our network is based on the distributional hypothesis, as described in the previous chapter.  As co-occurrence features, we select both lexical and syntactic contexts, indeed creating a linguistic resource that hold both types of information in order to get a complementary insight of words' relations. 


In the literature, regarding WSD approaches, we see that the use of a lexical knowledge base, such as Wordnet\footnote{\url{https://wordnet.princeton.edu}}, is pervasive in this task. On the other hand, WSI, while being a more flexible approach (language and word-usage independent, does not require human-made bases)  for solving WSD, its results are tightly linked to the quality of the clustering algorithm used. 
% 
%We refer to  linguistic-network modelization as the type of linguistic information and the means in which it is stored within a language network.

 With respect to the networks' modelization, we find that few approaches deal with syntactic attributes. We believe that finding semantic similarities can be improved by adding syntactic information not only  while using dependency relations but also by leveraging the constituency tree of each word. Moreover, using syntactic data along with semantic and/or lexical co-occurrences takes us into the heterogeneous network domain which has not been addressed in most of the approaches covered.  


Taking into account the described opportunities of research, in the following section we propose a  hypergraph modelization of a linguistic network that aims to solve some limitations stated above. 



\subsection{Proposed Model: Fusion Enriched Hypergraph Linguistic Network}

As stated before, our model consists on two parts (and two contributions). The first one, an hypergraph model that holds different types of linguistic relations extracted from a corpus. And the second one, the combination of linguistic features in order to generate a less sparse, enriched representation. 

Our model is  based on the use of a hypergraph. Its single most important difference with regular graphs, is being able to relate more than two vertices at the same type, which allows for a better characterization of interactions within a set of individual elements (in our case, words) \cite{heintz2014beyond}. Indeed, our hypergraph modelization initially integrates four types of relations between tokens: sentence co-occurrence, part-of-speech tags, words' constituents data and dependency relations in a single linguistic structure. These relationships were chosen because it is relatively easy to obtain them for high-resource languages. These features can be seen as building blocks for TALN models. In any case, our goal is to arrive to more complex annotations (e.g., named entities) from the selected features and relations. We decided to keep a lexical context at sentence level, so that it may complement the distributional semantic information provided by the dependency functions context as well as the phrase-constituency syntactic context. In short, we  aim to cover three levels of possible semantic relatedness via  three levels (in terms of the size of the neighborhood of a target word) of distributional co-occurrences: a short range with dependency functions, a medium range with phrase constituency membership, and a longer range with sentence lexical co-occurrences. The intuition is that when solving TALN tasks, having direct access to these three semantic spaces will help to determine a more appropriate meaning's relation between words. 

The second part of our proposed method deals with the fusion of textual features. Namely, we combine the features that describe terms into a single representation space. This new space aims to address two issues that arise while working with textual data: effectively using information coming from different linguistic levels (e.g., lexical, syntactic, semantic) while alleviating the sparsity typical of textual representations. In the multimodal fusion literature we can discern two main common types of techniques: early fusion and late fusion. A third and fourth type of fusion methods, cross-media fusion and hybrid fusion are also employed in multimedia analysis tasks. 

These four fusion operators naturally address the issue of dealing with heterogeneous data as they all mix one way or another the feature columns from each of two representations. Regarding alleviating sparsity, the intuition is that by combining matrices either by summing or element-wise multiplying them, the resulting matrix will have a denser structure. For example, by summing two matrices with the same shape, such as two term-term similarity matrices, we  obtain a resulting matrix that contains the similarities of both feature spaces. In the same sense, when multiplying two matrices we combine them while also obtaining a denser output matrix. Nonetheless, both sum and multiplication result depends evidently on the nature of the matrices employed. 
%\subsection{Proof of Concept: Wikipedia-based Corpus as an Enriched Hypergraph}

In order to materialize our proposed linguistic model we implemented a procedure that takes a corpus as input and outputs the linguistic resource we introduced in the previous section. We based our process on the online encyclopedia Wikipedia\footnote{\url{https://en.wikipedia.org}} which has been used as a source of valuable data as well as a common background corpus to solve diverse TALN/TM related tasks. 

%chap 4
\section{Applications à la reconnaissance d'entités nommées et à la désambiguïsation du sens du mot}
\subsection{Introduction}
In this application's chapter we set to solve two natural language processing tasks using as data source corpora in the form of our proposed model. We address the tasks of Named Entity Recognition (NER) and Word Sense Induction and Disambiguation (WSI/WSD). We employ both a fusion enriched and a raw hypergraph network based on benchmark corpora to validate the utility of our proposals.
\subsection{First Application: Named Entity Recognition and Word Sense Disambiguation}
 NER goal is to automatically discover, within a text, mentions that belong to a well-defined semantic category. The classic task of NER involves detecting, within a text, entities of type Location (LOC), Organization (ORG), Person (PER), Miscellaneous (MISC), or if the term is not an even an entity, assigning them a (O) label. The task is of great importance for more complex TALN systems, e.g, relation extraction, opinion mining \cite{nadeau2007survey}.  Generally, the common solution to NER involves training a supervised machine learning algorithm with large quantities of annotated text \cite{mining12Book}. As is usual with other TALN tasks, NER  requires textual features to represent words in order to determine their role within a phrase. We propose to build representations based on our fusion enriched hypergraph model. For the main experiments, we chose a structured perceptron learning algorithm because of its performance and its lower training time. 

We experiment with the four levels of fusion on three different datasets. The representation matrices for NER come from lexical context features $\mlex$, syntactical context features or other task-standard features.  On the other hand, experiments on WSI/WSD exclusively employ lexical and syntactic matrices. Our main goal is to compare the efficiency of the primary  fusion techniques applied to   named entity recognition. Then, we empirically determine a fusion combination operator able to leverage the complementarity of the features used.
We discover that using a recombination of several fusion operations, a so-called hybrid approach, we  improve over using the features individually and also over the use of the trivial early fusion operator. This indicates that the single feature matrix, enriched with other combined features, is enough to improve the results of said baselines. In general, in our experiments with NER, we see that the added enriched features are not the most important for the learning algorithm while taking a decision, nonetheless, they  provide the extra information needed to push the model towards the correct prediction, by enriching the features through cross and late fusion and by providing more descriptors for each word and consequently reducing the sparsity of the representation matrices.

Once we found a set of fusion operations that work reasonably well with NER, we experiment with another task, word sense induction and disambiguation, to confirm the usefulness of using fusion enriched representations to train better models.
\subsection{Second Application: Word Sense Induction and Disambiguation}
\subsubsection{Fusion Enriched Representations}
Having learned the best fusion configuration from the NER task, in these experiments we set to test if the improvements achieved can be transfered into another TALN task, namely Word Sensed Induction and Disambiguation (WSI/WSD). As preprocessing, we simply remove stopwords and tokens with less than three letters. The features we extracted from the tested corpora with the same tools as in the previous task.
Word Sense Induction and Disambiguation entails two closely related tasks. WSI aims to automatically discover the set of possible senses for a target word given a text corpus containing several occurrences of said target word. Meanwhile, WSD takes a set of possible senses and determines the most appropriate sense for each instance of the target word according to the instance's context. WSI is usually approached as an unsupervised learning task, i.e., a cluster method is applied to the words occurring in the instances of a target word. The groups found are interpreted as the senses of the target word. The WSD task is usually solved with knowledge-based approaches, or more recently, with supervised models which require annotated data. It can be also solved reasonably well by comparing the words surrounding each target word and the words belonging to the induced senses (or clusters) found during the WSI step, as we do in this section.

We employ spectral clustering on the input matrices in order to automatically discover senses (a cluster is considered a sense).  Regarding sense disambiguation, we trivially assign senses to the target word instances according to the number of common words in each cluster and the context words of the target word. In other words, for each test instance of a target word, we select the cluster (sense) with the maximum number of shared words with the current instance context.

In order to determine the best performing operations, that stray away from the trivial baselines, we propose a measure to help us identify those systems that perform the best. According to this metric, we find the combination of fusion operators is the best performing combination of features. Indeed, by transferring quality lexical similarities into its same feature matrix, we obtain more useful relations than by using any syntactic data. While the same operators that outperformed the rest in NER are not as adequate in this WSI/WSD experiment, we see that most of the feature combination techniques improve over the baselines of the single features and early fusion operations. 
\subsubsection{Leveraging the Linguistic Network Structure}

We leverage the relations that exist within the network to identify words that, together with their neighborhood, represent a sense. Thus, we propose a network-based algorithm to solve word sense induction. 
Our method  is inspired on previous approaches from both \cite{2004.Veronis} and \cite{2007.Klapaftis.UoY}. In Hyperlex,  the graph-based  method presented  in \cite{2004.Veronis}, the main intuition is that co-occurrence networks have small-world properties and thus it is possible to detect and isolate important heavily-connected nodes, called "hubs". The idea is that these hubs, and their connected nodes, represent a sense themselves. 
We apply our method to a well-known WSI/WSD dataset. We also use feature spaces produced with fusion operators.

Using our network-based approach, we obtained results that surpassed those from the similar methods while being more flexible in terms of use of parameters. Again, we found that using fusion operators yield better results compared to using single features or early fusion. While our fusion systems beat these baselines, the systems that perform the best do not employ heterogeneous data to do so. Indeed, the best systems that combine the two possible types of features lag behind the best fusion spaces. 

% chap 5
\section{Conclusions et travaux futurs}
\subsection{Conclusion}
Linguistic Networks are useful methods to understand the nature of our language. In the literature, they are generally used to comprehend either the dynamics of words and other textual units within language, and to solve practical TALN tasks. Nonetheless, no mater the objective, they are usually based on the distributional hypothesis, that is, words will be found in similar contexts if they tend to be semantically related. On the other hand, text data representations, described through contexts in a distributional framework, are sparse by nature: the large majority of the entries in a co-occurrence matrix are zero. To treat these concerns, on this thesis we proposed three contributions. The first and second entail a fusion enriched linguistic network,  which entails denser text representations by combining heterogeneous feature spaces. The second is a method based on graph structure to find groups of related words.

With regard to our fusion techniques, we tested them over both WSI/WSD and NER tasks. Particularly, in NER, we created new representation matrices that showed overall improvement in performance. In order to get to these improvements, which are consistent in the whole ensemble of datasets tested, we performed a high level of fusion aggregation. Concerning our graph-based model, we tested it on the WSI/WSD task, over the Semeval 2007 corpus. Using the free-scale presumption we found communities of words describing senses by using sentence-level lexical contexts and raw frequencies to weight the co-occurrences. The fusion operators produced representation spaces that improved over using single features, as in NER experiments.

Finally, the proposed hypergraph, through its fusion representations, generate large matrices that need to be correctly manipulated in order to solve TALN tasks. To address this challenge, we use simple solutions as simple as word filtering to more complex approaches that computationally deal with large, sparse, and dense, spaces, such as parallelization and out-of-core computing methods\footnote{Algorithms that only keep in memory the required parts of a matrix during computations, keeping the rest on the hard drive}.
\subsection{Travaux futurs}
Concerning fusion techniques, a more principled way to determine what type of context with what type of fusion operation would indeed reduce the need for exploring the whole space of possibilities. Finally,  comparing said methods with other well-established dimension reduction approaches would be interesting to understand the trade-offs of lower performance versus dimension reduction, while focusing on not-so-large corpora. Indeed, if the new wave of distributional representations, or word embeddings, has a shortcoming is that empirically it does not perform as well on smaller corpus. This may represent an avenue of opportunity to methods such as feature fusion functions.

Regarding the network-based algorithm for WSI/WSD, a deeper errors' analysis would deep a larger glimpse on the behavior of nouns and verbs according to the context. Understanding what is the syntactic or lexical difference among contexts, which induce the good or bad performance of each type of feature could make the system more flexible to other text domains. Also, the hypergraph could be better leveraged by using hypergraph-specific methods, mainly through spectral analysis.
%Fusion Enriched Hypergraph Linguistic Model
\bibliographystyle{ThesisStyle}
\bibliography{Thesis_f}

\end{document}