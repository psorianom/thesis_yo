
\documentclass[a4paper,11pt,twoside]{article}
\include{formatAndDefs_FR}

\begin{document}
%\chapter*{Résumé}
\section{Introduction}
\subsection{Contexte}


Donner du sens aux textes joue un rôle essentiel dans l'évolution de l'intelligence artificielle générale. Compte tenu de la génération croissante de données textuelles, il existe un besoin de systèmes de calcul capables d'extraire des informations utiles à partir de grandes quantités de collections textuelles pour faciliter nos activités quotidiennes et notamment de trouver des informations latentes utiles cachées derrière ces grandes quantités de données.
%
En effet, faire les ordinateurs apprendre est l'objectif général de la recherche sur l'intelligence artificielle \cite{Sugiyama2015}. Issu de ce domaine multidisciplinaire, le traitement automatique du langage naturel (TALN) est le domaine qui vise à faire comprendre aux machines notre langage \cite{JurafskyM09}.



Les solutions aux tâches TALN suivent généralement trois étapes pour atteindre leurs objectifs respectifs \cite{JurafskyM09,mining12Book}. Premièrement, en pré-traitement, un corpus d'entrée est "normalisé" de sorte qu'il sera plus facile à traiter par la machine dans les étapes suivantes. Deuxièmement, dans la représentation des caractéristiques, de nombreuses caractéristiques pertinentes sont extraites du texte prétraité. Troisièmement, une technique d'apprentissage automatique est utilisée pour apprendre un modèle capable de fournir un aperçu intéressant au sein des données existantes ainsi que sur de nouvelles instances futures. La sortie de ce système est généralement le modèle ou la connaissance du langage qui révèle une information intéressante contenue dans le corpus d'entrée.

\subsection{Problématiques et contributions}

Il y a plusieurs défis de recherche qui découlent des choix effectués dans chacune des étapes composant le flux d'un système TALN, décrit ci-dessus. Dans cette thèse, nous nous concentrons sur trois défis qui se posent à la fois dans les phases de représentation des caractéristiques et de découverte des connaissances. Ces défis sont: (1) la modélisation, l'extraction et le stockage de différents types d'éléments linguistiques à partir de textes bruts, (2) le traitement de la rareté inhérente aux données textuelles et leur combinaison pour obtenir de meilleures représentations; (3) profiter des relations entre les mots et ensuite les exploiter afin de découvrir leur parenté latente et être capable de résoudre les tâches de la PNL.


Afin de répondre à ces défis, nous proposons trois contributions:
\begin{itemize}
\item un modèle de réseau basé sur des hypergraphs pour contenir des données linguistiques hétérogènes.
\item une méthode pour combiner des représentations hétérogènes, tout en atténuant le problème des données creuses.
\item un algorithme basé sur le réseau du modèle proposé pour découvrir la relation sémantique entre mots liés.
\end{itemize}


Ces contributions sont évaluées et validées en utilisant deux taches sémantiques du TALN~: l'induction et désambiguïsation de mots (Word Sense Induction and Disambiguation ou WSDI/WSD) et la reconnaissance d'entités nommées (ou Named Entity Recongiontion NER). Nous attaquons ces tâches étant donnée qu'elles sont pièces centrales lors de la construction de systèmes TALN plus avancés.

\section{Antécédents}

\subsection{Hypothèse Distributionnelle}
Le travail que nous présentons dans cette thèse repose principalement sur l'hypothèse distributionnelle (HD). C'est aussi le cas pour la majorité des approches sémantiques actuelles dans le domaine du TALN. Cet aperçu de l'analyse du contexte textuel est crédité primordialement à \cite{harris1954}. L'hypothèse est simple mais puissante~: elle affirme que la similarité de la signifiance des mots est corrélée avec la similarité de leur contextes. Conséquemment, la signification d'un mot peut être déterminé par l'ensemble de contextes dans lesquels ce mot participe.
%
Dans ce travail nous nous focalisons exclusivement sur deux contextes~: la cooccurrence lexicale et la coocurrence syntaxique des contextes. Nous définissons l'occurrence lexicale à celle basée sur les mots qui cooccurent avec un autre mot dans un voisinage prédéfini. Les cooccurrences syntaxiques sont basées sur une analyse (ou parsing) plus profonde du texte afin d'obtenir des relations )et du sens) existantes entre les mots qui  cooccurrent.


%
%In this work we will exclusively focus on those two contexts: lexical co-occurrence and syntactic co-occurrence contexts. Lexical co-occurrences consist on those  words that co-occur with a given word in a predetermined neighborhood. Syntactic contexts are based on the analysis (or parsing) of text in order to obtain sense from them.

\subsection{Modèles de représentation}
% The Vector Space Model (VSM) consists in representing textual units in a multi-dimensional space. The textual units represented are not constrained to words themselves. We may describe co-occurrent features for documents, phrases, paragraphs, or other types \cite{manning1999foundations}. A matrix is used as the structure that holds each object and its context features. Distance metrics (or similarity measures) are used over the vectors of the matrix to determine a level of dissimilarity or similarity between these objects.
% Other type of representation models, based on graph structures are commonly used in the literature. Indeed, network based models have been studied deeply during the last years in the TALN field  \cite{Mihalcea2011}. While we can represent a graph as a matrix, and thus as a vector space model, graphs are useful representation formalism that can be applied to a large set of linguistic characteristics, from the relation between words in a text or between the features that describe them. Indeed, language being a dynamic complex system, networks provide an adequate model to represent and study the structure and evolution of linguistic systems \cite{Choudhury2009}.
%In this thesis we base our linguistic model proposition on a graph-based structure.

Le modèle d'espace vectoriel (VSM) consiste à représenter des unités textuelles dans un espace multidimensionnel. Les unités textuelles représentées ne sont pas contraintes aux mots eux-mêmes. Nous pouvons décrire des caractéristiques cooccurrentes pour des documents, des phrases, des paragraphes ou d'autres types \cite{manning1999foundations}. Une matrice est utilisée comme la structure qui contient chaque objet et ses caractéristiques contextuelles. Les métriques de distance (ou mesures de similarité) sont utilisées sur les vecteurs de la matrice pour déterminer un niveau de dissimilarité ou de similarité entre ces objets.

D'autres types de modèles de représentation, basés sur des graphes, sont couramment utilisés dans la littérature. En effet, les modèles basés sur les réseaux ont été étudiés au cours des dernières années dans le champ TALN \cite{Mihalcea2011}. Alors que nous pouvons représenter un graphique comme une matrice, et donc comme un modèle d'espace vectoriel, les graphes sont un formalisme de représentation utile qui peut être appliqué à un large ensemble de caractéristiques linguistiques, à la relation entre les mots dans un texte ou entre les caractéristiques qui les décrivent. En effet, le langage étant un système complexe dynamique, les réseaux fournissent un modèle adéquat pour représenter et étudier la structure et l'évolution des systèmes linguistiques \cite{Choudhury2009}.
Dans cette thèse, nous basons notre proposition de modèle linguistique sur une structure basée sur un graphe.

\subsection{Données creuses}

Sans tenir compte de leur type, les structures réseau sont finalement transformées en matrices avant d'être traitées par un calcul. Par conséquent, étant donné que nous sommes encore en train de modéliser le langage (les mots), les graphes peuvent être creuses tout comme les modèles d'espace vectoriel. En effet, le manque de données est un problème qui affecte la performance des approches de découverte des connaissances \cite{mining12Book, PerinetH15} appliquées aux données textuelles.

Une matrice de données éparse (ou creuse) a la plupart de ses entrées égal à zéro. Ainsi, dans notre contexte, la majorité des mots (lignes) dans le corpus sont décrits par très peu de contextes (colonnes). Ceci est un problème important car à la phase de découverte des connaissances de tout système TALN, nous visons à former un modèle d'apprentissage qui finira par prédire, classifier, grouper les mots d'une manière ou d'une autre. Si les mots sont représentés par un nombre limité de contextes, les algorithmes d'apprentissage ne pourront pas généraliser correctement.

Que ce soit basé sur une matrice ou sur un graphe, une représentation textuelle, explicite et distributionnelle sera éparse. Il y a trop de mots dans un texte et il est assuré que, bien qu'ils puissent apparaître dans d'autres textes, ils ne figureront pas dans un seul texte. Cela devient un problème important avec les systèmes TALN: les mots ne sont décrits que par un petit nombre de fonctionnalités. Dans ce qui suit, nous décrivons nos deux premières propositions qui traitent de l'utilisation d'informations hétérogènes pour représenter un terme et atténuent le manque de données qui accompagne ces types de représentations textuelles.

%Without regards to their type, network-based structures are ultimately transformed into matrices before being treated computationally. Therefore, given that we are still modeling language (words), graphs suffer from sparsity just as vector space models. Indeed, data sparsity is an issue that affects the performance of knowledge discovery approaches \cite{mining12Book,PerinetH15} applied to textual data.
%
%A sparse data matrix has most of its entries equal to zero. Thus, the majority of the words (rows) in the corpus are described by very few contexts (columns). This is a significant problem as on the knowledge discovery phase of any TALN system we aim to train a learning model that will eventually predict, classify, group our words in one way ot another. If the words are represented by a limited number of contexts, the learning algorithms will not be able to generalize properly. 
%%\subsection{Conclusion}
%
%Whether its vector based or graph-based, a textual, explicit, and distributional representation will be sparse. There are too many words in a text and its assured that, while they could occur in other texts, they will not occur in a single text. This becomes an important problem with TALN systems: words are described by only a small set of features. In the following describe our two first propositions which address the issue of using heterogeneous information to represent a term and alleviating the data sparsity that comes with such types of textual representations.


\section{Modèle linguistique basé sur des hypergraphes enrichi par la fusion}
%The first two contributions of this thesis are contained in a fusion enriched hypergraph linguistic model proposition. The model  consists on two components which address two research questions each: the issue of making sparsity less severe and leveraging different types of features  by using a single feature representation space. 
%
%The model we present here entails three  important characteristics: firstly, the possibility to leverage different types of information.  Secondly, as the words will be linked together, there is an inner structure that will emerge from the model and which we exploit in our experiments. Thirdly, given that we treat unstructured text data, the relations (or features)  between words are sparse, this is alleviated by combining features via fusion techniques.  The three of them are addressed with our propositions. 
%
%Our network is based on the distributional hypothesis, as described in the previous chapter.  As co-occurrence features, we select both lexical and syntactic contexts, indeed creating a linguistic resource that hold both types of information in order to get a complementary insight of words' relations. 
%
%
%In the literature, regarding WSD approaches, we see that the use of a lexical knowledge base, such as Wordnet\footnote{\url{https://wordnet.princeton.edu}}, is pervasive in this task. On the other hand, WSI, while being a more flexible approach (language and word-usage independent, does not require human-made bases)  for solving WSD, its results are tightly linked to the quality of the clustering algorithm used. 
%% 
%
%With respect to the networks' modelization, we find that few approaches deal with syntactic attributes. We believe that finding semantic similarities can be improved by adding syntactic information not only  while using dependency relations but also by leveraging the constituency tree of each word. Moreover, using syntactic data along with semantic and/or lexical co-occurrences takes us into the heterogeneous network domain which has not been addressed in most of the approaches covered.  
%
%
%Taking into account the described opportunities of research, in the following section we propose a  hypergraph modelization of a linguistic network that aims to solve some limitations stated above. 

Les deux premières contributions de cette thèse sont contenues dans la proposition d'un modèle linguistique basé sur des hypergraphes et enrichie avec des techniques de fusion. Ainsi, le modèle traite les deux premières questions de recherche exposées ci-dessus: l'exploitation de différents types de caractéristiques en utilisant un espace de représentation unique et au même temps, en réduisant l'eparsité de données caractéristique aux données textuelles.

Le modèle que nous présentons ici comporte trois caractéristiques importantes: premièrement, la possibilité de tirer parti des différents types d'informations textuelles. Deuxièmement, comme les mots du texte seront liés ensemble, il y aura une structure interne qui émergera du modèle et que nous exploiterons dans nos expériences. Troisièmement, étant donné que nous traitons des données textuelles non structurées, les relations (ou caractéristiques) entre les mots sont rares, ce qui est atténué par la combinaison de caractéristiques via les techniques de fusion. 

Notre modèle est basé sur l'hypothèse distributionnelle, telle que décrite précédemment. En tant que caractéristiques de cooccurrence de mots, nous sélectionnons à la fois des contextes lexicaux et syntaxiques, créant une ressource linguistique qui contient les deux types d'informations afin d'obtenir un aperçu complémentaire des relations entre les mots.


%Dans la littérature, nous voyons que l'utilisation d'une base de connaissances lexicales, telle que Wordnet \footnote{\url{https://wordnet.princeton.edu}}, est omniprésente dans cette tâche. D'autre part, WSI, tout en étant une approche plus flexible (langage et utilisation de mots indépendants, ne nécessite pas de bases artificielles) pour résoudre WSD, ses résultats sont étroitement liés à la qualité de l'algorithme de clustering utilisé.
%

En effet, dans la littérature, nous constatons que peu d'approches traitent les attributs syntaxiques. Nous pensons que la recherche de similitudes sémantiques peut être améliorée en ajoutant des informations syntaxiques non seulement lors de l'utilisation de relations de dépendance, mais aussi en exploitant l'arbre de constituants de chaque mot. En outre, l'utilisation de données syntaxiques avec des cooccurrences sémantiques et/ou lexicales nous amène dans le domaine de réseau hétérogène qui n'a pas été abordé dans la plupart des approches existentes.


En prenant en compte les opportunités de recherche décrites, nous proposons dans la une modélisation basée sur des hypergraphes d'un réseau linguistique qui vise à résoudre certaines limitations énoncées ci-dessus.

%\subsection{Proposed Model: Fusion Enriched Hypergraph Linguistic Network}

%As stated before, our model consists on two parts (and two contributions). The first one, an hypergraph model that holds different types of linguistic relations extracted from a corpus. And the second one, the combination of linguistic features in order to generate a less sparse, enriched representation. 
%
%Our model is  based on the use of a hypergraph. Its single most important difference with regular graphs, is being able to relate more than two vertices at the same type, which allows for a better characterization of interactions within a set of individual elements (in our case, words) \cite{heintz2014beyond}. Indeed, our hypergraph modelization initially integrates four types of relations between tokens: sentence co-occurrence, part-of-speech tags, words' constituents data and dependency relations in a single linguistic structure. These relationships were chosen because it is relatively easy to obtain them for high-resource languages. These features can be seen as building blocks for TALN models. In any case, our goal is to arrive to more complex annotations (e.g., named entities) from the selected features and relations. We decided to keep a lexical context at sentence level, so that it may complement the distributional semantic information provided by the dependency functions context as well as the phrase-constituency syntactic context. In short, we  aim to cover three levels of possible semantic relatedness via  three levels (in terms of the size of the neighborhood of a target word) of distributional co-occurrences: a short range with dependency functions, a medium range with phrase constituency membership, and a longer range with sentence lexical co-occurrences. The intuition is that when solving TALN tasks, having direct access to these three semantic spaces will help to determine a more appropriate meaning's relation between words. 
%
%The second part of our proposed method deals with the fusion of textual features. Namely, we combine the features that describe terms into a single representation space. This new space aims to address two issues that arise while working with textual data: effectively using information coming from different linguistic levels (e.g., lexical, syntactic, semantic) while alleviating the sparsity typical of textual representations. In the multimodal fusion literature we can discern two main common types of techniques: early fusion and late fusion. A third and fourth type of fusion methods, cross-media fusion and hybrid fusion are also employed in multimedia analysis tasks. 
%
%These four fusion operators naturally address the issue of dealing with heterogeneous data as they all mix one way or another the feature columns from each of two representations. Regarding alleviating sparsity, the intuition is that by combining matrices either by summing or element-wise multiplying them, the resulting matrix will have a denser structure. For example, by summing two matrices with the same shape, such as two term-term similarity matrices, we  obtain a resulting matrix that contains the similarities of both feature spaces. In the same sense, when multiplying two matrices we combine them while also obtaining a denser output matrix. Nonetheless, both sum and multiplication result depends evidently on the nature of the matrices employed. 
%
%In order to materialize our proposed linguistic model we implemented a procedure that takes a corpus as input and outputs the linguistic resource we introduced in the previous section. We based our process on the online encyclopedia Wikipedia\footnote{\url{https://en.wikipedia.org}} which has been used as a source of valuable data as well as a common background corpus to solve diverse TALN/TM related tasks. 


Comme indiqué précédemment, notre proposition consiste en deux parties. La première, un modèle à base d'hypergraphes qui contient différents types de relations linguistiques extraites d'un corpus. Et la seconde, la combinaison de caractéristiques linguistiques afin de générer une représentation moins creuse et enrichie, a travers des techniques de fusion.

Notre modèle est basé sur l'utilisation d'un hypergraphe. Sa différence la plus importante avec les graphes réguliers est de pouvoir relier plus de deux sommets au même temps, ce qui permet une meilleure caractérisation des interactions au sein d'un ensemble d'éléments individuels (dans notre cas, les mots) \cite{heintz2014beyond}. En effet, notre modélisation hypergraphique intègre initialement quatre types de relations entre les mots: la co-occurrence de phrase, les étiquettes morpho-syntaxiques, la structure syntaxique, et les relations de dépendance dans une structure linguistique unique. Ces relations ont été choisies car il est relativement facile de les obtenir pour les langues à ressources élevées. Ainsi, ces caractéristiques peuvent être considérées comme des blocs de construction pour les modèles TALN. Dans tous les cas, notre but est d'arriver à des annotations plus complexes (par exemple, des entités nommées) à partir des caractéristiques et des relations pertinentes.

Nous avons décidé de garder un contexte lexical au niveau de la phrase, de sorte qu'il puisse compléter l'information sémantique fournie par le contexte basé sur les relations de dépendance ainsi que le contexte syntaxique de la phrase. En bref, nous visons à couvrir trois niveaux de relation sémantique: un niveau proche avec les fonctions de dépendance, une distance moyenne avec une appartenance à une phrase nominale (avec la structure syntaxique), et un niveau plus long avec la coexistence lexicale de phrase, c-à-d, le voisinage des mots a niveau de toute la phrase. L'intuition est que lors de la résolution de tâches TALN, avoir un accès direct à ces trois espaces sémantiques aidera à déterminer une relation de signification plus appropriée entre les mots.

La deuxième partie de notre méthode proposée traite sur la fusion des caractéristiques textuelles. À savoir, nous combinons les caractéristiques qui décrivent les termes en un seul espace de représentation unique. Ce nouvel espace vise à répondre à deux problèmes qui se posent lors de l'utilisation de données textuelles: comment utiliser efficacement des informations provenant de différents niveaux linguistiques (par exemple, lexical, syntaxique, sémantique) tout en allégeant l'eparsité typique dans les représentations textuelles. 

Dans la littérature sur la fusion multimodale, nous pouvons discerner deux types principaux de techniques: la fusion précoce et la fusion tardive. Un troisième et un quatrième type de procédés de fusion, fusion croisée multimédias et fusion hybride sont également utilisés dans des tâches d'analyse multimédia. Nous utilisons ses techniques aux cours de nos expériences.

Ces quatre opérateurs de fusion abordent naturellement la question du traitement de données hétérogènes car ils mélangent tous d'une manière ou d'une autre les colonnes de caractéristiques de chacune des deux représentations (dans le contexte d'une matrice de représentation). En ce qui concerne la réduction du taux des données creuses, l'intuition est qu'en combinant les matrices soit en les sommant, soit en les multipliant par éléments, la matrice résultante aura une structure plus dense. Par exemple, en additionnant deux matrices ayant la même forme, telles que deux matrices de similarité terme à terme, nous obtenons une matrice résultante qui contient les similarités des deux espaces caractéristiques. Dans le même sens, en multipliant ses deux matrices, nous les combinons tout en obtenant une matrice de sortie plus dense. Néanmoins, le résultat de la somme et de la multiplication dépend évidemment de la nature des matrices utilisées.

Nous appliquons ces opérateurs de fusion a notre modèle base sur des hypergraphes et nous rendons un modèle unique qui réponde aux questions posées initialement.

Afin de matérialiser le modèle linguistique proposé, nous avons mis en place une procédure qui prend en compte un corpus et produit la ressource linguistique que nous venons d'introduire dans les paragraphes précédents. Nous avons basé notre processus sur l'encyclopédie en ligne Wikipédia \footnote{\url{https://fr.wikipedia.org}} qui a été utilisée comme une source de données importante ainsi qu'un corpus de fond commun pour résoudre divers tâches du TALN et du texte mining.

\section{Applications à la reconnaissance d'entités nommées et à la désambiguïsation du sens du mot}

%\subsection{Introduction}
%
%In this application's chapter we set to solve two natural language processing tasks using as data source corpora in the form of our proposed model. We address the tasks of Named Entity Recognition (NER) and Word Sense Induction and Disambiguation (WSI/WSD). We employ both a fusion enriched and a raw hypergraph network based on benchmark corpora to validate the utility of our proposals.

Dans cette section, nous avons décidé de résoudre deux tâches de traitement du langage naturel en utilisant comme corpus des données sous la forme de notre modèle proposé. Nous abordons les tâches de reconnaissance d'entités nommées (NER) et d'induction et de désambiguïsation de sens des mots (WSI / WSD). Nous utilisons un réseau hypergraphique enrichi basé sur des corpus de référence pour valider l'utilité de nos propositions.

\subsection{Première application: reconnaissance d'entités nommées et désambiguïsation des sens des mots}

%NER goal is to automatically discover, within a text, mentions that belong to a well-defined semantic category. The classic task of NER involves detecting, within a text, entities of type Location (LOC), Organization (ORG), Person (PER), Miscellaneous (MISC), or if the term is not an even an entity, assigning them a (O) label. The task is of great importance for more complex TALN systems, e.g, relation extraction, opinion mining \cite{nadeau2007survey}.  Generally, the common solution to NER involves training a supervised machine learning algorithm with large quantities of annotated text \cite{mining12Book}. As is usual with other TALN tasks, NER  requires textual features to represent words in order to determine their role within a phrase. We propose to build representations based on our fusion enriched hypergraph model. For the main experiments, we chose a structured perceptron learning algorithm because of its performance and its lower training time. 
%
%We experiment with the four levels of fusion on three different datasets. The representation matrices for NER come from lexical context features $\mlex$, syntactical context features or other task-standard features.  On the other hand, experiments on WSI/WSD exclusively employ lexical and syntactic matrices. Our main goal is to compare the efficiency of the primary  fusion techniques applied to   named entity recognition. Then, we empirically determine a fusion combination operator able to leverage the complementarity of the features used.
%We discover that using a recombination of several fusion operations, a so-called hybrid approach, we  improve over using the features individually and also over the use of the trivial early fusion operator. This indicates that the single feature matrix, enriched with other combined features, is enough to improve the results of said baselines. In general, in our experiments with NER, we see that the added enriched features are not the most important for the learning algorithm while taking a decision, nonetheless, they  provide the extra information needed to push the model towards the correct prediction, by enriching the features through cross and late fusion and by providing more descriptors for each word and consequently reducing the sparsity of the representation matrices.
%
%Once we found a set of fusion operations that work reasonably well with NER, we experiment with another task, word sense induction and disambiguation, to confirm the usefulness of using fusion enriched representations to train better models.

Le but du NER est de découvrir automatiquement, dans un texte, des mentions appartenant à une catégorie sémantique bien définie. La tâche classique de NER consiste à détecter, dans un texte, des entités de type Lieu (LOC), Organisation (ORG), Personne (PER), Divers (MISC) ou, si le terme n'est pas une entité nommée, leur attribuer une étiquette appropriée (à l'occurrence, l'étiquette O). La tâche est d'une grande importance pour les systèmes TALN plus complexes. Par exemple, pour l'extraction de relations ou la fouille de l'opinion \cite{nadeau2007survey}. Généralement, la solution commune au NER consiste à entraîner un algorithme d'apprentissage automatique supervisé avec de grandes quantités de texte annoté \cite{mining12Book}. Comme d'habitude avec des autres tâches TALN, le NER nécessite des représentations (ou features) pour les mots afin de déterminer leur rôle dans une phrase. Nous proposons de construire ses représentations en nous basant sur notre modèle hypergraphique enrichi par fusion. Pour les expériences principales, nous avons choisi un algorithme d'apprentissage de type perceptron structuré en raison de ses performances et de son temps de formation réduit.

Nous expérimentons les quatre niveaux de fusion sur trois ensembles de données différents. Les matrices de représentation pour NER proviennent des fonctions contextuelles lexicales, des fonctions des dépendances syntaxiques ou d'autres caractéristiques dites standard (standard pour la tache en question). Notre objectif principal est de comparer l'efficacité des techniques de fusion primaires appliquées à la reconnaissance d'entités nommées. Aussi, nous déterminons empiriquement un opérateur de combinaison de fusion capable de tirer parti de la complémentarité des caractéristiques utilisées.
%
Nous découvrons qu'en utilisant une recombinaison de plusieurs opérations de fusion, une approche dite hybride, nous améliorons l'utilisation des caractéristiques individuellement et aussi sur l'utilisation de l'opérateur de fusion précoce trivial, qui consiste à la simple concaténation de features dans un espace unique. Ceci indique que la matrice de caractéristiques uniques, enrichie avec d'autres caractéristiques combinées, est suffisante pour améliorer les résultats desdites \textit{baselines}. En général, dans nos expériences avec NER, nous voyons que les fonctionnalités enrichies ajoutées ne sont pas les plus importantes pour l'algorithme d'apprentissage lors de la prise d'une décision, néanmoins, elles fournissent les informations supplémentaires nécessaires pour pousser le modèle vers la prédiction correcte, en enrichissant caractéristiques à travers la fusion croisée et tardive et en fournissant plus de descripteurs pour chaque mot et par conséquent réduire  aussi le niveau des données creuses dans les matrices de représentation.

Une fois que nous avons trouvé un ensemble d'opérations de fusion qui fonctionnent raisonnablement bien avec NER, nous expérimentons une autre tâche du TALN, l'induction et désambiguïsation du sens des mots, pour confirmer l'intérêt d'utiliser des représentations enrichies par fusion pour arriver à des meilleurs modèles.


\subsection{Deuxième application: Induction et désambiguïsation de mots}
\subsubsection{Représentations enrichies par fusion}

%\subsection{Second Application: Word Sense Induction and Disambiguation}
%\subsubsection{Fusion Enriched Representations}


%Having learned the best fusion configuration from the NER task, in these experiments we set to test if the improvements achieved can be transfered into another TALN task, namely Word Sensed Induction and Disambiguation (WSI/WSD). As preprocessing, we simply remove stopwords and tokens with less than three letters. The features we extracted from the tested corpora with the same tools as in the previous task.
%Word Sense Induction and Disambiguation entails two closely related tasks. WSI aims to automatically discover the set of possible senses for a target word given a text corpus containing several occurrences of said target word. Meanwhile, WSD takes a set of possible senses and determines the most appropriate sense for each instance of the target word according to the instance's context. WSI is usually approached as an unsupervised learning task, i.e., a cluster method is applied to the words occurring in the instances of a target word. The groups found are interpreted as the senses of the target word. The WSD task is usually solved with knowledge-based approaches, or more recently, with supervised models which require annotated data. It can be also solved reasonably well by comparing the words surrounding each target word and the words belonging to the induced senses (or clusters) found during the WSI step, as we do in this section.
%
%We employ spectral clustering on the input matrices in order to automatically discover senses (a cluster is considered a sense).  Regarding sense disambiguation, we trivially assign senses to the target word instances according to the number of common words in each cluster and the context words of the target word. In other words, for each test instance of a target word, we select the cluster (sense) with the maximum number of shared words with the current instance context.
%
%In order to determine the best performing operations, that stray away from the trivial baselines, we propose a measure to help us identify those systems that perform the best. According to this metric, we find the combination of fusion operators is the best performing combination of features. Indeed, by transferring quality lexical similarities into its same feature matrix, we obtain more useful relations than by using any syntactic data. While the same operators that outperformed the rest in NER are not as adequate in this WSI/WSD experiment, we see that most of the feature combination techniques improve over the baselines of the single features and early fusion operations. 

Après avoir appris la meilleure configuration de fusion à partir de la tâche NER, nous avons testé dans les expériences qui suivent si les améliorations obtenues peuvent être transférées à une autre tâche TALN, à savoir, l'induction et la désambiguïsation de sens de mots (WSI/WSD).

L'induction (Word Sense Induction WSI) et la désambiguïsation (Word Sense Disambiguation WSD) des mots impliquent deux tâches étroitement liées. Le WSI vise à découvrir automatiquement l'ensemble des sens possibles pour un mot cible donné relatif à un corpus de texte contenant plusieurs occurrences dudit mot cible. D'un autre côté, WSD prend un ensemble de sens possibles pour un mot et s'occupe de déterminer le sens le plus approprié pour chacune des instance du ce mot cible en fonction de son contexte. Le WSI est généralement approché comme une tâche d'apprentissage non supervisée, c'est-à-dire qu'une méthode de clustering est appliquée aux mots apparaissant dans les contexte des instances d'un mot cible. Puis, les groupes trouvés sont interprétés comme les sens du mot cible. La tâche WSD est généralement résolue avec des approches basées sur des graphes de connaissances, ou plus récemment, avec des modèles supervisés qui nécessitent des données annotées. Il peut également être résolu raisonnablement bien en comparant les mots entourant chaque mot cible et les mots appartenant aux sens induits trouvés lors de l'étape WSI, comme nous le faisons dans cette section.

Nous utilisons un clustering spectral sur les matrices d'entrée afin de découvrir automatiquement les sens (encore un fois, un cluster est considéré comme un sens). En ce qui concerne la désambiguïsation des sens, nous affectons trivialement les sens aux instances de mots cibles en fonction du nombre de mots communs dans chaque groupe et les mots du contexte du mot cible. En d'autres termes, pour chaque instance de test d'un mot cible, nous sélectionnons le groupe (sens) avec le nombre maximum de mots partagés avec le contexte de l'instance actuel.

Afin de déterminer les opérations les plus performantes, éloignées des baselines triviaux, nous proposons une mesure pour nous aider à identifier les systèmes les plus performants. Selon cette métrique, nous trouvons que la combinaison des opérateurs de fusion est la combinaison de caractéristiques la plus performante. En effet, en transférant des similarités lexicales de qualité dans la même matrice de caractéristiques, nous obtenons des relations plus utiles qu'en utilisant des données syntaxiques. Alors que les mêmes opérateurs qui ont surpassé le reste dans NER ne sont pas aussi adéquats dans cette expérience WSI / WSD, nous voyons que la plupart des techniques de combinaison d'entités s'améliorent sur les lignes de base des caractéristiques uniques et des opérations de fusion précoce.


%\subsubsection{Leveraging the Linguistic Network Structure}
\subsubsection{Tirer parti de la structure du réseau linguistique}
%
%We leverage the relations that exist within the network to identify words that, together with their neighborhood, represent a sense. Thus, we propose a network-based algorithm to solve word sense induction. 
%Our method  is inspired on previous approaches from both \cite{2004.Veronis} and \cite{2007.Klapaftis.UoY}. In Hyperlex,  the graph-based  method presented  in \cite{2004.Veronis}, the main intuition is that co-occurrence networks have small-world properties and thus it is possible to detect and isolate important heavily-connected nodes, called "hubs". The idea is that these hubs, and their connected nodes, represent a sense themselves. 
%We apply our method to a well-known WSI/WSD dataset. We also use feature spaces produced with fusion operators.
%
%Using our network-based approach, we obtained results that surpassed those from the similar methods while being more flexible in terms of use of parameters. Again, we found that using fusion operators yield better results compared to using single features or early fusion. While our fusion systems beat these baselines, the systems that perform the best do not employ heterogeneous data to do so. Indeed, the best systems that combine the two possible types of features lag behind the best fusion spaces. 


Nous tirons parti des relations qui existent au sein du réseau pour identifier les mots qui, avec leur voisinage, représentent un sens. Ainsi, nous proposons un algorithme basé sur le réseau pour résoudre l'induction du sens du mot.
Notre méthode est inspirée des approches précédentes de \ cite {2004.Veronis} et de \ cite {2007.Klapaftis.UoY}. Dans Hyperlex, la méthode basée sur les graphes présentée dans \ cite {2004.Veronis}, l'intuition principale est que les réseaux de cooccurrence ont des propriétés de petit monde et il est donc possible de détecter et d'isoler des n?uds importants connectés, appelés "hubs". ". L'idée est que ces hubs, et leurs n?uds connectés, représentent eux-mêmes un sens.
Nous appliquons notre méthode à un jeu de données WSI / WSD bien connu. Nous utilisons également des espaces de fonctions produits avec des opérateurs de fusion.

En utilisant notre approche basée sur le réseau, nous avons obtenu des résultats qui surpassaient ceux des méthodes similaires tout en étant plus flexibles en termes d'utilisation des paramètres. Encore une fois, nous avons constaté que l'utilisation d'opérateurs de fusion donne de meilleurs résultats par rapport à l'utilisation de caractéristiques uniques ou fusion précoce. Alors que nos systèmes de fusion battent ces bases, les systèmes qui fonctionnent le mieux n'emploient pas de données hétérogènes pour le faire. En effet, les meilleurs systèmes qui combinent les deux types de fonctionnalités possibles sont en retard par rapport aux meilleurs espaces de fusion.

\section{Conclusions et travaux futurs}
\subsection{Conclusion}
%Linguistic Networks are useful methods to understand the nature of our language. In the literature, they are generally used to comprehend either the dynamics of words and other textual units within language, and to solve practical TALN tasks. Nonetheless, no mater the objective, they are usually based on the distributional hypothesis, that is, words will be found in similar contexts if they tend to be semantically related. On the other hand, text data representations, described through contexts in a distributional framework, are sparse by nature: the large majority of the entries in a co-occurrence matrix are zero. To treat these concerns, on this thesis we proposed three contributions. The first and second entail a fusion enriched linguistic network,  which entails denser text representations by combining heterogeneous feature spaces. The second is a method based on graph structure to find groups of related words.
%
%With regard to our fusion techniques, we tested them over both WSI/WSD and NER tasks. Particularly, in NER, we created new representation matrices that showed overall improvement in performance. In order to get to these improvements, which are consistent in the whole ensemble of datasets tested, we performed a high level of fusion aggregation. Concerning our graph-based model, we tested it on the WSI/WSD task, over the Semeval 2007 corpus. Using the free-scale presumption we found communities of words describing senses by using sentence-level lexical contexts and raw frequencies to weight the co-occurrences. The fusion operators produced representation spaces that improved over using single features, as in NER experiments.
%
%Finally, the proposed hypergraph, through its fusion representations, generate large matrices that need to be correctly manipulated in order to solve TALN tasks. To address this challenge, we use simple solutions as simple as word filtering to more complex approaches that computationally deal with large, sparse, and dense, spaces, such as parallelization and out-of-core computing methods\footnote{Algorithms that only keep in memory the required parts of a matrix during computations, keeping the rest on the hard drive}.
Les réseaux linguistiques sont des méthodes utiles pour comprendre la nature de notre langue. Dans la littérature, ils sont généralement utilisés pour comprendre la dynamique des mots et d'autres unités textuelles dans le langage, et pour résoudre des tâches TALN pratiques. Néanmoins, quel que soit l'objectif, ils sont généralement basés sur l'hypothèse distributionnelle, c'est-à-dire que les mots seront trouvés dans des contextes similaires s'ils ont tendance à être liés sémantiquement. D'un autre côté, les représentations de données textuelles, décrites par des contextes dans un cadre distributionnel, sont rares par nature: la grande majorité des entrées dans une matrice de cooccurrence sont nulles. Pour traiter ces préoccupations, nous avons proposé trois contributions sur cette thèse. Le premier et le second impliquent un réseau linguistique enrichi par fusion, qui implique des représentations de texte plus denses en combinant des espaces de caractéristiques hétérogènes. La seconde est une méthode basée sur la structure d'un graphe pour trouver des groupes de mots apparentés.

En ce qui concerne nos techniques de fusion, nous les avons testées sur des tâches WSI / WSD et NER. En particulier, dans le TNS, nous avons créé de nouvelles matrices de représentation qui ont montré une amélioration globale des performances. Afin d'obtenir ces améliorations, qui sont cohérentes dans l'ensemble des ensembles de données testés, nous avons effectué un niveau élevé d'agrégation de fusion. Concernant notre modèle graphique, nous l'avons testé sur la tâche WSI / WSD, sur le corpus Semeval 2007. En utilisant la présomption d'échelle libre, nous avons trouvé des communautés de mots décrivant des sens en utilisant des contextes lexicaux au niveau des phrases et des fréquences brutes pour pondérer les cooccurrences. Les opérateurs de fusion ont produit des espaces de représentation améliorés par l'utilisation de caractéristiques uniques, comme dans les expériences NER.

Enfin, l'hypergraphe proposé, à travers ses représentations de fusion, génère de grandes matrices qui doivent être correctement manipulées pour résoudre les tâches TALN. Pour relever ce défi, nous utilisons des solutions simples aussi simples que le filtrage de mots pour des approches plus complexes qui traitent des espaces volumineux, clairsemés et denses, tels que la parallélisation et les méthodes de calcul externes [footnote {Algorithmes mémoriser les parties requises d'une matrice pendant les calculs, en gardant le reste sur le disque dur}.


\subsection{Travaux futurs}
%Concerning fusion techniques, a more principled way to determine what type of context with what type of fusion operation would indeed reduce the need for exploring the whole space of possibilities. Finally,  comparing said methods with other well-established dimension reduction approaches would be interesting to understand the trade-offs of lower performance versus dimension reduction, while focusing on not-so-large corpora. Indeed, if the new wave of distributional representations, or word embeddings, has a shortcoming is that empirically it does not perform as well on smaller corpus. This may represent an avenue of opportunity to methods such as feature fusion functions.
%
%Regarding the network-based algorithm for WSI/WSD, a deeper errors' analysis would deep a larger glimpse on the behavior of nouns and verbs according to the context. Understanding what is the syntactic or lexical difference among contexts, which induce the good or bad performance of each type of feature could make the system more flexible to other text domains. Also, the hypergraph could be better leveraged by using hypergraph-specific methods, mainly through spectral analysis.
En ce qui concerne les techniques de fusion, une façon plus raisonnée de déterminer quel type de contexte avec quel type d'opération de fusion réduirait en fait la nécessité d'explorer tout l'espace des possibilités. Enfin, la comparaison de ces méthodes avec d'autres approches de réduction de dimension bien établies serait intéressante pour comprendre les compromis entre la réduction des performances et la réduction des dimensions, tout en se concentrant sur les corpus moins importants. En effet, si la nouvelle vague des représentations distributionnelles, ou des plongements de mots, a un inconvénient, c'est empiriquement qu'elle ne fonctionne pas aussi bien sur les corpus plus petits. Cela peut représenter une avenue d'opportunité pour des méthodes telles que les fonctions de fusion de caractéristiques.

En ce qui concerne l'algorithme basé sur le réseau pour WSI / WSD, une analyse plus approfondie des erreurs approfondirait un plus grand aperçu du comportement des noms et des verbes en fonction du contexte. Comprendre quelle est la différence syntaxique ou lexicale entre les contextes, qui induisent les bonnes ou mauvaises performances de chaque type de fonctionnalité, pourrait rendre le système plus flexible à d'autres domaines de texte. En outre, l'hypergraphe pourrait être mieux exploité en utilisant des méthodes hypergraphes, principalement par l'analyse spectrale.


\bibliographystyle{ThesisStyle}
\bibliography{Thesis_f}

\end{document}