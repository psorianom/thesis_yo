% !TeX spellcheck = fr-moderne
\documentclass[a4paper,11pt,twoside]{article}
\include{formatAndDefs_FR}
\title{Hypergraphes et fusion d'information pour l'enrichissement de la représentation de termes. Applications à la reconnaissance d'entités nommées et à la désambiguïsation des sens des mots}
\date{}
\begin{document}
\maketitle
%\chapter*{Résumé}
\section{Introduction}
\subsection{Contexte}


Appréhender la sémantique portée par des documents textes joue un rôle essentiel dans l'évolution de l'intelligence artificielle. Compte tenu de la génération croissante de données textuelles, il existe, en effet, un besoin fort de systèmes capables d'extraire des informations pertinentes et porteur de sens à partir de grandes quantités de documents. Ces extractions aident à l'indexation, l'interprétation, l'exploitation, bref à l'analyse de l'information contenue dans des textes, et trouvent ainsi de nombreuses applications dans nos activités quotidiennes.
%
De façon plus générale, rendre les ordinateurs capables de reproduire avec efficacité les aptitudes cognitives propres aux humains est l'objectif de la recherche en intelligence artificielle \cite{Sugiyama2015}. Issu de ce domaine multidisciplinaire, le traitement automatique du langage naturel (TALN) ou encore la linguistique computationnelle est la branche qui à rendre capables des machines de comprendre et de générer notre langage \cite{JurafskyM09}.

Les solutions aux tâches de TALN suivent généralement trois étapes pour être mises en oeuvre \cite{JurafskyM09,mining12Book}. Premièrement, en pré\-/traitement, un corpus d'entrée est "normalisé" de sorte qu'il soit plus facile à traiter par la machine dans les étapes suivantes. Deuxièmement, dans la représentation des unités linguistiques (mots, groupes de mots, phrases, documents,\ldots), de nombreuses caractéristiques pertinentes sont extraites du texte pré-traité. Troisièmement, une technique d'apprentissage automatique est utilisée pour apprendre un modèle capable de résoudre de façon efficace la tâche sur les données d'apprentissage mais surtout sur de nouvelles données. La sortie de ce système est le modèle instancié qui révèle une certaine connaissance du langage permettant de traiter efficacement la tâche donnée.

\subsection{Problématiques et contributions}

Il y a plusieurs défis de recherche qui découlent des choix effectués dans chacune des étapes composant le flux d'un système TALN, décrit ci-dessus. Dans cette thèse, nous nous concentrons sur trois défis qui se posent à la fois dans les phases de représentation des caractéristiques et de découverte des connaissances. Ces défis sont: (1) la modélisation, l'extraction et le stockage de différents types d'éléments linguistiques à partir de textes bruts, (2) le traitement de la rareté inhérente aux données textuelles et leur combinaison pour obtenir de meilleures représentations; (3) profiter des relations entre les mots et ensuite les exploiter afin de découvrir leur parenté latente et être capable de résoudre des tâches du TALN.

Afin de répondre à ces défis, nous proposons trois contributions:
\begin{itemize}
\item un modèle de réseau basé sur des hypergraphes pour contenir des données linguistiques hétérogènes.
\item des méthodes de fusion permettant de combiner des représentations hétérogènes mais complémentaires, tout en atténuant le problème des données creuses.
\item un algorithme basé sur le réseau du modèle proposé pour découvrir la relation sémantique entre mots liés.
\end{itemize}

Ces contributions sont évaluées et validées en utilisant deux taches sémantiques du TALN~: la désambiguïsation lexicale (Word Sense Induction and Disambiguation ou WSDI/WSD) et la reconnaissance d'entités nommées (Named Entity Recognition ou NER). Nous choisissons  ces tâches étant donné qu'elles sont centrales dans les systèmes de TALN les plus avancés.

\section{Généralités}

\subsection{Hypothèse Distributionnelle}
Le travail que nous présentons dans cette thèse repose principalement sur l'hypothèse distributionnelle (HD)\cite{harris1954}. C'est aussi le cas pour la majorité des approches sémantiques actuelles dans le domaine du TALN. L'hypothèse est simple mais puissante~: les mots qui apparaissent dans les mêmes contextes linguistiques partagent des significations similaires. Par conséquent, la signification d'un mot peut être déterminée par l'ensemble des contextes dans lesquels ce mot participe. 
%
Dans ce travail nous nous focalisons exclusivement sur deux contextes~: la co-occurrence lexicale et la co-occurrence syntaxique des contextes. Nous définissons l'occurrence lexicale comme celle basée sur les mots qui co-occurrent avec un autre mot dans un voisinage prédéfini. Les co-occurrences syntaxiques sont basées sur une analyse (ou parsing) plus profonde du texte afin d'obtenir des relations (et le sens) existantes entre les mots qui co-occurrent.


%
%In this work we will exclusively focus on those two contexts: lexical co-occurrence and syntactic co-occurrence contexts. Lexical co-occurrences consist on those  words that co-occur with a given word in a predetermined neighborhood. Syntactic contexts are based on the analysis (or parsing) of text in order to obtain sense from them.

\subsection{Modèles de représentation}
% The Vector Space Model (VSM) consists in representing textual units in a multi-dimensional space. The textual units represented are not constrained to words themselves. We may describe co-occurrent features for documents, phrases, paragraphs, or other types \cite{manning1999foundations}. A matrix is used as the structure that holds each object and its context features. Distance metrics (or similarity measures) are used over the vectors of the matrix to determine a level of dissimilarity or similarity between these objects.
% Other type of representation models, based on graph structures are commonly used in the literature. Indeed, network based models have been studied deeply during the last years in the TALN field  \cite{Mihalcea2011}. While we can represent a graph as a matrix, and thus as a vector space model, graphs are useful representation formalism that can be applied to a large set of linguistic characteristics, from the relation between words in a text or between the features that describe them. Indeed, language being a dynamic complex system, networks provide an adequate model to represent and study the structure and evolution of linguistic systems \cite{Choudhury2009}.
%In this thesis we base our linguistic model proposition on a graph-based structure.

Le modèle d'espace vectoriel consiste à représenter des unités textuelles dans un espace multidimensionnel. Les unités textuelles ne sont pas forcément les mots. Nous pouvons décrire des caractéristiques co-occurrentes pour les documents, les phrases, les paragraphes ou d'autres types d'unités \cite{manning1999foundations}. Une matrice est utilisée comme la structure qui contient chaque objet et ses caractéristiques contextuelles. Les métriques de distance (ou mesures de similarité) sont utilisées sur les vecteurs de la matrice pour déterminer un niveau de dissimilarité ou de similarité entre ces objets.

D'autres types de modèles de représentation, basés sur des graphes, sont couramment utilisés dans la littérature. En effet, les modèles basés sur les réseaux ont été étudiés au cours des dernières années dans le champ du TALN \cite{Mihalcea2011}. Alors que nous pouvons représenter un graphe comme une matrice, et donc comme un modèle dans un espace vectoriel, les graphes sont un formalisme de représentation utile qui peuvent être appliqués à un large ensemble de caractéristiques linguistiques comme la relation entre les mots dans un texte ou entre les caractéristiques qui les décrivent. En effet, le langage étant un système complexe et dynamique, les réseaux fournissent un modèle adéquat pour représenter et étudier la structure et l'évolution des systèmes linguistiques \cite{Choudhury2009}.
Dans cette thèse, nous basons notre proposition de modèle linguistique sur un hypergraphe dont les hyperarêtes peuvent être de différents types.

\subsection{Données creuses}

Peu importe leur type, les réseaux d'information sont généralement transformés en matrices avant d'être traités par un calcul. Par conséquent, comme nous essayons de modéliser le langage, les matrices associées aux graphes ou aux réseaux d'informations sont souvent des matrices creuses. En effet, le manque de données ainsi que leur grande dimensionnalité sont des problèmes qui affectent les performances des approches de découverte de connaissances \cite{mining12Book, PerinetH15} appliquées aux données textuelles.

Une matrice de données éparse (ou creuse) a la plupart de ses entrées égales à zéro. Ainsi, dans notre contexte, la majorité des mots (lignes) dans le corpus sont décrits par très peu de contextes (colonnes) mais qui sont tout à la fois en grand nombre. Ceci est un problème important car pendant la phase de découverte de connaissances de tout système TALN, nous visons à former un modèle d'apprentissage qui finira par prédire, classifier, grouper les mots d'une manière ou d'une autre. Si les mots sont représentés par un nombre limité de contextes, les algorithmes d'apprentissage ne pourront pas généraliser correctement.

Une représentation textuelle explicite et distributionnelle est donc éparse. Il y a beaucoup de mots dans un corpus de documents et même si certains mots apparaissent dans plusieurs textes, tous les mots du corpus ne peuvent pas être présents dans un même texte. Cela devient un problème important avec les systèmes de TALN : les mots ne sont décrits que par un petit nombre de fonctionnalités. Dans ce qui suit, nous décrivons nos deux premières propositions qui traitent de l'utilisation d'informations hétérogènes pour représenter un terme et atténuer le manque de données qui accompagne ces types de représentations textuelles.

%Without regards to their type, network-based structures are ultimately transformed into matrices before being treated computationally. Therefore, given that we are still modeling language (words), graphs suffer from sparsity just as vector space models. Indeed, data sparsity is an issue that affects the performance of knowledge discovery approaches \cite{mining12Book,PerinetH15} applied to textual data.
%
%A sparse data matrix has most of its entries equal to zero. Thus, the majority of the words (rows) in the corpus are described by very few contexts (columns). This is a significant problem as on the knowledge discovery phase of any TALN system we aim to train a learning model that will eventually predict, classify, group our words in one way ot another. If the words are represented by a limited number of contexts, the learning algorithms will not be able to generalize properly. 
%%\subsection{Conclusion}
%
%Whether its vector based or graph-based, a textual, explicit, and distributional representation will be sparse. There are too many words in a text and its assured that, while they could occur in other texts, they will not occur in a single text. This becomes an important problem with TALN systems: words are described by only a small set of features. In the following describe our two first propositions which address the issue of using heterogeneous information to represent a term and alleviating the data sparsity that comes with such types of textual representations.


\section{Modèle linguistique basé sur des hypergraphes et enrichi par la fusion}
%The first two contributions of this thesis are contained in a fusion enriched hypergraph linguistic model proposition. The model  consists on two components which address two research questions each: the issue of making sparsity less severe and leveraging different types of features  by using a single feature representation space. 
%
%The model we present here entails three  important characteristics: firstly, the possibility to leverage different types of information.  Secondly, as the words will be linked together, there is an inner structure that will emerge from the model and which we exploit in our experiments. Thirdly, given that we treat unstructured text data, the relations (or features)  between words are sparse, this is alleviated by combining features via fusion techniques.  The three of them are addressed with our propositions. 
%
%Our network is based on the distributional hypothesis, as described in the previous chapter.  As co-occurrence features, we select both lexical and syntactic contexts, indeed creating a linguistic resource that hold both types of information in order to get a complementary insight of words' relations. 
%
%
%In the literature, regarding WSD approaches, we see that the use of a lexical knowledge base, such as Wordnet\footnote{\url{https://wordnet.princeton.edu}}, is pervasive in this task. On the other hand, WSI, while being a more flexible approach (language and word-usage independent, does not require human-made bases)  for solving WSD, its results are tightly linked to the quality of the clustering algorithm used. 
%% 
%
%With respect to the networks' modelization, we find that few approaches deal with syntactic attributes. We believe that finding semantic similarities can be improved by adding syntactic information not only  while using dependency relations but also by leveraging the constituency tree of each word. Moreover, using syntactic data along with semantic and/or lexical co-occurrences takes us into the heterogeneous network domain which has not been addressed in most of the approaches covered.  
%
%
%Taking into account the described opportunities of research, in the following section we propose a  hypergraph modelization of a linguistic network that aims to solve some limitations stated above. 

Les deux premières contributions de cette thèse sont contenues dans la proposition d'un modèle linguistique basé sur des hypergraphes et enrichi avec des techniques de fusion. Ainsi, le modèle traite les deux premières questions de recherche exposées précédemment : l'exploitation de différents types de caractéristiques en utilisant un espace de représentation unique et qui, en même temps, permet de réduire l'eparsité des données caractéristique aux données textuelles.

Le modèle que nous présentons ici comporte trois caractéristiques importantes: premièrement, la possibilité de tirer profit des différents types d'informations textuelles par le biais de techniques de fusion visant à exploiter la complémentarité entre caractéristiques linguistiques distinctes. Deuxièmement, les relations de similarité sémantique se trouvant renforcées, nous pouvons exploiter la structure interne enrichie du réseau de similarité entre mots par le biais d'outils d'analyse de graphe. Troisièmement, notre modèle par l'utilisation de méthode de fusion, permet également de tenir compte du problème de l'éparsité de données en dépit d'une dimensionalité grandissante causée par l'hétérogénéité des descripteurs. 

%% Notre modèle est basé sur l'hypothèse distributionnelle, telle que décrite précédemment. 
En tant que caractéristiques de co-occurrence de mots, nous sélectionnons à la fois des contextes lexicaux et syntaxiques, créant ainsi une ressource linguistique qui contient les deux types d'informations afin d'obtenir des vues complémentaires des relations entre les mots.

%Dans la littérature, nous voyons que l'utilisation d'une base de connaissances lexicales, telle que Wordnet \footnote{\url{https://wordnet.princeton.edu}}, est omniprésente dans cette tâche. D'autre part, WSI, tout en étant une approche plus flexible (langage et utilisation de mots indépendants, ne nécessite pas de bases artificielles) pour résoudre WSD, ses résultats sont étroitement liés à la qualité de l'algorithme de clustering utilisé.
%

En effet, dans la littérature, nous constatons que peu d'approches traitent les attributs syntaxiques. Nous pensons que la recherche de similitudes sémantiques peut être améliorée en ajoutant des informations syntaxiques non seulement lors de l'utilisation de relations de dépendance, mais aussi en exploitant l'arbre de constituants de chaque mot. En outre, l'utilisation de données syntaxiques avec des co-occurrences sémantiques et/ou lexicales nous conduit à avoir un réseau d'informations hétérogènes enrichi, ce qui n'est pas le cas dans la plupart des approches existantes.

En prenant en compte les opportunités de recherche décrites ci-dessus, nous proposons une modélisation  d'un réseau linguistique basée sur un hypergraphe hétérogène que nous enrichissons par l'utilsation de méthodes de fusion qui peuvent être vues telles des techniques d'enrichissement de liens sommets-sommets et/ou sommets-hyperarêtes.

%\subsection{Proposed Model: Fusion Enriched Hypergraph Linguistic Network}

%As stated before, our model consists on two parts (and two contributions). The first one, an hypergraph model that holds different types of linguistic relations extracted from a corpus. And the second one, the combination of linguistic features in order to generate a less sparse, enriched representation. 
%
%Our model is  based on the use of a hypergraph. Its single most important difference with regular graphs, is being able to relate more than two vertices at the same type, which allows for a better characterization of interactions within a set of individual elements (in our case, words) \cite{heintz2014beyond}. Indeed, our hypergraph modelization initially integrates four types of relations between tokens: sentence co-occurrence, part-of-speech tags, words' constituents data and dependency relations in a single linguistic structure. These relationships were chosen because it is relatively easy to obtain them for high-resource languages. These features can be seen as building blocks for TALN models. In any case, our goal is to arrive to more complex annotations (e.g., named entities) from the selected features and relations. We decided to keep a lexical context at sentence level, so that it may complement the distributional semantic information provided by the dependency functions context as well as the phrase-constituency syntactic context. In short, we  aim to cover three levels of possible semantic relatedness via  three levels (in terms of the size of the neighborhood of a target word) of distributional co-occurrences: a short range with dependency functions, a medium range with phrase constituency membership, and a longer range with sentence lexical co-occurrences. The intuition is that when solving TALN tasks, having direct access to these three semantic spaces will help to determine a more appropriate meaning's relation between words. 
%
%The second part of our proposed method deals with the fusion of textual features. Namely, we combine the features that describe terms into a single representation space. This new space aims to address two issues that arise while working with textual data: effectively using information coming from different linguistic levels (e.g., lexical, syntactic, semantic) while alleviating the sparsity typical of textual representations. In the multimodal fusion literature we can discern two main common types of techniques: early fusion and late fusion. A third and fourth type of fusion methods, cross-media fusion and hybrid fusion are also employed in multimedia analysis tasks. 
%
%These four fusion operators naturally address the issue of dealing with heterogeneous data as they all mix one way or another the feature columns from each of two representations. Regarding alleviating sparsity, the intuition is that by combining matrices either by summing or element-wise multiplying them, the resulting matrix will have a denser structure. For example, by summing two matrices with the same shape, such as two term-term similarity matrices, we  obtain a resulting matrix that contains the similarities of both feature spaces. In the same sense, when multiplying two matrices we combine them while also obtaining a denser output matrix. Nonetheless, both sum and multiplication result depends evidently on the nature of the matrices employed. 
%
%In order to materialize our proposed linguistic model we implemented a procedure that takes a corpus as input and outputs the linguistic resource we introduced in the previous section. We based our process on the online encyclopedia Wikipedia\footnote{\url{https://en.wikipedia.org}} which has been used as a source of valuable data as well as a common background corpus to solve diverse TALN/TM related tasks. 


Comme indiqué précédemment, notre proposition consiste en deux parties. La première, un modèle à base d'hypergraphes qui contient différents types de relations linguistiques extraites d'un corpus. Et la seconde, la combinaison de caractéristiques linguistiques afin de générer une représentation moins creuse et enrichie, à travers des techniques de fusion.

Notre modèle est basé sur l'utilisation d'un hypergraphe. La différence la plus importante avec les graphes classiques est de pouvoir relier plus de deux sommets en même temps, ce qui permet une meilleure caractérisation des interactions au sein d'un ensemble d'éléments individuels (dans notre cas, les mots) \cite{heintz2014beyond}. En effet, notre modélisation à base d'hypergraphes intègre initialement quatre types de relations entre les mots: la co-occurrence de phrase, les étiquettes morpho-syntaxiques, la structure syntaxique, et les relations de dépendance dans une structure linguistique unique. Ces relations ont été choisies car il est relativement facile de les obtenir pour les langues où il y a beaucoup de ressources linguistiques. Ainsi, ces caractéristiques peuvent être considérées comme des blocs de construction pour les modèles de TALN. Dans tous les cas, notre but est d'arriver à des annotations plus complexes (par exemple, des entités nommées) à partir des caractéristiques et des relations pertinentes.

Nous avons décidé de garder le contexte lexical au niveau de la phrase, de sorte qu'il puisse compléter l'information sémantique fournie par le contexte basé sur les relations de dépendance ainsi que le contexte syntaxique de la phrase. En bref, nous visons à couvrir trois niveaux de relation sémantique: un niveau proche avec les fonctions de dépendance, un niveau moyen avec une appartenance à une phrase nominale (avec la structure syntaxique), et un niveau plus long avec la coexistence lexicale de phrase, c-à-d, le voisinage des mots à niveau de toute la phrase. L'intuition est que lors de la résolution de tâches de TALN, avoir un accès direct à ces trois espaces sémantiques aidera à déterminer une relation de signification plus appropriée entre les mots.

La deuxième partie de notre méthode traite de la fusion des caractéristiques textuelles. À savoir, nous combinons les caractéristiques qui décrivent les termes en un seul espace de représentation unique. Ce nouvel espace vise à répondre à deux problèmes qui se posent lors de l'utilisation de données textuelles : comment utiliser efficacement des informations provenant de différents niveaux linguistiques (par exemple, lexical, syntaxique, sémantique) tout en allégeant l'eparsité typique dans les représentations textuelles. 

Dans la littérature sur la fusion multimodale, nous pouvons discerner deux types principaux de techniques : la fusion précoce et la fusion tardive. Un troisième et un quatrième type de fusion, fusion croisée multimédias et fusion hybride sont également utilisés dans des tâches d'analyse multimédia. Nous utilisons ces techniques pour enrichir le modèle.

Ces quatre types de fusion abordent naturellement la question du traitement de données hétérogènes car ils mélangent tous d'une manière ou d'une autre les colonnes de caractéristiques de chacune des deux représentations (dans le contexte d'une matrice de représentation). En ce qui concerne la réduction du taux des données creuses, l'intuition est qu'en combinant les matrices soit en les sommant, soit en les multipliant par éléments, la matrice résultante aura une structure plus dense. Par exemple, en additionnant deux matrices ayant la même forme, telles que deux matrices de similarité terme à terme, nous obtenons une matrice résultante qui contient les similarités des deux espaces caractéristiques. Dans le même esprit, en multipliant ces deux matrices, nous les combinons tout en obtenant une matrice de sortie plus dense. Néanmoins, le résultat de la somme et de la multiplication dépend évidemment de la nature des matrices utilisées.

Nous appliquons ces opérateurs de fusion à notre modèle basé sur les hypergraphes et nous obtenons un modèle unique qui réponde aux questions posées initialement.

Afin de matérialiser le modèle linguistique proposé, nous avons mis en place une procédure qui prend en compte un corpus et produit la ressource linguistique que nous venons d'introduire dans les paragraphes précédents. Nous avons basé notre processus sur l'encyclopédie en ligne Wikipédia \footnote{\url{https://fr.wikipedia.org}} qui a été utilisée comme une source de données importante ainsi qu'un corpus de fond commun pour résoudre divers tâches du TALN et du text mining.

\section{Applications à la reconnaissance d'entités nommées et à la désambiguïsation des sens des mots}

%\subsection{Introduction}
%
%In this application's chapter we set to solve two natural language processing tasks using as data source corpora in the form of our proposed model. We address the tasks of Named Entity Recognition (NER) and Word Sense Induction and Disambiguation (WSI/WSD). We employ both a fusion enriched and a raw hypergraph network based on benchmark corpora to validate the utility of our proposals.

Dans cette section, nous avons décidé de résoudre deux tâches de traitement du langage naturel en utilisant comme corpus, des données sous la forme de notre modèle . Nous abordons les tâches de reconnaissance d'entités nommées et d'induction et de désambiguïsation des sens des mots (WSI / WSD). Nous utilisons un réseau basé sur des hypergraphes enrichis, réseau construit sur des corpus de référence, pour valider l'utilité de nos propositions.

\subsection{Première application: reconnaissance d'entités nommées}

%NER goal is to automatically discover, within a text, mentions that belong to a well-defined semantic category. The classic task of NER involves detecting, within a text, entities of type Location (LOC), Organization (ORG), Person (PER), Miscellaneous (MISC), or if the term is not an even an entity, assigning them a (O) label. The task is of great importance for more complex TALN systems, e.g, relation extraction, opinion mining \cite{nadeau2007survey}.  Generally, the common solution to NER involves training a supervised machine learning algorithm with large quantities of annotated text \cite{mining12Book}. As is usual with other TALN tasks, NER  requires textual features to represent words in order to determine their role within a phrase. We propose to build representations based on our fusion enriched hypergraph model. For the main experiments, we chose a structured perceptron learning algorithm because of its performance and its lower training time. 
%
%We experiment with the four levels of fusion on three different datasets. The representation matrices for NER come from lexical context features $\mlex$, syntactical context features or other task-standard features.  On the other hand, experiments on WSI/WSD exclusively employ lexical and syntactic matrices. Our main goal is to compare the efficiency of the primary  fusion techniques applied to   named entity recognition. Then, we empirically determine a fusion combination operator able to leverage the complementarity of the features used.
%We discover that using a recombination of several fusion operations, a so-called hybrid approach, we  improve over using the features individually and also over the use of the trivial early fusion operator. This indicates that the single feature matrix, enriched with other combined features, is enough to improve the results of said baselines. In general, in our experiments with NER, we see that the added enriched features are not the most important for the learning algorithm while taking a decision, nonetheless, they  provide the extra information needed to push the model towards the correct prediction, by enriching the features through cross and late fusion and by providing more descriptors for each word and consequently reducing the sparsity of the representation matrices.
%
%Once we found a set of fusion operations that work reasonably well with NER, we experiment with another task, word sense induction and disambiguation, to confirm the usefulness of using fusion enriched representations to train better models.

Le but de la reconnaissance d'entités nommées est de découvrir automatiquement, dans un texte, des mentions appartenant à une catégorie sémantique bien définie. La tâche classique consiste à détecter, dans un texte, des entités de type Lieu (LOC), Organisation (ORG), Personne (PER), Divers (MISC) ou, si le terme n'est pas une entité nommée, lui attribuer une étiquette appropriée (en l'occurrence, l'étiquette O). La tâche est d'une grande importance pour les systèmes de TALN plus complexes comme par exemple, l'extraction de relations ou la fouille d'opinion \cite{nadeau2007survey}. Généralement, la solution pour la reconnaissance d'entités nommées consiste à entraîner un algorithme d'apprentissage automatique supervisé avec de grandes quantités de texte annoté \cite{mining12Book}. Comme pour d'autres tâches de TALN, la reconnaissance d'entités nommées nécessite des représentations (ou \textit{features}) pour les mots afin de déterminer leur rôle dans une phrase. Nous proposons de construire ces représentations en nous basant sur notre modèle à base d'hypergraphes enrichi par fusion. Pour les expériences, nous avons choisi un algorithme d'apprentissage de type perceptron structuré en raison de ses performances et de son temps de d'entraînement réduit.

Nous expérimentons les quatre niveaux de fusion sur trois ensembles de données différents. Les matrices de représentation pour la reconnaissance d'entités nommées proviennent des caractéristiques contextuelles lexicales, des caractéristiques des dépendances syntaxiques ou d'autres caractéristiques dites standard pour la tâche en question. Notre objectif principal est de comparer l'efficacité des techniques de fusion primaires appliquées à la reconnaissance d'entités nommées. Aussi, nous déterminons empiriquement un opérateur de combinaison de fusion capable de tirer parti de la complémentarité des caractéristiques utilisées.
%
Nous découvrons qu'en utilisant une combinaison de plusieurs opérations de fusion, une approche dite hybride, nous améliorons l'utilisation des caractéristiques individuellement et aussi sur l'utilisation de l'opérateur de fusion précoce trivial, qui consiste à la simple concaténation des caractéristiques dans un espace unique. Ceci indique que la matrice de caractéristiques unique, enrichie avec d'autres caractéristiques combinées, est suffisante pour améliorer les résultats desdites \textit{baselines}. En général, dans nos expériences pour la reconnaissance d'entités nommées, nous voyons que les fonctionnalités enrichies ajoutées ne sont pas les plus importantes pour l'algorithme d'apprentissage lors de la prise d'une décision; néanmoins, elles fournissent des informations supplémentaires nécessaires pour pousser le modèle vers la prédiction correcte, en enrichissant les caractéristiques à travers la fusion croisée et tardive, en fournissant plus de descripteurs pour chaque mot et par conséquent en réduisant ainsi le niveau des données creuses dans les matrices de représentation.

Une fois que nous avons trouvé un ensemble d'opérations de fusion qui fonctionnent raisonnablement bien pour la reconnaissance d'entités nommées, nous expérimentons avec une autre tâche du TALN, la désambiguïsation lexicale, afin de  confirmer l'intérêt d'utiliser des représentations enrichies par fusion.


\subsection{Deuxième application: induction et désambiguïsation de mots}
\subsubsection{Représentations enrichies par fusion}

%\subsection{Second Application: Word Sense Induction and Disambiguation}
%\subsubsection{Fusion Enriched Representations}


%Having learned the best fusion configuration from the NER task, in these experiments we set to test if the improvements achieved can be transfered into another TALN task, namely Word Sensed Induction and Disambiguation (WSI/WSD). As preprocessing, we simply remove stopwords and tokens with less than three letters. The features we extracted from the tested corpora with the same tools as in the previous task.
%Word Sense Induction and Disambiguation entails two closely related tasks. WSI aims to automatically discover the set of possible senses for a target word given a text corpus containing several occurrences of said target word. Meanwhile, WSD takes a set of possible senses and determines the most appropriate sense for each instance of the target word according to the instance's context. WSI is usually approached as an unsupervised learning task, i.e., a cluster method is applied to the words occurring in the instances of a target word. The groups found are interpreted as the senses of the target word. The WSD task is usually solved with knowledge-based approaches, or more recently, with supervised models which require annotated data. It can be also solved reasonably well by comparing the words surrounding each target word and the words belonging to the induced senses (or clusters) found during the WSI step, as we do in this section.
%
%We employ spectral clustering on the input matrices in order to automatically discover senses (a cluster is considered a sense).  Regarding sense disambiguation, we trivially assign senses to the target word instances according to the number of common words in each cluster and the context words of the target word. In other words, for each test instance of a target word, we select the cluster (sense) with the maximum number of shared words with the current instance context.
%
%In order to determine the best performing operations, that stray away from the trivial baselines, we propose a measure to help us identify those systems that perform the best. According to this metric, we find the combination of fusion operators is the best performing combination of features. Indeed, by transferring quality lexical similarities into its same feature matrix, we obtain more useful relations than by using any syntactic data. While the same operators that outperformed the rest in NER are not as adequate in this WSI/WSD experiment, we see that most of the feature combination techniques improve over the baselines of the single features and early fusion operations. 

Après avoir appris la meilleure configuration de fusion à partir de la tâche de reconnaissance d'entités nommées, nous avons testé dans les expériences qui suivent si les améliorations obtenues peuvent être transférées à une autre tâche de TALN, à savoir, l'induction et la désambiguïsation des sens des mots.

L'induction (Word Sense Induction, WSI) et la désambiguïsation (Word Sense Disambiguation, WSD) des sens des mots impliquent deux tâches étroitement liées. Le WSI vise à découvrir automatiquement l'ensemble des sens possibles d'un mot cible donné relativement à un corpus de textes contenant plusieurs occurrences dudit mot cible. D'un autre côté, le WSD prend un ensemble de sens possibles pour un mot et cherche à déterminer le sens le plus approprié pour chacune des instances du mot cible en fonction de son contexte. Le WSI est généralement traité comme une tâche d'apprentissage non supervisée, c-à-d qu'une méthode de \textit{clustering} est appliquée aux mots apparaissant dans les contextes des instances d'un mot cible. Puis, les groupes trouvés sont interprétés comme les differents sens du mot cible. La tâche WSD est généralement résolue avec des approches basées sur des graphes de connaissances, ou plus récemment, avec des modèles supervisés qui nécessitent des données annotées. Il peut également être résolu raisonnablement bien en comparant les mots entourant chaque mot cible et les mots appartenant aux sens induits trouvés lors de l'étape WSI, comme nous le faisons dans cette section.

Nous utilisons un \textit{clustering} spectral sur les matrices d'entrée afin de découvrir automatiquement les sens. En ce qui concerne la désambiguïsation de ces derniers, nous affectons trivialement les sens aux instances de mots cibles en fonction du nombre de mots communs dans chaque groupe et les mots du contexte du mot cible. En d'autres termes, pour chaque instance de test d'un mot cible, nous sélectionnons le groupe (sens) avec le nombre maximum de mots partagés avec le contexte de l'instance actuelle.

De plus, afin de déterminer les systèmes les plus performants, éloignés des \textit{baseline} triviales, nous proposons une mesure pour identifier les solutions les plus performantes. Selon cette métrique, nous trouvons que la combinaison des opérateurs de fusion est, à nouveau, la combinaison de caractéristiques la plus performante. En effet, en transférant des similarités lexicales de qualité dans la même matrice de caractéristiques, nous obtenons des relations plus utiles qu'en utilisant des données syntaxiques. Alors que les mêmes opérateurs qui ont surpassé les autre approches dans le cas de la reconnaissance d'entités nommées ne sont pas aussi adéquats dans cette expérience WSI/WSD, nous voyons que la plupart des techniques de combinaison améliorent les \textit{baseline} à base de caractéristiques uniques et des opérations de fusion précoce.


%\subsubsection{Leveraging the Linguistic Network Structure}
\subsubsection{Tirer parti de la structure du réseau linguistique}
%
%We leverage the relations that exist within the network to identify words that, together with their neighborhood, represent a sense. Thus, we propose a network-based algorithm to solve word sense induction. 
%Our method  is inspired on previous approaches from both \cite{2004.Veronis} and \cite{2007.Klapaftis.UoY}. In Hyperlex,  the graph-based  method presented  in \cite{2004.Veronis}, the main intuition is that co-occurrence networks have small-world properties and thus it is possible to detect and isolate important heavily-connected nodes, called "hubs". The idea is that these hubs, and their connected nodes, represent a sense themselves. 
%We apply our method to a well-known WSI/WSD dataset. We also use feature spaces produced with fusion operators.
%
%Using our network-based approach, we obtained results that surpassed those from the similar methods while being more flexible in terms of use of parameters. Again, we found that using fusion operators yield better results compared to using single features or early fusion. While our fusion systems beat these baselines, the systems that perform the best do not employ heterogeneous data to do so. Indeed, the best systems that combine the two possible types of features lag behind the best fusion spaces. 


Nous tirons parti des relations qui existent au sein du réseau pour identifier les mots qui, avec leur voisinage, représentent un sens. Ainsi, nous proposons un algorithme basé sur le réseau pour résoudre l'induction des sens des mots.
Notre méthode est inspirée des approches précédentes de \cite {2004.Veronis} et de \cite{2007.Klapaftis.UoY}. Dans Hyperlex, méthode basée sur les graphes \cite{2004.Veronis}, l'intuition principale est que les réseaux de co-occurrences ont des propriétés dites de ``petit monde'' et il est donc possible de détecter et d'isoler les n\oe uds importants connectés, appelés \textit{hubs}. L'idée est que ces \textit{hubs}, et leurs n\oe uds connectés, représentent eux-mêmes des sens. Nous utilisons également des espaces de représentation produits par des opérateurs de fusion.

En utilisant notre structure linguistique basée sur les hypergraphes, nous avons obtenu des résultats qui surpassaient ceux des méthodes similaires tout en étant plus flexibles en termes d'utilisation des paramètres. Encore une fois, nous avons constaté que l'utilisation d'opérateurs de fusion donne de meilleurs résultats par rapport à l'utilisation de caractéristiques uniques ou fusion précoce. Cependant, alors que nos systèmes de fusion donnent de meilleurs résultats que les \textit{baseline}, les systèmes qui fonctionnent le mieux n'emploient pas de données hétérogènes. En effet, les meilleurs systèmes qui combinent les deux types de fonctionnalités possibles donnent de moins bons résultats que les meilleurs espaces de fusion basée sur une information homogène.

\section{Conclusion et travaux futurs}
\subsection{Conclusion}
%Linguistic Networks are useful methods to understand the nature of our language. In the literature, they are generally used to comprehend either the dynamics of words and other textual units within language, and to solve practical TALN tasks. Nonetheless, no mater the objective, they are usually based on the distributional hypothesis, that is, words will be found in similar contexts if they tend to be semantically related. On the other hand, text data representations, described through contexts in a distributional framework, are sparse by nature: the large majority of the entries in a co-occurrence matrix are zero. To treat these concerns, on this thesis we proposed three contributions. The first and second entail a fusion enriched linguistic network,  which entails denser text representations by combining heterogeneous feature spaces. The second is a method based on graph structure to find groups of related words.
%
%With regard to our fusion techniques, we tested them over both WSI/WSD and NER tasks. Particularly, in NER, we created new representation matrices that showed overall improvement in performance. In order to get to these improvements, which are consistent in the whole ensemble of datasets tested, we performed a high level of fusion aggregation. Concerning our graph-based model, we tested it on the WSI/WSD task, over the Semeval 2007 corpus. Using the free-scale presumption we found communities of words describing senses by using sentence-level lexical contexts and raw frequencies to weight the co-occurrences. The fusion operators produced representation spaces that improved over using single features, as in NER experiments.
%
%Finally, the proposed hypergraph, through its fusion representations, generate large matrices that need to be correctly manipulated in order to solve TALN tasks. To address this challenge, we use simple solutions as simple as word filtering to more complex approaches that computationally deal with large, sparse, and dense, spaces, such as parallelization and out-of-core computing methods\footnote{Algorithms that only keep in memory the required parts of a matrix during computations, keeping the rest on the hard drive}.
Les réseaux linguistiques sont des structures utiles pour comprendre la nature de notre langue. Dans la littérature, ils sont généralement utilisés pour comprendre la dynamique des mots et d'autres unités textuelles dans le langage, et pour résoudre des tâches pratiques de TALN. Néanmoins, quel que soit l'objectif, ils sont généralement basés sur l'hypothèse distributionnelle, c'est-à-dire que les mots seront trouvés dans des contextes similaires s'ils ont tendance à être liés sémantiquement. D'un autre côté, les représentations de données textuelles, décrites par des contextes dans un cadre distributionnel, sont rares par nature: la grande majorité des entrées dans une matrice de co-occurrence sont nulles. Pour traiter ces problématiques, nous avons proposé trois contributions dans cette thèse. Le premier et le second impliquent un réseau linguistique enrichi par des techniques de fusion, qui aboutit à des représentations de texte plus denses en combinant des espaces de caractéristiques hétérogènes. La seconde est une méthode basée sur la structure d'un graphe pour trouver des groupes de mots apparentés.

En ce qui concerne les techniques de fusion, nous les avons testées sur deux tâches: WSI/WSD et la reconnaissance d'entités nommées. En particulier, dans cette dernière, nous avons créé de nouvelles matrices de représentation qui ont montré une amélioration globale des performances. Afin d'obtenir ces améliorations,  nous avons effectué un niveau élevé d'agrégation de fusion en combinant différents types de fusion. Concernant notre modèle basé sur la structure du réseau linguistique, nous l'avons testé sur la tâche WSI/WSD. En supposant l'hypothèse \textit{scale-free}, nous avons trouvé des communautés de mots décrivant des sens en utilisant des contextes lexicaux au niveau des phrases avec des fréquences brutes pour pondérer les co-occurrences. Les opérateurs de fusion ont produit des espaces de représentation améliorés par l'utilisation de caractéristiques uniques, comme dans les expériences NER.

Enfin, l'hypergraphe proposé, à travers ses représentations de fusion, génère de grandes matrices qui doivent être correctement manipulées en machine afin d'améliorer le traitement du point de vue computationnel. Pour relever ce défi, nous utilisons des approches telles que la parallélisation et les méthodes de calcul dites out-of-core\footnote{En bref, il s'agit d'algorithmes qui stockent dans la mémoire vive que les parties d'une matrice requises pour le calcul et qui gardent le reste sur le disque dur}.


\subsection{Travaux futurs}
%Concerning fusion techniques, a more principled way to determine what type of context with what type of fusion operation would indeed reduce the need for exploring the whole space of possibilities. Finally,  comparing said methods with other well-established dimension reduction approaches would be interesting to understand the trade-offs of lower performance versus dimension reduction, while focusing on not-so-large corpora. Indeed, if the new wave of distributional representations, or word embeddings, has a shortcoming is that empirically it does not perform as well on smaller corpus. This may represent an avenue of opportunity to methods such as feature fusion functions.
%
%Regarding the network-based algorithm for WSI/WSD, a deeper errors' analysis would deep a larger glimpse on the behavior of nouns and verbs according to the context. Understanding what is the syntactic or lexical difference among contexts, which induce the good or bad performance of each type of feature could make the system more flexible to other text domains. Also, the hypergraph could be better leveraged by using hypergraph-specific methods, mainly through spectral analysis.
En ce qui concerne les techniques de fusion, nous avons montré qu'il existait des approches hybrides permettant d'améliorer les résultats mais il serait nécessaire d'analyser plus finement quels types d'opérateurs de fusion et quels types de combinaisons hybrides seraient les plus efficaces afin de limiter l'espace des possibilités. Par ailleurs, la comparaison de ces méthodes avec d'autres approches de réduction de dimension bien établies serait intéressantes pour comprendre les compromis entre la réduction des performances et la réduction des dimensions, tout en se concentrant sur les corpus moins importants. En effet,  les techniques récentes de représentations distributionnelles, ou de plongements de mots, ont un inconvénient majeur: elles ne fonctionnent pas très bien sur les corpus de petites tailles. Les méthodes de fusion de caractéristiques comme celle que nous avons utilisées pourraient apporter des réponses intéressantes dans ce cas.

D'une autre part, en ce qui concerne l'algorithme basé sur le réseau pour WSI/WSD, une analyse plus approfondie des erreurs permettrait un plus grand aperçu du comportement des noms et des verbes en fonction du contexte. Comprendre quelle est la différence syntaxique ou lexicale entre les contextes, qui induisent les bonnes ou mauvaises performances de chaque type de fonctionnalité, pourrait rendre le système plus flexible à d'autres domaines et d'autres tâches de TALN. En outre, l'hypergraphe pourrait être mieux exploité en utilisant des méthodes hypergraphes, principalement par l'analyse spectrale.


\bibliographystyle{ThesisStyle}
\bibliography{Thesis_f}

\end{document}
