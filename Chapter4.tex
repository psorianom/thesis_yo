\chapter{Applications to named entity recognition and word sense disambiguation} 
\label{chap:wsd}
\begin{abstractchap}
This chapter presents the experiments we performed as applications of the proposed model. On the first subsection, we use well-known methods to solve named entity recognition while using fusion enriched representation spaces. We show that these kinds of representations, leveraging heterogeneous information and alleviating its data sparsity, are useful to improve the performance of the task. Indeed, our results on three different datasets using enriched representations are better than those of the baselines we propose, and more importantly, our results show that the combination of textual features indeed improves the performance compared to single feature and the trivial feature concatenation. We also give a detailed analysis into how the fusion operations get to improve the performance of the task at hand.

In the second subsection, we change the NLP task to word sense induction and disambiguation. First, we apply the same fusion operations as before to solve the task using an existing literature approach. Our experiments on two different corpora show that the improvements shown in named entity recognition are consistent in word sense induction and disambiguation. Secondly, we propose a method to exploit the structure of the network within our linguistic model. 
Although this approach has been studied before, we improve over the previous  literature results while having a reduced number of parameters and employing heterogeneous features to solve the task. We also analyze the results obtained according to the  type of word studied, whether nouns or verbs, and according to the effect of the use of either lexical or syntactical information.


%Secondly, in the second section of the chapter, we explore the use of well-known multi-modal fusion techniques to solve two prominent Natural Language Processing tasks. Specifically, we focus on solving Named Entity Recognition and Word  Induction and Disambiguation by applying feature-combination methods that have already shown their efficiency in the multi-media analysis domain. We present a series of experiments employing fusion techniques in order to combine textual linguistic features. 
%Furthermore, we perform an extensive analysis on the importance of each feature relevance with respect to the senses and classes discovered in WSI/WSD and NER, respectively.

\end{abstractchap}

\minitoc
\section{Introduction}
In this applications chapter we set to solve two natural language processing tasks using as data source corpora in the form of the model described in Chapter \ref{chap:ling_net}. We address the tasks of Named Entity Recognition (NER) and Word Sense Induction and Disambiguation (WSI/WSD). Both tasks are located on the  semantics sub-domain of NLP. 

These experiments represent the third and final contribution of this thesis, after introducing the theoretical fusion enriched model in the previous chapter. Indeed, this contribution is the continuation of our set of propositions, as shown in Figure \ref{fig:main_diag2}. We employ both a fusion enriched and a raw hypergraph network based on benchmark corpora to validate the utility of our proposals.

The general objectives of the experiments described below are two: (1) to test the effectiveness of using fusion enriched representations to solve NLP tasks, while combining heterogeneous information and densifying the feature space;  and (2), to leverage the structure of a network built using the hypergraph structure described before.

 %By making use of the network we want to test the effectiveness of using different types of linguistic features. 
There are two main parts  in this chapter. First we address NER, we study how the different types of fusion operations affect the performance of the task. We train well-known classification algorithms with representations obtained from fusion operations.  According to our results, we find that it is indeed interesting to combine different types of features into a single representation space. We also delve into a result's analysis to try to understand the reason behind the improvement using fusion techniques.

The second part deals with WSI/WSD. This subsection is divided in two segments. First, once we determined that combining features is interesting for NER, we want to verify that this combined representations are indeed useful for other NLP tasks, such as WSI/WSD. As before, we use a literature learning technique with the representation obtained by the combination of available features. Our results show that the fusion enrichment is useful also to solve WSI/WSD.

Secondly, we leverage the structure within the hypergraph resource to solve WSI/WSD. Briefly, the goal is to detect important words, according to their connections with other words, and extract their neighborhood to determine possible senses  within the network. While this method has been used before, we improve the performance and reduce the number of parameters employed by those literature methods. We also study the effect of using different types of linguistic information, namely lexical and syntactic contexts.

At the end of the chapter we conclude by discussing the experiments performed, their limitations and  future improvements and further research. 

\begin{figure}
\centering
\includegraphics[width=.9\linewidth]{images/Chapitre4/main_diag2.pdf}
\caption{Complete description of the three contributions of this work. In this chapter we focus on the third contribution, addressing WSD/WSI and NER with the model proposed in the previous chapter.}
\label{fig:main_diag2}
\end{figure}

\section{First Application: Named Entity Recognition}
\label{sec:ner}


 NER goal is to automatically discover, within a text, mentions that belong to a well-defined semantic category. The classic task of NER involves detecting, within a text, entities of type Location (LOC), Organization (ORG), Person (PER), Miscellaneous (MISC), or if the term is not an even an entity, assigning them a (O) label. The task is of great importance for more complex NLP systems, e.g, relation extraction, opinion mining \cite{nadeau2007survey}.  
 
 Generally, two common solutions to NER involve the use of matching patterns, created manually or extracted in a semi-automatically fashion\cite{gupta2015distantly}; or more popularly, by training a supervised machine learning algorithm with large quantities of annotated text \cite{mining12Book} . The latter being the currently more popular solution to this task. As is usual with other NLP tasks, NER  requires textual features to represent words in order to determine their role within a phrase. We propose to build representations based on our fusion enriched hypergraph model.


Usually, representations employed for NER are obtained from the surrounding context of the words in the  input corpus. Mainly,  two types of representations are used: lexical and syntactic. As we know, the first type requires no extra information than that contained already in the analyzed text itself. The second type, syntactic features  are based on part of speech tags, phrase constituents information, and syntactical functionality between words, the later portrayed by syntactical dependencies. Likewise, there are specific features that are particular to one task are also be employed.


The main intuition of these experiments is that word similarities may be found at different levels according to the type of features employed. In order to exploit these similarities, we leverage our fusion enriched framework. Specifically, in our experiments, we try to mutually complement independent representations by utilizing said fusion techniques to generate a single feature space that improves the performance of NER, specially compared to the using features independently and the trivial feature concatenation (early fusion). Consequently, the main goal is to assess the effectiveness of simple, yet untested (as far as we know) fusion techniques and their combination.

%The rest of the chapter is organized as follows: in section \ref{chap6:back}, we go into further details about fusion techniques. 
%%as well as its application in the NLP domain. 
%We introduce the fusion operators that we use in our experiments in section \ref{chap6:application}. Then, in section \ref{chap6:expes} we show the effectiveness of the presented methods by testing them on NER and WSI/WSD and their respective datasets. Finally, in section \ref{chap6:conclusion} we present our conclusions and future directions to explore. A more general overview on fusion for multimedia analysis can be found in \cite{AtreyHEK10}.


We consider the first three types of fusion techniques described in subsection \ref{sec:fusion} (early fusion, late fusion and cross fusion) as the building blocks to the experiments we conduct.  While we work with a single modality, i.e., textual data, we consider the different kinds of features extracted from it as distinct modalities. Our intuition being that the semantic similarities among words in these different spaces can be combined in order to exploit the latent complementarity between the lexical and syntactical representations. We also find new combinations empirically. Nonetheless, as we will show, their effectiveness replicates to different datasets and NLP tasks. 

As it will be clear in the next sections, the methods used to solve both NER and WSI/WSD (in the following subsection) are mostly not original. We first focus specially on the generation of new representation spaces. We use well-proven methods to solve both WSI/WSD and NER, so that the learning part of our experiments is known to be robust.
%The fusion should therefore improve the performance of the NLP tasks at hand, first NER and later on WSI/WSD

\subsection{Fusion Enriched Representations}
\label{chap6:application}
 Our first goal in this subsection is to assess the effectiveness of the classic fusion methods and then, as a second goal, to propose new combinations that yield better outcomes in terms of performance than the simpler approaches. The new combinations are found through experimentation. Nonetheless, as we will show, their effectiveness replicates to different datasets and NLP tasks. 





\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{images/Chapitre4/diag_metodoNER.pdf}
\caption{Steps followed on our fusion-enriched representations for NER  experiments. First the corpus is preprocessed, then features are extracted from the text. A fusion matrix is generated, which in turn is used as input to a learning algorithm. Finally, the system yields its results and to be analyzed.}
\label{fig:diagmetodo}
\end{figure}

The experiments we carry on consist in generating fusion matrices that will serve as input to a learning algorithm in order to solve NER. These input feature matrices are based upon lexical, syntactical, or other NER standard types of representation. The procedure can be sen in Figure \ref{fig:diagmetodo}.

\paragraph{Representation Spaces}\label{sec:rep_spaces}
In  Chapter \ref{chap:ling_net} we presented the fusion operators to be used in our experiments. Below we detail the three types of features matrices used to generate the fusion-enriched combinations that describe the words of the corpus tested.
\subparagraph{Lexical Matrix (L)}
For each token in the corpus, we use a lexical window of two words to the left and two words to the right, plus the token itself. Specifically, for a target word $w$, its lexical context is $(w_{-2}, w_{-1}, w, w_{+1}, w_{+2})$. This type of context features is typical for general systems studying the surroundings of a word and in particular for the named entity recognition task \cite{Daume2006,Nothman2009,RatinovR09}. 
 We retake the example phrase from \cite{LevyG14}, the lexical-based features of the phrase \textit{Australian scientist discovers start with telescope}, are shown in Table \ref{tab:lex-contextos}.
 
\begin{table}[ht]
\centering
\begin{tabular}{ll}
\hline 
 \textbf{Word} & \textbf{Features} \\ 
\hline 
Australian & word:Australian, word+1:scientist, word+2:discovers\\ 
scientist  &  word-1:Australian, word:scientist, word+1:discovers, word+2:star\\ 
discovers & word-2:Australian, word-1:scientist, $\dots$, word+2:telescope \\ 
star & word-2:scientist, word-1:discovers, word:star, $\dots$, word+2:telescope \\ 
with & word-2:discovers, word-1:star, word:with, word+1:telescope \\ 
telescope  &  word-2:star, word-1:with, word:telescope \\ 
\hline \
\end{tabular} 

\caption{Lexical features corresponding to the phrase \textit{Australian scientist discovers start with telescope}.}
\label{tab:lex-contextos}
\end{table} 

\subparagraph{Syntactical Matrix (S)}
Based on the syntactic features used in   \cite{LevyG14,Panchenko2017}, we derive contexts based on the syntactic relations a word participates in, as well as including the part of speech (PoS) of the arguments of these relations. Formally, for a word $w$ with modifiers $m_1, \dots, m_k$ and their corresponding PoS tags $p^m_1, \dots, p^m_k$; a head $h$ and its corresponding PoS tag $p_h$, we consider the context features $(m_1, p_{m_1}, lbl_1), \dots, \allowbreak (m_k, p_{m_k}, lbl_k), \allowbreak (h,p_h,lbl\_inv_h)$. In this case, $lbl$ and $lbl_{inv}$ indicate the label of the dependency relation and its inverse, correspondingly. Using syntactic dependencies as features should yield more specific similarities, closer to synonymy, instead of the broader topical similarity found through lexical contexts.

For the phrase \textit{Australian scientist discovers start with telescope} the dependency-based context is shown in Table \ref{tab:syn-contexts}.
\subparagraph{NER Standard Features Matrix (T)}
The features used for NER are based roughly on the same as those used in \cite{Daume2006,Balasuriya2009}. The feature set consists of: the word itself, whether the word begins with capital letter, prefix and suffix up to three characters (also within a window of two words to the left and two words to the right), and the PoS tag of the current word. These features are considered to be standard in the literature. We note that the matrix generated with these features is exclusively used in the experiments regarding NER.	

\begin{table*}
\centering
\begin{tabular}{ll}
\hline 
 Word & Contexts \\ 
\hline 
Australian & scientist/NN/amod\_inv \\ 
scientist  &  Australian/JJ/amod, discovers/VBZ/nsubj\_inv\\ 
discovers & scientist/NN/nsubj, star/NN/dobj, telescope/NN/nmod:with \\ 
star & discovers/VBZ/dobj\_inv \\ 
telescope  &  discovers/VBZ/nmod:with\_inv \\ 
\hline \
\end{tabular} 
\caption{Syntactic contexts corresponding to the phrase \textit{Australian scientist discovers start with telescope}.}
\label{tab:syn-contexts}
\end{table*}


\paragraph{Learning Methods}
NER being a supervised learning task, we use an averaged structured perceptron \cite{Collins2002,Daume2006} to determine the tags of the named entities. We considered logistic regression and linear SVM. For the main experiments, we chose the perceptron because of its performance and the lower training time. On the other hand, for the analysis of the results, we use a logistic regression as it is considerably easier to interpret its results.




\subsection{Experiments and Evaluation}
\label{chap6:expes}

We experiment with the four levels of fusion discussed before: Single Features (SF), First-degree Fusion (1F), Second-degree Fusion (2F) and Higher-degree Fusion (HF). The representation matrices for NER come from lexical context features $M^{LEX}$, syntactical context features $M^{SYN}$ or standard features $M^{STD}$.  On the other hand, experiments on WSI/WSD exclusively employ matrices $M^{LEX}$ and $M^{SYN}$.

We recall that our first goal is to compare the efficiency of the primary  fusion techniques applied to   named entity recognition. Then, we empirically determine a fusion combination operator able to leverage the complementarity of the features used.

To this end, we evaluate the aforementioned 4 fusion levels. We note that the fusion combinations in the third and fourth level (2F and HF) are proposed based on the results obtained in the previous levels. In other words, in order to reduce the number of experiments, we restrict our tests to the best performing configurations. This is due to the large number of possible fusion combinations that can be tested.
% (an argument to a fusion operation may be any valid output of a second fusion operation).



\paragraph{Preprocessing}

As is usual when preprocessing text before performing named entity recognition, \cite{RatinovR09}, we normalize tokens that include numbers. For example, the token 1980 becomes *DDDD* and 212-325-4751 becomes *DDD*-*DDD*-*DDDD* . This allows a degree of abstraction to tokens that contain years, phone numbers, etc. We do not normalize punctuation marks.

\paragraph{Features}
The linguistic information we use is again extracted with the Stanford's CoreNLP parser. We recall that the features used for these experiments on NER are those described before: lexical, syntactic and standard features, i.e., $M^{LEX}$, $M^{SYN}$, and $M^{STD}$, respectively. 

\paragraph{Test Datasets}We work with three corpus coming from different domains:
\begin{itemize}
\item [(1)] CoNLL-2003 (CONLL): This dataset was used in the language-independent named entity recognition CoNLL-2003 shared task \cite{SangM03}. It contains selected news-wire articles from the Reuters Corpus. Each article is annotated manually. It is divided in three parts:  training (\textit{train}) and two testing sets (\textit{testa} and \textit{testb}). The training part contains 219,554 lines, while the test sets contain 55,044 and 50,350 lines, respectively. The task was evaluated on the \textit{testb} file, as in the original task.
\item [(2)]WikiNER (WNER): A more recent dataset \cite{Nothman2009} of selected English \allowbreak Wikipedia articles, all of them annotated automatically with the author's semi-supervised \allowbreak method. In total, it contains 3.5 million words from an unspecified number of articles. 
\item[(3)] Wikigold (WGLD): Also a corpus of Wikipedia articles \cite{Balasuriya2009}. Nonetheless, this one was annotated manually. This dataset is the smaller, using 149 articles and 41,011 words. We used this corpus to validate human-tagged Wikipedia text. These three datasets are tagged with the same four types of entities: Location, Organization, Person and Miscellaneous. Otherwise, while it is faster to train models with this corpus, it may be the case that they are not able to properly fit the data given its size, and thus performance is lower than the other datasets.

The three of these datasets employ the BIO text segment tagging schemes. This tag set suggests that a word is in the \textbf{B}eginning, \textbf{I}nside, or \textbf{O}utside of a named entity. Indeed, given that there are four categories, person (PER), location (LOC), organization (ORG) and miscellaneous (MISC), there are indeed 9 different classes (B and I for each category plus O).



\end{itemize}
%
\paragraph{Evaluation Measures}
We evaluate our NER models following the standard CoNLL-2003 evaluation script. Given the large amount of experiments we carried out and to reduce the number of reported results, we report exclusively the total F-measure for the four types of entities (Location, Organization, Person, Miscellaneous). WNER and WGLD datasets are evaluated on a 5-fold cross validation.

\subsection{Results and Discussion}
We present in this subsection the results obtained in the named entity recognition task, while employing the 4 levels of fusion proposed in the previous section.

In contrast to other related fusion works \cite{Ah-PineCC15,ClinchantAC11,GialampoukidisM16}, we do not focus our analysis on the impact of the parameters of the fusion operators. Instead, we focus our analysis on the effect of the type of linguistic data being used and how, by transferring information from one feature type to another, they can be experimentally recombined to generate more complete representations.

Regarding the fusion operators' parameters, we empirically found the best configuration for $\beta$, from late fusion $L_\beta(A,B) = \beta \cdot A + (1 - \beta)\cdot B$, is $\beta=0.5$. This implies that an equal combination is the best linear fusion for two different types of features.

In respect of the $\gamma$ parameter, used in cross fusion $X_{\gamma}(A,B) = \mathbf{K}(A,\gamma) \times B$, we set $\gamma=5$. This indicates that just few high quality similarities attain better results than utilizing a larger quantity of lower quality similarities.

\begin{table}[!tbp]
\centering
\caption{NER F-measure results using the Single Features over the three datasets. These values serve as a first set of baselines. }
\label{tab:ner-blines}
\begin{tabular}{@{}lccc@{}}
\toprule
$A$                           & \multicolumn{3}{c}{\textbf{Single Features}} \\ \midrule
                & \textbf{CONLL}    & \textbf{WNER}     & \textbf{WGLD}    \\ \cmidrule{2-4}
$M^{\scriptscriptstyle STD}$                        & 77.41    & 77.50    & 59.66   \\
$M^{\scriptscriptstyle LEX}$                       & 69.40    & 69.17    & 52.34   \\
$M^{\scriptscriptstyle SYN}$                        & 32.95    & 28.47    & 25.49   \\ \bottomrule
\end{tabular}

\end{table}
\paragraph{Single Features}
Looking at Table \ref{tab:ner-blines}, we see that the best independent features, in terms of F-measure come from the standard representation matrix $\mstd$. This is not surprising as these features, simple as they may be, have been used and proved extensively in the NER community. On the other hand, $\mlex$ performs relatively well, considering it only includes information contained in the dataset itself. Nevertheless, this representation that this kind of lexical context features are the foundation of most word embedding techniques used nowadays.
While we expected better results from the syntactical features $\msyn$, as they are able to provide not only general word similarity, but also functional, getting close to synonymy-level \cite{LevyG14},  we believe that the relatively small size of the datasets do not provide enough information to generalize 


\begin{table}[!t]
\centering
\setlength\tabcolsep{2pt}

\caption{NER F-measure results using first degree fusion (1F). $B$ is either indicated on the table or specified as follows. Looking at EF,  $\hat{b}_{EF} = E(\msyn, \mstd)$. In XEF, ${b}^*_{\scriptscriptstyle XEF}$ takes the matrix from the set $\{M^{\scriptscriptstyle LEX}, M^{\scriptscriptstyle STD} \}$ which yields the best performing result. In XLF, $\hat{b}_{\scriptscriptstyle XLF}^{*}$ corresponds to the best performing matrix in $\{S^{\scriptscriptstyle LEX},S^{\scriptscriptstyle SYN}\}$. These configurations serve as the main set of baseline results.}
\label{tab:ner-1d}
\centering
%\tablewidth=\textwidth
\begin{tabular}{@{}llccc@{}}
\toprule
    $A$      &    $B$       & \multicolumn{3}{r}{\textbf{Early Fusion} }                                            \\ \midrule
          &           & \textbf{CONLL}                      & \textbf{WNER}                      & \textbf{WGLD}                      \\ \cmidrule{3-5}
$M^{\scriptscriptstyle LEX}$ & $M^{\scriptscriptstyle SYN}$ & 72.01                      & 70.59                     & 59.38                     \\
$M^{\scriptscriptstyle LEX}$ & $M^{\scriptscriptstyle STD}$ & 78.13                      & 79.78                     & 61.96                     \\
$M^{\scriptscriptstyle SYN}$ & $M^{\scriptscriptstyle STD}$ & 77.70                      & 78.10                     & 60.93                     \\
$M^{\scriptscriptstyle LEX}$ & $\hat{b}_{EF}$ & \textbf{78.90}                      & \textbf{80.04}                     & \textbf{63.20}                   \\
\midrule
          &           & \multicolumn{3}{r}{\textbf{Late Fusion} }                                             \\
\midrule     
          &           & \textbf{CONLL}                      & \textbf{WNER}                      & \textbf{WGLD}                      \\ \cmidrule{3-5}
$S^{\scriptscriptstyle LEX}$ & $S^{\scriptscriptstyle SYN}$ & \textbf{61.65}                      & 58.79                     & 44.29                     \\
$S^{\scriptscriptstyle LEX}$ & $S^{\scriptscriptstyle STD}$ & 55.64                      & \textbf{67.70}                     & 48.00                     \\
$S^{\scriptscriptstyle SYN}$ & $S^{\scriptscriptstyle STD}$ & 50.21                      & 58.41                     & \textbf{49.81}                     \\
\midrule
          &           & \multicolumn{3}{r}{\textbf{Cross Early Fusion}} \\
\midrule
          &           & \textbf{CONLL}                      & \textbf{WNER}                      & \textbf{WGLD}                      \\ \cmidrule{3-5}
$S^{\scriptscriptstyle LEX}$ &$M^{\scriptscriptstyle STD}$        & 49.90                      & \textbf{70.27}                     & \textbf{62.69}                    \\
$S^{\scriptscriptstyle SYN}$ & $M^{\scriptscriptstyle STD}$ & 47.27                      & 51.38                     & 48.53                     \\
$S^{\scriptscriptstyle STD}$ & ${b}^*_{\scriptscriptstyle XEF}$        & \textbf{52.89}                      & 62.21                     & 50.15                     \\
\midrule
          &           & \multicolumn{3}{r}{\textbf{Cross Late Fusion}}  \\
\midrule
          &           & \textbf{CONLL}                      & \textbf{WNER}                      & \textbf{WGLD}                      \\ \cmidrule{3-5}
$S^{\scriptscriptstyle LEX}$ & $S^{\scriptscriptstyle STD}$ & 27.75                      & \textbf{59.12}                     & 38.35                     \\
$S^{\scriptscriptstyle SYN}$ & ${b}_{\scriptscriptstyle XLF}^{*}$       & 36.87                      & 40.92                     & 39.62                     \\
$S^{\scriptscriptstyle STD}$ & ${b}_{\scriptscriptstyle XLF}^{*}$        & \textbf{41.89}                      & 52.03                     & \textbf{39.92}                     \\ \bottomrule
\end{tabular}
\end{table}

\paragraph{First Degree Fusion }
In Table \ref{tab:ner-1d} we present the First Degree fusion level. The best performance is obtained by trivially concatenating the representation matrices. This baseline proved to be the toughest result to beat. Late fusion does not perform well in this setting, still, we see further on that by linearly combining weighted representation matrices, we can add information to an already strong representation. Finally, regarding the cross fusion techniques, cross early and late fusion, we see that they depend directly on the information contained in the similarity matrices. We note that, as is the case on single features, the combinations with matrix $\sstd$ yield almost always the best results. While these fusion techniques by themselves may not offer the best results, we see below that by recombining them with other types of fusion we can improve the general performance of a representation.





\begin{table}[t!]
\centering

\caption{NER F-measure results using second degree fusion operations (2F). In XLEF, ${a^*}$ corresponds to the best performing matrix in the set $\{ X(S^{\scriptscriptstyle STD}, S^{\scriptscriptstyle LEX}),X(S^{\scriptscriptstyle LEX}, S^{\scriptscriptstyle STD}), \allowbreak X(S^{\scriptscriptstyle STD}, S^{\scriptscriptstyle SYN})\}$. For XEEF,  $\hat{b}_{\scriptscriptstyle XEEF}=E(\mlex, \mstd)$. In EXEF, $b^*_{\scriptscriptstyle EXEF}$  takes the best performing matrix from $\{X(\ssyn, \mlex), \allowbreak X(\slex, \mlex), X(\slex, \mstd), \allowbreak X(\ssyn, \mlex), X(\ssyn, \mstd) \}$. Finally, in LXEF, $\hat{b}_{\scriptscriptstyle LXEF}$ takes the best possible matrix from $\{X(\slex, \mstd), X(\ssyn, \mstd), \allowbreak X(\ssyn, \mlex) \}$.}
\label{tab:ner_2d}
\begin{tabular}{@{}llccc@{}}
	\toprule
	$A$                      & $B$            & \multicolumn{3}{r}{\textbf{Cross Late Early Fusion}}  \\ \midrule
	                         &                & \textbf{CONLL} & \textbf{WNER}  &             \textbf{WGLD}             \\
	\cmidrule{3-5}
$\hat{a}$ & $M^{\scriptscriptstyle STD}$      & 37.69 & 59.44 &            \textbf{41.71}             \\
	$\hat{a}$                & $M^{\scriptscriptstyle LEX}$      & \textbf{38.31} & \textbf{58.73} &            41.56             \\
	$\hat{a}$                & $M^{\scriptscriptstyle SYN}$      & 29.31 & 52.06 &            34.91             \\ \midrule
	                         &                & \multicolumn{3}{r}{\textbf{Cross Early Early Fusion}} \\ \midrule
	                         &                & \textbf{CONLL} & \textbf{WNER}  &             \textbf{WGLD}             \\
	\cmidrule{3-5}
$S^{\scriptscriptstyle STD}$ & $\hat{b}_{\scriptscriptstyle XEEF}$          &   \textbf{54.34}    &    \textbf{64.20}   & 39.59 \\
	$S^{\scriptscriptstyle LEX}$                &$\hat{b}_{\scriptscriptstyle XEEF}$         &  49.71     &   71.84    &  \textbf{45.14}\\
	$S^{\scriptscriptstyle SYN}$                & $\hat{b}_{\scriptscriptstyle XEEF}$         &  47.54     &   53.77    & 43.32 \\ \midrule
	                         &                & \multicolumn{3}{r}{\textbf{Early Cross Early Fusion}} \\ \midrule
	                         &                & \textbf{CONLL} & \textbf{WNER}  &             \textbf{WGLD}             \\
	\cmidrule{3-5}
$M^{\scriptscriptstyle STD}$ & $b^*_{\scriptscriptstyle EXEF}$          & 49.58 & \textbf{77.32} &            \textbf{61.69}             \\
	$M^{\scriptscriptstyle LEX}$                & $b^*_{\scriptscriptstyle EXEF}$      & 49.79 & 66.22 &            53.54             \\
	$M^{\scriptscriptstyle SYN}$                & $b^*_{\scriptscriptstyle EXEF}$           & \textbf{51.53} & 70.94 &            53.70             \\ \midrule
	                         &                & \multicolumn{3}{r}{\textbf{Late Cross Early Fusion}}  \\ \midrule
	                         &                & \textbf{CONLL} & \textbf{WNER}  &             \textbf{WGLD}             \\
	\cmidrule{3-5}
$M^{\scriptscriptstyle STD}$ &$\hat{b}_{\scriptscriptstyle LXEF}$           &  54.82   & \textbf{75.70} &            \textbf{54.73}             \\
	$M^{\scriptscriptstyle LEX}$                & $\hat{b}_{\scriptscriptstyle LXEF}$  & \textbf{56.53} & 62.27 &            52.39             \\ \bottomrule
\end{tabular}
\end{table}
           

\paragraph{Second Degree Fusion} 
The second degree fusion techniques presented in Table \ref{tab:ner_2d} show that the recombination of cross fusion techniques gets us closer to the early fusion baseline. Except for cross late early fusion, the rest of the recombination schemes yield interesting results. First, in cross early fusion, the best results, for the most part, are obtained while using the $\slex$ matrix combined with the output of $E(\mlex, \mstd)$, which is still far from the baseline values. Concerning, EXEF, we get already close to surpass the baselines with the $\mstd$ matrix, with the exception of the CONLL dataset. In LXEF, even though the cross fusion $X(\ssyn, \mlex)$ is not the best performing, we found experimentally that by combining it with $\mlex$ through a late fusion, it gets  a strong complementary representation. Our intuition in this case was to complement $\mlex$ with itself but enriched with the $\ssyn$ information. In the following high degree fusion results we discover that indeed this propagation of information helps us beat the baselines we set before.


\begin{table}[t]
\centering

\caption{F-measure results using high degree  (HF) fusion operators. In ELXEF, $ \hat{b}_{\scriptscriptstyle ELXEF}=L(\mlex, X(\ssyn, \mlex))$. For EELXEF, $\hat{b}_{\scriptscriptstyle EELXEF} =  E(E(\mstd, 	 L(\mlex, X(\ssyn, \mlex))), \allowbreak L(\mlex, X(\sstd, \mlex)))$ for CONLL and $\hat{b}_{\scriptscriptstyle EELXEF} =  E(E(\mstd, 	 L(\mstd, X(\ssyn, \mstd))), L(\mlex, X(\ssyn, \mlex)))$ for WNER and WGLD. The best result is obtained in EELXEF when $\alpha=0.95$.}

\label{tab:ner-nf1}
\begin{tabular}{@{}llccc@{}}
\toprule
    $A$      &    $B$      & \multicolumn{3}{r}{\makecell{\textbf{Early Late} \\ \textbf{Cross Early Fusion}}}                                            \\ \midrule
          &      &      \textbf{CONLL}                     & \textbf{WNER}                      & \textbf{WGLD}                      \\ \cmidrule{3-5}
$M^{\scriptscriptstyle STD}$ & $ \hat{b}_{\scriptscriptstyle ELXEF}$ & 67.16                      & 79.45                     & 62.37                     \\
\midrule
          &        &   \multicolumn{3}{r}{\makecell{\textbf{Early Early} \\ \textbf{Late Cross Early Fusion}}}                                             \\
\midrule     
          &          & \textbf{CONLL}                      & \textbf{WNER}                      & \textbf{WGLD}                      \\ \cmidrule{3-5}
%&& EF & EF & EF \\        
%\cmidrule(l){3-3}\cmidrule(l){4-4}\cmidrule(l){5-5}

  
$M^{\scriptscriptstyle LEX}$ & $ \hat{b}_{\scriptscriptstyle EELXEF}$ & 65.01                      & 78.02                     & 62.34                    \\
$M^{\scriptscriptstyle LEX}_{\alpha=0.95}$ & $ \hat{b}_{\scriptscriptstyle EELXEF}$  & \textbf{79.67}                      & \textbf{81.79}                     & \textbf{67.05	}                     \\ \midrule
\multicolumn{2}{l}{EF Baseline} & 78.90                      & 80.04                     & 63.20                     \\ 
\bottomrule
\end{tabular}

% | B in $\{M^{\scriptscriptstyle LEX}, M^{\scriptscriptstyle STD}, M^{\scriptscriptstyle SYN}\}$
%| B in $\{S^{\scriptscriptstyle LEX}, S^{\scriptscriptstyle STD}, S^{\scriptscriptstyle SYN}\}$
\end{table}
	
           
           
\paragraph{High Degree Fusion}
Finally, the last set of experiments are shown in Table \ref{tab:ner-nf1}. Using a recombination of fusion techniques, a so-called hybrid approach, we finally beat the baselines (single features and early fusion) for each dataset. We note that the best configuration made use of a weighted early fusion with $\alpha=0.95$. This indicates that the single feature matrix, $\mlex$ is enriched a small amount by the fusion recombination, which is enough to improve the results of said baselines. In CONLL, the early fusion (see Table \ref{tab:ner-1d}) baseline being 78.13, we reached 78.69, the lowest improvement of the three datasets. Regarding the Wikipedia corpus, in WNER, we passed from 79.78 to 81.75; and in WGLD, from 61.96 to 67.29, the largest improvement of all. It is important that we tried the weighted Early Fusion operator with different $\alpha$ and the best result does not beat these fusion results.

\subsection{Fusion Analysis}
In this subsection we present an analysis on the results obtained with the combination fusion operators shown  above. Namely, we set to understand how each addition of fusion operators helps to improve the result of the NER task. For simplicity,  we focus on the most successful fusion combination found for the three tested corpora. While the procedure to build the models analyzed herein is the same as before, we do have certain dissimilarities due to the need to explain said models in an effective way. Namely, there are two important changes in the methodology presented before: (1) we focus exclusively on the Wikigold corpus, and more importantly (2), we change the learning method from a structured perceptron to a multinomial logistic regression with L1 regularization. The main reason is that the regression is somewhat easier to interpret as it fits a sparse vector of weights for each feature and for each possible class. While the structured perceptron, also fits a matrix of feature weights, its interpretation is complex as these weights are then used to decode the best combination of tags given a complete phrase, considering the preceding and following words for each term in the corpus. In other words, in the logistic regression we can explain each word prediction independently based on a sparse vector of fitted weights for each feature and the vector that represent the word itself On the other hand, while using the structured perceptron, we need to look at whole phrases  while considering precedent and subsequent words at each time, making the interpretation quite complex. While the performance is considerably lower with the logistic regression, we note that there exists a sequential improvement  by enriching the feature space as shown in the results above (with the structured perceptron).

The most performing fusion combination is shown in the second to last line in Table \ref{tab:ner-nf1} and it is fully expressed as:    
\begin{equation}\label{eq:big_fusion}
E_{\alpha=0.95}(\mlex,  E(E(\mstd, 	 L(\mstd, X(\ssyn, \mstd))), L(\mlex, X(\ssyn, \mlex))))
\end{equation}

This fusion is principally base on early fusion operators. While we defined them in Chapter \ref{chap:ling_net} as  binary functions, we may also express it as a $n$-ary function which concatenates all the input values into a single representation\footnote{The parameter $\alpha$ still applies exclusively to the first input and the set of all the following.}. Thus, equation \ref{eq:big_fusion} can be written as:
\begin{equation}\label{eq:big_fusion2}
\overbrace{\underbrace{\overbrace{E_{\alpha=0.95}(\underbrace{\mlex}_{\circled{1}},\mstd}^{\circled{2}},L(\mstd, X(\ssyn, \mstd))}_{\circled{3}}, L(\mlex, X(\ssyn, \mlex))}^{\circled{4}})
\end{equation}


Explicitly, these numbered operations are below. We associate to each operations a model, which is trained using the representation obtained with the corresponding fusion operation.
\begin{enumerate}
\item[\circled{1}] $\mlex$ \label{eq:f1} used to train model $M_1$.
\item[\circled{2}] $E_{\alpha=0.95}(\mlex, \mstd)$ \label{eq:f2} used to train model $M_2$.
\item[\circled{3}] $E_{\alpha=0.95}(\mlex, \mstd, L(\mstd, X(\ssyn, \mstd)))$ used to train model $M_3$. \label{eq:f3}
\item[\circled{4}] $E_{\alpha=0.95}(\mlex, \mstd, L(\mstd, X(\ssyn, \mstd)), L(\mlex, X(\ssyn, \mlex)))$ used to train model $M_4$. \label{eq:f4}
\end{enumerate}

As can be seen, the operation in Equation \ref{eq:big_fusion2} is a concatenation of four elements, the feature matrices $\mlex$ and $\mstd$, and two late fusions, each one containing a cross early fusion. The analysis we make tries to elucidate the role of each numbered fusion combination. To this end, we analyze the models $M_1$ to $M_4$ and their corresponding predictions given to certain word instances. 

\paragraph{Per-Entity Performance Gain}
First, we are interested into discovering what is the contribution of each model to  the F-measure metric overall and for each specific type of named entity. In that sense,  Table \ref{tab:4ops} identifies the gains in performance due to the incremental addition of  fusion operations. In the first line  we see the results using the $M_1$. As said before, the results are lower than those obtained with the structured perceptron. On the second line, it is shown that the increment in F-measure (shown in parentheses) for all classes obtained by using $M_2$ is considerable and in fact the largest (17.50) of them all. Also, while all the classes improve, the most important gain is obtained for the class PER (person), shown in bold letters. In the same sense, on the third line, for model $M_3$, the best improvement is found for the class ORG (organization). Finally, the last mode $M_4$, improves LOC (location) class among the rest of the classes.




% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\caption{Results and improvements between four multinomial linear regression (L1 normalization) models. The performance (in F-measure) is lower than before but the improvement trend with more fusion enrichment is kept.   }
\label{tab:4ops}
\begin{tabular}{@{}llllll@{}}
\toprule
& \multicolumn{5}{c}{\textbf{NER Tags}} \\ \cline{2-6}
 \textbf{Model} & \textbf{All Tags}           & \textbf{LOC}                   & \textbf{MISC}         & \textbf{ORG}                   & \textbf{PER}                    \\ \midrule
$M_1$ & 38.03         & 49.02                 & 30.24        & 27.49                 & 41.52                  \\
$M_2$ & 55.53 (17.50) & 65.04 (16.02)         & 40.03 (9.79) & 39.46 (11.97)         & \textbf{69.19 (27.67)} \\
$M_3$ & 56.11 (0.58)  & 65.75 (0.71)          & 40.26 (0.23) & \textbf{41.13 (1.67)} & 68.99 (-0.20)          \\
$M_4$& 56.28 (0.17)  & \textbf{66.08 (0.33)} & 40.49 (0.23) & 41.07 (-0.06)         & 69.31 (0.32)           \\ \bottomrule
\end{tabular}
\end{table}

In summary, the second element of the fusion operator we analyze improves on the PER class, the third on class ORG and the fourth and last on class LOC. This knowledge allows us to frame more easily our next analysis. In the following, we are interested in determining which are the features that most likely make each model take  a decision towards one class or another. To that end, we look at three different words that were wrongly classified in a first model and correctly categorized in the next fusion enriched model. We study words whose correct tags match the tag of the enriched model with the best improvement (see Table \ref{tab:4ops}). For example, we are interested in the word \textit{Kory}, which is wrongly classified by model $M_1$ (it is assigned a tag O) but it is correctly classified as PER in model $M_2$, since PER is the class with the largest improvement in regard to $M_1$.

To determine which features are the most relevant, we look into the words non-zero-valued feature columns and match them to the logistic regression  coefficients' vectors (corresponding to the model's fitted decision function). In this way we can infer which features contribute or deter  the model from selecting a given class according to whether these values are negative or positive. The words we study are:
\begin{itemize}
\item \textit{Kory}: wrongly classified as O (out of an named entity) by $M_1$ and correctly classified as PER by $M_2$.
\item \textit{A-League}: wrongly classified as O by $M_2$ and correctly classified as ORG by $M_3$.
\item \textit{Green}: wrongly classified as  ORG by $M_3$ and correctly classified as LOC by $M_4$.
\end{itemize}

In what follows we are interested in determining which features help to determine the correct classification of the words discussed.
                                                                                                                         
\paragraph{Per-Model Feature Importance}
In Figures \ref{fig:trans_M1_M2}, \ref{fig:trans_M2_M3}, and \ref{fig:trans_M3_M4} we present six heatmaps showing the features that contribute and prevent words from being classified as one of the five tags available according (broadly) to the weights fitted for each feature during training. Specifically, there is a line for each possible class and a column for each feature that has a non-zero fitted coefficient and a non-zero value on the representation space of its corresponding word. In parentheses, next to the classes, we see the product of  the feature vector of the studied word times the coefficients' matrix of the corresponding model. These values serve as an indicator\footnote{Indeed, these values are used to obtain the  probability of each class by applying to them a logistic function, namely a softmax function.} of the class predicted by the model. Color wise,  white indicates zero values, red indicates positive values and blue represent negative values. The color intensity is directly associated with the absolute value of the coefficient.

\subparagraph{From $M_1$ to $M_2$} In the analysis from model $M_1$ to model $M_2$, we consider the word $Kory$, which changes from being ill classified in model $M1$ as O to being well classified in $M2$ as PER. We consider this last tag since its the class model $M2$ improves the most. 

In model $M_1$ (Figure \ref{fig:trans_M1}) we see it is wrongly classified as not a named entity, or tag O, as the feature \textit{word-1:senior}, i.e., the previous word was \textit{senior}, drives this decision. The tag PER is also supported by the features \textit{word+2:and} and \textit{word-2:,} but it is not enough to drive the decision towards it considering the features' coefficients  of the O class.

On the other hand, regarding the model $M_2$ (Figure \ref{fig:trans_M2}), \textit{Kory} is correctly classified  as person, since we added features from the $\mstd$ representation space. Specifically, its PoS tag (NNP), contributes towards the selection of PER by the regression decision function. At the same time, the feature CAP which determines whether the word is in upper case or not, contributes against the word being tagged as O.

\begin{figure}[h!]
	\centering
	
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\captionsetup{width=0.7\textwidth}
	\includegraphics[width=0.9\linewidth]{images/Chapitre4/M1_383.pdf}
	\captionof{figure}{The word \textit{Kory} is predicted to be class O by model $M_1$ given the coefficients of the features indicated.}
	\label{fig:trans_M1}
	\end{subfigure}% 
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\captionsetup{width=0.7\textwidth}
	\includegraphics[width=0.9\linewidth]{images/Chapitre4/M2_383.pdf}
	\captionof{figure}{The same word is correctly predicted to be class PER by model $M_2$ given the new features in the model.}
	\label{fig:trans_M2}	
	\end{subfigure}
	\caption{Non-zero coefficients heatmaps for models  $M_1$ and $M_2$ corresponding to the word \textit{Kory}. On the left, $M_1$ fitted using the feature matrix $\mlex$. On the right, model $M_2$ trained using the term $E_{\alpha=0.95}(\mlex, \mstd)$. Red colors are positive, blue are negative. The color intensity varies according to the magnitude of the value.}
	\label{fig:trans_M1_M2}
\end{figure}
\begin{figure}
	\centering
	\begin{subfigure}[t]{.9\textwidth}
	\captionsetup{width=\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{images/Chapitre4/M2_1140.pdf}
	\captionof{figure}{The word \textit{A-League} is predicted to be class O by model $M_2$ given the coefficients of the features indicated.}
	\label{fig:trans_M22}
	\end{subfigure}\\%  
	\begin{subfigure}[t]{.9\textwidth}
	\captionsetup{width=\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{images/Chapitre4/M3_1140.pdf}
	\captionof{figure}{The same word is correctly predicted to be class ORG by model $M_3$ given the new features in the model.}
	\label{fig:trans_M3}
	\end{subfigure}
	\caption{Non-zero coefficients heatmaps   for models $M_2$ and $M_3$ corresponding to the word \textit{A-League}. On top, $M_2$ fitted using the term $E_{\alpha=0.95}(\mlex, \mstd)$. On the bottom, model  $M_2$ trained using the  $E_{\alpha=0.95}(\mlex, \mstd, L(\mstd, X(\ssyn, \mstd)))$ combination. Red colors are positive, blue are negative. The color intensity varies according to the magnitude of the value.}
	\label{fig:trans_M2_M3}
\end{figure}



\subparagraph{From $M_3$ to $M_4$} Going from model $M_2$ to $M_3$, we focus on the word \textit{A-League}. In the first model, $M_3$ (Figure \ref{fig:trans_M22}), \textit{A-League} is classified as O, since it being a  noun\footnote{The PoS tagger identified it as a simple noun.} (and not a proper noun) seems to be a good indication of a noun not being part of named entity, among other features, such as \textit{suf+2:r}, i.e., the last letter of the second word to the right of \textit{A-League}, in this case \textit{r}. 

With respect to model $M_3$ (see Figure \ref{fig:trans_M3}), \textit{A-League} is correctly tagged as ORG. While the largest coefficients are assigned to the features of the model $M2$ (namely CAP and NN), we can see that the enriched features \textit{CAP\_M3}\footnote{Features added by the third fusion operation are labeled with a \textit{\_M3} suffix. The same is done for the fourth fusion with the suffix \textit{\_M4}} and \textit{suf-1:the\_M3} play a decisive role into the assignation of the class ORG, as most of the values corresponding to the newly added features are positive for this class.



\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{.9\textwidth}
	\centering
	\captionsetup{width=\textwidth}
	\includegraphics[width=1\linewidth]{images/Chapitre4/M3_16029.pdf}
	\captionof{figure}{The word \textit{Green} is predicted to be class ORG by model $M_3$ given the coefficients of the features indicated.}
	\label{fig:trans_M33}
	\end{subfigure}\\% 
	\begin{subfigure}[t]{.9\textwidth}
	\centering
	\captionsetup{width=\textwidth}	
	\includegraphics[width=1\linewidth]{images/Chapitre4/M4_16029.pdf}
	\captionof{figure}{The same word is correctly predicted to be class LOC by model $M_4$ given the new features in the model.}
	\label{fig:trans_M4}
	\end{subfigure}
	\caption{Non-zero coefficients heatmaps for  models $M_3$ and $M_4$ corresponding to the word \textit{Green}. On top, $M_3$ fitted using the term $E_{\alpha=0.95}(\mlex, \mstd, L(\mstd, X(\ssyn, \mstd)))$. On the bottom, model $M_4$ trained using   $E_{\alpha=0.95}(\mlex, \mstd, L(\mstd, X(\ssyn, \mstd)), L(\mlex, X(\ssyn, \mlex)))$ as fusion operator. Red colors are positive, blue are negative. The color intensity varies according to the magnitude of the value.}
	\label{fig:trans_M3_M4}
\end{figure}

\subparagraph{From $M_2$ to $M_3$} Finally, going from model $M3 $ to model $M4$, in Figure \ref{fig:trans_M3_M4} we have an incorrect ORG classification to a correct LOC classification after the application of the last fusion operation. The chosen word is \textit{Green}. Both coefficients' values are quite similar  to each other (see Figures \ref{fig:trans_M33} and \ref{fig:trans_M4}). In fact, the score in parentheses for both LOC and ORG are quite close in both models. This is expected as their difference in performance is small (see Table \ref{tab:4ops}). Not surprisingly, there are only two features coming from the last fusion (the last two columns, indicated with a \textit{\_M4} suffix. Nonetheless, it seems that one of these enriched features, \textit{word-1:in\_M4} determines the model decision towards the LOC class, thus making the correct classification.

In general, in these experiments, we see that the added enriched features are not the highest valued in the fitted coefficients vectors, nonetheless, they  provide the extra information needed to push the model towards the correct prediction, by enriching the features through cross and late fusion and by providing more descriptors for each word and consequently reducing the sparsity of the representation matrices.

Once we found a set of fusion operations that work reasonably well with NER, we experiment with another task, word sense induction and disambiguation, to confirm the usefulness of using fusion enriched representations to train better models.

In the next subsection we present a series of analogous experiments, this time solving WSI/WSD.


\section{Second Application: Word Sense Induction and Disambiguation}
\label{sec:wsd_wsi}
Word Sense Induction and Disambiguation entails two closely related tasks\footnote{Even though these tasks are closely related, they are independent from one another. Still,  we consider them to be a single one: WSI/WSD.}. WSI aims to automatically discover the set of possible senses for a target word given a text corpus containing several occurrences of said target word. Meanwhile, WSD takes a set of possible senses and determines the most appropriate sense for each instance of the target word according to the instance's context. WSI is usually approached as an unsupervised learning task, i.e., a cluster method is applied to the words occurring in the instances of a target word. The groups found are interpreted as the senses of the target word. The WSD task is usually solved with knowledge-based approaches, based on WordNet; or more recently with supervised models which require annotated data. It can be also solved reasonably well by comparing the words surrounding each target word and the words belonging to the induced senses (or clusters) found during the WSI step.

We believe that in order to solve WSD in a truly end-to-end unsupervised way, one would need to first automatically find a list of senses for a word without the help of pre-built semantic networks. In other words, solve WSI. Word sense induction is usually solved as follows:

Given an input document with a set of target words, coupled with a set of contexts (a target word in a unique context is called an instance), the goal is to discover a list of senses for each target word and then assign each instance in the document with an automatically generated sense (this part corresponds to WSD). The common four steps used are the following:

\begin{enumerate}
\item Build a lexical co-occurrence network (LCN), or similar, assigning tokens as nodes and  establishing edges between them if they co-occur in a given context (usually if they both appear in the same sentence, paragraph or fixed window of words).
\item Determine the weights for each edge either according to a frequency metric or using binary weights. 
\item Apply a graph clustering algorithm. Each cluster found will represent a sense of the polysemous target word.
\item Match target word instances with the clusters found (the senses) by using the word context. Specifically, assign a sense to each instance by looking at the tokens in the context. This step is actually the word sense disambiguation task. 
\end{enumerate}	


Word sense induction, while being an unsupervised and thus more flexible task (language and word-domain independent, does not require human-made knowledge bases), require a good quality clustering algorithm, as its results are tightly linked to its performance.
\subsection{Fusion Enriched Representations}

In this subsection we also employ the hypergraph model introduced before to propose a solution to both WSD and WSI tasks, specifically the enrichment of features via fusion techniques.

The WSI method, i.e., the clustering algorithm, we employ is not original, as we will see. Nonetheless, our interest lies on  using a combined representation, which is able to address certain concerns that are not deeply studied, namely the use of heterogeneous context features to solve semantic tasks while reducing the number of parameters compared to similar approaches.  Our method is evaluated with a corpus corresponding to the WSI task of the international workshop of semantic evaluations, edition 2007, or Semeval-2007. 

As shown in Figure \ref{fig:diagmetodoWSD}, the procedure we follow is very similar to that of the previous NER experiments. The difference being the task addressed and the features employed (we find clusters using only lexical and syntactic contexts).
\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{images/Chapitre4/diag_metodoWSD.pdf}
\caption{Steps followed on our fusion-enriched representation for WSI/WSD experiments. First the corpus is lightly preprocessed, then features are extracted from the text. A fusion matrix is generated, which in turn is used as input to the learning algorithm. Finally, the system yields its results and to be analyzed.}
\label{fig:diagmetodoWSD}
\end{figure}
%\subsection{Introduction}



We discovered a set of successful fusion operations in the previous experiments. In these experiments we set to test if the improvements obtained before, using said fusion schemes, can be transfered into WSI/WSD and other corpora. 

\paragraph{Representation Spaces}
We use the same set of features from the previous subsection (see \ref{sec:rep_spaces}), except for the standard NER features, that is, those represented by $M^{STD}$, as they are specifically designed to tackle that task. Consequently, we will experiment with two representation matrices $\mlex$ and $\msyn$.

\paragraph{Learning Methods}
Regarding the machine learning methods to induce senses, (the WSI part), we employ spectral clustering, as in  \cite{GoyalH14}, on the input matrices in order to automatically discover senses (a cluster is considered a sense). Regarding disambiguation, we trivially assign senses to the target word instances according to the number of common words in each cluster and the context words of the target word. In other words, for each test instance of a target word, we select the cluster (sense) with the maximum number of shared words with the current instance context.

\paragraph{Experiments and Evaluation}
Having learned the best fusion configuration from the previous task, in these experiments we set to test if the improvements achieved can be transfered into another NLP task, namely Word Sensed Induction and Disambiguation (WSI/WSD). As preprocessing, we simply remove stopwords and tokens with less than three letters. The features we extracted from the tested corpora with the same tools as in the previous task.
\subparagraph{Test Dataset}
The WSI/WSD model is tested on the dataset of  the Semeval-2007 WSID task \cite{Agirre2007}. The task was based on a set of 100 target words (65 verbs and 35 nouns), each  word having a set of instances, which are specific contexts where the word appear. Senses are induced from these contexts and applied to each one of the instances.

%In the case of Semeval-2007, the unsupervised evaluation assumed the induced senses as clusters of examples. These clusters are compared to the sets of examples tagged with the given gold standard word senses (classes), and evaluated using the \textit{F-Score} measure for clusters. 
% 
%The supervised setting maps the induced senses to manually-defined gold standard senses, and use a mapping produced by the organizers to tag the test corpus with gold standard tags. The mapping is
%automatically produced by the organizers, and the resulting results evaluated according to the
%usual precision and recall measures for supervised word sense disambiguation systems. 


\subparagraph{Evaluation Measures}
Being an unsupervised task, the evaluation metrics of WSI/WSD are debated in terms of quality \cite{CruysA11}. We consider supervised recall and unsupervised F-measure, as in the competition original paper \cite{Agirre2007}. The first one maps the output of a system to the true senses of the target words' instances and the second one measures the quality of the correspondence between the automatically found clusters and the senses. 
We consider that the number of senses found by the system is also a rather good indicator of performance: the best competition baseline assigns the most frequent sense to each target word (this baseline is called MFS), thus this baseline system would have an average of 1 sense (cluster) per word. A system that goes near this average may be indeed not resolving the task efficiently but finding the MFS trivial solution. Consequently, to show that we do not fall in the MFS solution, we display in our results the average number of clusters.


\begin{table}[htp!]
\centering
\setlength\tabcolsep{3pt}
\caption{Supervised Recall and Unsupervised F-measure for the Semeval-2007 corpus. We also display the average number of clusters found by each fusion configuration.}
\label{ref:wsd-semeval}
\begin{tabular}{@{}lccc|cccc@{}}
\toprule
\textbf{Method} & \multicolumn{3}{c|}{\textbf{Recall (\%)}} & \multicolumn{3}{c}{\textbf{FM (\%)}} & \# \textbf{cl} \\ \midrule
       & all           & noun          & verb          & all          & noun         & verb         &\\ 
       \midrule
       & \multicolumn{7}{r}{\textbf{Single Features}} \\ \midrule
       $M^{LEX}$                    & 79.20 & 82.10 & 75.80 &	72.70	 & 76.90 & 67.90 & 4.13\\
 

       $M^{SYN}$                    & 79.10 & 81.60 & 76.20 &	69.30	& 69.40 & 69.20 & 4.47\\
       \midrule
       &            \multicolumn{7}{r}{\textbf{Early Fusion}}       \\ \midrule
       $E(M^{LEX}, M^{SYN})$		& 78.70 & 81.11 & 76.10 &	74.00	& 76.66 & 71.11 & 4.46\\
	  \midrule
       &            \multicolumn{7}{r}{\textbf{Cross Early Fusion}}       \\ \midrule	         
	   
	   $X(S^{LEX}, M^{LEX})$		& 79.20 & 82.30 & 75.70 &	76.20	& 79.60 & 72.50 & 3.63 \\	   
       $X(S^{LEX}, M^{SYN})$		& 78.30 & 80.90 & 75.30 &	74.60	& 75.10 & 73.90 & 3.08 \\
	   $X(S^{SYN}, M^{LEX})$		& 78.60 & 80.90 & 76.10 &	78.90	& 80.70 & 76.90	 & 1.08 \\	   
       $X(S^{SYN}, M^{SYN})$		& 78.90 & 81.40 & 76.10 &	73.70	& 77.70 & 70.00 & 2.72 \\       
 \midrule
       &            \multicolumn{7}{r}{\textbf{Cross Late Fusion}}       \\ \midrule
	   
 
	   $X(S^{SYN}, S^{LEX})$		& 78.70 & 80.90 & 76.20 &	78.90	& 80.80 & 76.80 & 1.01 \\
	   $X(S^{LEX}, S^{SYN})$		& 78.80 & 80.90	& 76.06 &	78.70	& 80.50  & 76.80 & 1.33 \\
	  
 \midrule
       &            \multicolumn{7}{r}{\textbf{Cross Late Early Fusion}}       \\ \midrule
	   
 


       $X(X(S^{LEX}, S^{SYN}), M^{LEX})$		& 78.40 & 80.40 & 76.10 &	70.00	&68.70  & 71.40& 3.11 \\	   
       $X(X(S^{LEX}, S^{SYN}), M^{SYN})$		& 78.90 & 81.80 & 75.60 &	75.20	& 77.40 & 72.80 & 3.16 \\	   
       	  \midrule
       &            \multicolumn{7}{r}{\textbf{Early Cross Early Fusion}}       \\ \midrule	         
       
       $E(M^{LEX}, X(S^{LEX}, M^{LEX}))$		& 79.20 & 82.40 & 75.70 &	76.00	& 79.50  & 72.10 & 3.57 \\	   
	   $E(M^{SYN}, X(S^{LEX}, M^{LEX}))$		& 78.30 & 80.50 & 75.80 &	75.20	&75.40  & 75.00 & 1.95 \\	   
       \midrule
       &            \multicolumn{7}{r}{\textbf{Late Cross Early Fusion}}       \\ \midrule	  
	   $L(M^{SYN}, X(S^{LEX}, M^{SYN}))$		& 78.60 & 81.10 & 75.80 &	67.80	& 71.40&63.80 & 4.22 \\	   
	   $L(M^{LEX}, X(S^{LEX}, M^{LEX}))$		& 79.50 & 82.80 & 75.70 &	76.09	&79.10 &72.70 & 3.96 \\	   
       \midrule
       &            \multicolumn{7}{r}{\textbf{Early Late Cross Early Fusion}}       \\ \midrule	  
	   $E(M^{LEX}, L(M^{SYN}, X(S^{LEX}, M^{SYN})))$		& 78.50 & 81.40 & 75.40 &	74.20	& 78.20 & 69.80& 4.26 \\	   
	   $E(M^{LEX}, L(M^{LEX}, X(S^{LEX}, M^{LEX})))$		& 79.50 & 82.70 & 75.90 &	75.80	& 78.50&72.70 & 3.99 \\
	   \midrule
	   &            \multicolumn{7}{c}{Early Early Late Cross Early Fusion}       \\ 
	   
	   
	   Semeval-2007 Best Systems& 81.60 & 86.80 & 75.70 &	78.70	& 80.80&76.30 & 3.06, 1.15 \\ 	 
%	   Baseline 		& 78.70 & 80.90 & 76.20 &	78.90	& 80.70&76.80 & 1.00 \\ 	   	 	    
		   
       \bottomrule
\end{tabular}
\end{table}

\paragraph{Results and Discussion}
Word sense induction and disambiguation results are found in Table \ref{ref:wsd-semeval}. Again, as with NER, we aim to surpass the baseline of the single features and early fusion.  We experimentally set $\beta=0.90$ and $\gamma=50$. In this task, in late fusion, when the first matrix  is deemed more relevant than the second one, the performance is higher. This may be due to the fact that, in this task, the feature matrices rows contain types (that is, each line represent an unique word), and thus they are more dense, which may entail more noisy data. By reducing the relevance of the second matrix in late fusion, we are effectively attenuating the less important information. Regarding $\gamma=50$, again due to the denser characteristic of the matrices, there is a larger quantity of true similar words that are useful to project information into another matrix, through cross fusion.  

In the following paragraphs, we will discuss these results all at once. We omit certain configurations that do not yield interesting results either by converging to the MFS solution (one sense found per target word) or because the performance shown by those  configurations is simply not interesting.

\subparagraph{Single Features}
Regarding Single Features, $\mlex$ comes on top of $\msyn$ again. Nonetheless, $\msyn$ is much closer in terms of performance, and as expected, it is actually higher with regards to verbs. 

\subparagraph{First Degree Fusion}
On the 1F level, we see that the early fusion technique in this task does not surpass the independent features representation. Our intuition is that the similarities of both matrices seem to be correlated. In cross early fusion, the best result is obtained by $X(\slex, \mlex)$,  regarding the unsupervised F-measure. This configuration already beats our baselines, improving both noun and verb results on the unsupervised evaluation, improving the supervised recall of nouns, and staying on the same level considering all words. Also, it produces more senses than the MSF average number of senses (1 sense per target word), which is good but not indicative of results correctness. Regarding cross late fusion, given the average number of clusters produced, it seems that both results converge towards the MFS, therefore we do not consider these results.

\subparagraph{Second Degree Fusion}
Beginning with the fusion recombinations, in level 2F, both cross late early fusions yield average results. In  early late cross early fusion, the early fusion of $\mlex$ with  $X(\slex, \mlex)$ yields very similar results than $X(\slex, \mlex)$. The next natural step is to test this fusion via a linear combination, with a late fusion. The result obtained confirmed the intuition of enriching a single feature matrix with another weighted-down matrix to improve the performance. Indeed, we consider that $L(M^{LEX}, X(S^{LEX}, M^{LEX}))$ gets the best results in terms of all-words supervised recall and the second best all-words unsupervised F-measure (we do not consider solutions that are too close to the MFS baseline). 

\subparagraph{High Degree Fusion}
We test the same configurations as in NER, within the HF level, to try and improve our results. Nonetheless, in general, they do not overcome the best result found previously.
In general, we found that the recombination fusion techniques work in terms of improving the performance of the tasks addressed.

Looking at the result table, we show that the fusion schemes tested with NER are indeed useful also with WSI/WSD. 

In the following subsection, we put aside the fusion enrichment and we focus into another characteristic of our proposed hypergraph structure. We leverage the links (features) among nodes (words) to induce senses. Specifically, we propose a method that clusters together words which represent induced senses for a set of target words.


\subsection{Leveraging the Linguistic Network Structure}
Until now, we have employed the hypergraph representation in terms of leveraging the heterogeneous information to enrich and densify a feature space.
Now, we will leverage the relations that exist within the network to identify words that, together with their neighborhood, represent a sense. Thus, we propose a network-based algorithm to solve word sense induction. 

%Word sense disambiguation, or WSD, is the task of examining a word in different contexts (we refer to a word in a particular context as an instance)  and determining which is the sense being used in each one of the contexts analyzed. Usually a list of senses from where to chose the correct one is given as an input. When this list is not available, WSD then becomes a different, complementary task: word sense induction, or WSI. WSI also analyzes  tokens of a target word context but before assigning a sense to each of its instances, it first generates a list of possible senses from where to select from. 





%

%\paragraph{State of the art discussion}\label{sec:survey_disc}


With respect to the information contained in the network, we find that few approaches include syntactic attributes into their model. We believe that finding semantic similarities can be improved by leveraging syntactic information by using dependency relations. 
%Moreover, using syntactic data along with semantic and/or lexical co-occurrences takes us into the heterogeneous network domain which has not been addressed in most of the approaches covered.
%Being able to design new similarity metrics that deal with different types of information opens new avenues of research in the semantic similarity domain. % Finally, concerning the algorithms employed, few approaches make direct use of the graph Laplacian representation. New similarities could be defined using the Laplacian as a starting point. 




\paragraph{Proposed Method}
	
%The cited works use a LCN as described before while other works such as \cite{2014.Tao.Qian.LexicalChainHypergraphWSI} represent the co-occurrence by means of a hypergraph schema. In short, a hypergraph structure is a graph generalization where an edge (called hyperedge) can link multiple vertices per edge and thus it is able to provide a more complete description of the interactions between several nodes.


Formally, the objective of WSI/WSD is the following: given a document $d$ with a set $T$ of  target words $tw \in T$ and the set $C$ with contexts for each target word $ct_{tw}$. 
Specifically, each paragraph represents the context  of a target word. A target word in a specific context is also called an instance.
% $tw$. %Each word $tw$ features several $ct$.
The goal is first to solve the WSI task, that is, automatically determine a list of senses for a given $tw$, and then assign one meaning from this list to each of its instances, the WSD task.
%
%As described before, WSI  is usually solved following four steps: (1) creation of a linguistic network, (2) determine the level of similarity between nodes within the network, (3) cluster nodes together, thus creating individual senses, and (4)  assign a cluster (sense) to each instance of a target word in the input document (this step amounts to WSD).


Our method follows is inspired on previous approaches from \cite{2004.Veronis} and \cite{2007.Klapaftis.UOY}. In Hyperlex,  the graph-based  method presented  in \cite{2004.Veronis}, the main intuition is that co-occurrence networks have small-world properties and thus it is possible to detect and isolate important heavily-connected nodes, call "hubs". These hubs, and their connected nodes represent a sense in itself. 

Hyperlex performs WSI and WSD using a weighted lexical co-occurrence network. The process is performed for each target word in the document. As a first step, they build a  graph by defining the vertices (the target word node is removed) as the tokens found in the  co-occurring  context of a target word. The edges link two words co-occurring together. Each edge is assigned a weight that decreases as the association frequency of the words increases. The second step consists on iteratively finding the hubs and removing them, along with their adjacent nodes, from the target word graph. Again, the intuition of the method is that these isolated hubs, and their adjacent words, represent a sense of the analyzed word. The third and final step carries out the disambiguation. A new graph is created by adding the target word to the co-occurrence graph. Zero-weighted edges are added between each hub and the target word. A minimum spanning tree is then calculated and the sense component found to have the closest set of nodes is chosen as the target word sense.

The second approach, \textit{UOY}, described in \cite{2007.Klapaftis.UOY},  relies itself on the small-world intuition presented  by Hyperlex to find hubs and its adjacent nodes to represent senses.   In short, these methods, as ours, exploit the real-world characteristics of linguistic networks by theorizing that there are certain few high-degree nodes (called hubs) that carry an important role in the network and therefore may represent, coupled with their neighbors, a sense for a given target word. Particularly, UOY considers bigrams and trigrams that co-occur in a paragraph as hyperedges. Under a frequent-itemset setting, they determine important hyperedges given their \textit{support} and their \textit{confidence}  values. Then, the clustering of words takes place by finding the hubs and considering them as sense carriers only if they satisfy a threshold mainly set upon their containing-hyperedge confidence value. Finally, once the senses are identified, each target word instance (represented by a context) is assigned to a sense according to the sum of confidences of the hyperedge appearing on said context. 

In our method, we generate a network for each $tw$ and consider that the high-degree nodes inside this network may represent a $tw$ sense. Figure \ref{fig:wsd_wsi_process}  shows an overview of the process. Also, in Algorithm \ref{alg:wsd} we show the general flow of our approach.  We detail the steps taken alongside the corresponding line in the algorithm below. 

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{images/Chapitre4/wsd_wsi_process.png}
\caption{Block diagram of the WSI/WSD method proposed.}
\label{fig:wsd_wsi_process}
\end{figure}


\paragraph{Creation of the linguistic network}
%In the previous sections we worked with the English Wikipedia as background corpus to build and model our proposed linguistic network. Given the large size of Wikipedia, and to iterate faster our experiments, we decided to change the corpus to one with a more manageable size.

%We use the Open American National Corpus (OANC) \cite{oanc} as background document collection to build a hypergraph network $G_H$ following our proposed model.  The OANC  includes texts from several domains and encompasses 11,406,155 words. We split the documents of the corpus in sentences, then we tokenize and parse them with Stanford's CoreNLP \cite{corenlp}. As described before, the dependency and constituency tree are used to build the hypergraph: words are depicted by nodes, and they may exist inside any of the three different types of hyperedges defined (sentence,  noun phrase or dependency contexts). If any  hyperedge is repeated through the corpus, we increment a counter and keep the number of apparitions instead of adding redundant columns to the hypergraph incidence matrix.

\begin{algorithm}[!htp]
\SetAlgoLined
\KwIn{A set $tw\_set=\{tw_1, tw_2, ..., tw_n\}$ of target words }
\KwIn{A background linguistic network $G_H$}
\KwIn{Filtering thresholds $th_1$, $th_2$}

\KwOut{A set $SoS_{tw}$ of senses for each target word}
\ForEach{target word $tw$ in tw\_set}{
	
	$G_{tw}$ = \texttt{extract\_subgraph}($G_H$, $tw$)\;
	$B_{tw}$ = \texttt{induce\_bipartite\_graph}($G_{tw}$)\;
	$S_{tw}$ = \texttt{sim\_matrix}($B_{tw}$)\;
	$F_{tw}$ = \texttt{induce\_hypergraph}($S_{tw}$, \textit{th1})\;
	$candidate\_hubs$ = \texttt{sort(degree}($F_{tw}$))[:100]\;
	$SoS_{tw}$ = [~]\;
	\ForEach{candidate\_hub in candidate\_hubs}{
%		\uIf{\texttt{degree}(candidate\_hub) $<$ th2}{
%			\textbf{continue}\;
%			}
%		\tcc{get all hyperedges where candidate\_hub appears}
		$candidate\_hyperedges$ = \texttt{get\_hyperedges}($candidate\_hub$, $F_{tw}$)\;
		$candidate\_avg_jaccard$ = 0\;
		\ForEach{ hyperedge in candidate\_hyperedges}{
%		\tcc{get average jaccard of all words (nodes) within hyperedge}
			$candidate\_avg\_jaccard$ += \texttt{get\_avg\_jaccard}($hyperedge$)\;
			}
		\uIf{candidate\_jaccard $>$ th2}{
			$SoS_{tw}$.\texttt{add}(\texttt{get\_words}($candidate\_hyperedges$))\;
%			\tcc{remove hyperedges of candidate hub from tw (filtered) hypergraph.}
			$F_{tw}$ = $F_{tw} \setminus candidate\_hyperedges$\;
		}
	}
	\Return{$SoS_{tw}$}
}

\caption{Pseudo-code of our WSI/WSD network-based approach}
\label{alg:wsd}
\end{algorithm}


In order to find senses from the contexts of a target word, the first step in our procedure is to build a linguistic graph $G_H$ from a background corpus. As described in previous sections, the dependency and constituency trees are used to build the hypergraph: words are depicted by nodes, and they may exist inside any of the three different types of hyperedges defined (sentence,  noun phrase or dependency contexts). If any  hyperedge is repeated through the corpus, we increment a counter and keep the number of apparitions instead of adding redundant columns to the hypergraph incidence matrix.

At each step, that is, for each $tw$ in the test input document, we extract a subgraph $G_{tw}$ from $G_H$ that contains all the words that appear together with $tw$ (line 2), whether by lexical or syntactic co-occurrence. The $tw$ is removed from $G_{tw}$. In this approach we focus specifically on dependency relations and lexical co-occurrence. 

We note that for the syntactic co-occurrence, that is, the dependency relations between words, we apply two strategies: when dealing with a noun target word, we use the co-occurrent relations between said noun and other words having a similar  dependency head token. On the other hand, when dealing with a verb target word, we select the co-occurrent words having said verb as head of the dependency relation.  The reason is that usually verbs are more often than not the head of dependency relations, so the intuition is that words which have the same verb governor are somehow semantically related. %Otherwise, if we took the relations between verbs and other words having a similar dependency head, we would 

\paragraph{Computing the similarity between nodes}
In order to computationally treat $G_{tw}$, we first induce a bipartite graph $B_{tw}=(U,W,E)$ from $G_{tw}$ (line 3). The set of left nodes $U$ represent words and the set of right nodes $W$ depicts the membership to a given hyperedge. Thus, we have as many nodes in $W$  as we had hyperedges in $G_H$.

We compute the Jaccard index between each node $n_{i,j} \in U$ as $Jaccard(i,j)=\frac{|N(i)\cap N(j)|}{|N(i)\cup N(j)|}$ in order to build a $|U|\times|U|$ similarity matrix $S_{tw}$ (line 4). We induce from $S_{tw}$ a new filtered  hypergraph incidence matrix $F_{tw}$ (line 5), which contains word nodes as rows and columns as hyperedges. Each of these hyperedges represent a set of words that are deemed similar between them according to their Jaccard  index value, which must be equal or higher than an assigned threshold $th_1$ .  

\paragraph{Clustering words  together}
Once the incidence matrix $F_{tw}$ is built we can proceed to induce senses for a target word by clustering words (vertices) together. First, we calculate the degree of each node $n_i \in F_{tw}$. The degree of a node is simply the number of hyperedges it is incident in. Nodes are sorted in descending order and evaluated one by one. We consider the top $c$-nodes as sense hub candidates (line 6). We accept or reject a node $n \in F_{tw}$ as a sense carrying word according to one condition. As shown from line 11 to 17 in the pseudo-code, we set a minimum limit to the average of the Jaccard similarities between each pair of neighbors of node $n \in F_{tw}$ within each hyperedge  $n$ belongs to. Formally, for a node $n$, we define the average Jaccard measure as: $$AvgJaccard(n)=\frac{1}{|hedges(n)|}\sum_{h\in hedges(n)}\frac{\sum_{\substack{i\in h\\j\in h;i\neq j}}Jaccard(i,j)}{|h|}$$ 
where $hedeges(n)$ is the set of hyperedges $n$ is incident in and its cardinality is defined as $|hedges(n)|$, and $|h|$ is the number of nodes in the hyperedge $h$. 

The Jaccard similarity measure allows us to easily determine the neighbors of each node in the current bipartite hypergraph representation. As each node is joined to a sentence or dependency node, calculating the Jaccard similarity amounts to determining the level of co-occurrence between each word according to a specific type of hyperedge (represented as a node from the other graph partition) while taking into account the total number of hyperedges the words participate in. We differentiate specifically from the previously described method, UOY, in that in the case of that system, the weighting of the hyperedges is done by computing the average confidence metric of each hyperedge. In this regard, the Jaccard similarity is more flexible with respect to the confidence metric, as the confidence requires in the numerator the number of contexts (paragraphs in UOY's case) shared by all the members of the hyperedge, whereas the Jaccard measure takes pairs of members individually and thus is less strict in the apparition of all the elements of the hyperedge in the contexts. Given the nature of the features used (lexical and syntactical dependencies), we fix our thresholds in a manual but simpler way by defining percentiles and taking the value of the threshold directly, without having to change it according to the characteristics of the data.

If node $n$ satisfies both thresholds $th_1$ and $th_2$, it is deemed as a sense purveyor and all its neighbors (words that appear in the same hyperedges as $n$) are conflated into a single set representing a $tw$ sense. This new sense is added to $SoS_{tw}$ (line 17). The sense set is then removed from $F_{tw}$.

The process is repeated until no more nodes satisfy both boundaries. When the process is complete, we obtain a set of senses $SoS_{tw}$ where each set contains words that ideally represent a unique meaning for each target word. 

\paragraph{Sense assignation}

The assignation of a sense consists in looking at each $tw$ instance represented by a context $ct$ and simply determining which sense $s$ in $SoS_{tw}$ shares the highest amount of words with $ct$. The sense $s$ is thus assigned to that instance. If two senses in $SoS_{tw}$ share the same amount of words with $ct$, one of them is randomly chosen.  This operation is repeated for each instance of each target word. 



	
\paragraph{Experiments and Evaluation}
\subparagraph{Test Datasets}
We trained and evaluated our system on two datasets: Semeval-2007 Task 2(as in the previous experiments) and Semeval-2010 Task 14 \cite{Semeval2010}. We recall that the Semeval-2007 task consisted in the induction and disambiguation of a single set of 100 words, 65 verbs and 35 nouns, each target word having a set of contexts where the word appear. On the other hand, the Semeval-2010 task consisted also on 100 words, with 50 being verbs and 50 being nouns. This time, a training set from which the senses of a word have to be induced is provided. In our experiment, for the Semeval-2010 dataset, we induce the senses from the training set and disambiguate the target words present within the test set.

We apply a light pretreatment, consisting on token lemmatization and we remove all words that appear less than four times. Concerning the individual graphs of each target word, we work only with nouns and if the extracted graph has fewer than 100 nodes, we do not apply any filtering (we keep all the extracted words). We do this in order to avoid empty solutions.



\subparagraph{Implementation}
The objective of this experiment is to understand the performance of both lexical and syntactic co-occurrence information, used independently, while solving WSI and WSD tasks while using the method described in the previous subsection. To that end we build two independent systems: LEX, which uses exclusively lexical co-occurrence hyperedges, and DEP, which employs only syntactic dependency hyperedges. 
 %
Each type of hyperedge has its own network characteristics as mentioned before. Sentence hyperedges tend to have a much smaller number of words than those of the dependency category. This make sense as sentences usually contain less than 30 words, meanwhile a dependency hyperedge may contain up to hundreds of words (several words may share the same dependency relation). Taking this into consideration we set different threshold values for LEX and for DEP. First, we  consider only the top 100 nodes as candidate sense hubs. Secondly, we do not set the thresholds' values directly but instead we set up a percentile value for the Jaccard similarity ($th_1$) and for the average Jaccard similarity ($th_2$). This is a practical solution to the changing nature of the network model according to the features being employed. We experimentally found the best values for  each threshold used.



%Given that the variation of these thresholds affect the performance of the systems, we decided to experimentally fix threshold $th_1$ at 0.7 for \textit{dep} and at 0.030 for \textit{lex}. The difference of scales is determined by the characteristics stated before, that is, the sparse nature of the sentence hyperedges compared to the density of dependency hyperedges. The second threshold $th_2$ is automatically set for both systems, as explained above.
%
%This leaves only one threshold left, $th_3$. We experiment with two different ranges of values, one for each system. For \textit{dep} we set the range $[0.3, 0.65]$ with a step of 0.05. For \textit{lex} we set $[0.01, 0.08]$ with a step of 0.1. These ranges were chosen experimentally with two constraints in mind: (1) lower threshold values usually gave the same results\footnote{Still, some of the values used produced equal results and thus are not visible in Figure \ref{fig:prec_recall}.} as those already included in our ranges, and (2) higher threshold values forced the system to either give only one sense per word (resulting in the most frequent baseline), or even worse, not accepting any sense, thus having an null solution. 



\paragraph{Results and Discussion}
%Both Semeval-2007 and Semeval-2010 tasks are evaluated by an unsupervised and supervised set of measures. 
\subparagraph{Semeval-2007}

\begin{table}[!htb]
\centering

\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{FS (\%)} & \textbf{all} & \textbf{nouns} & \textbf{verbs} & \textbf{\#cl} \\ \midrule
1c1word          & 78.9         & 80.7           & 76.8           & 1.00             \\
UBC-AS           & 78.7         & 80.8           & 76.3           & 1.32          \\
DEP     & 74.9         & 80.2           & 69.0           & 3.27          \\
LEX     & 61.4         & 62.6           & 60.1           & 4.26         \\
UoY(2007)        & 56.1         & 65.8           & 45.1           & 9.28          \\
Random           & 37.9         & 38.1           & 37.7           & 19.7             \\
1c1instance & 	9.5         & 6.6           & 12.7           & 48.51             \\ \bottomrule
\end{tabular}
\caption{Unsupervised F-Score (FS) for the Semeval-2007 test set}
\label{tab:sem2007_unsup_FS}
\end{table}

In Table \ref{tab:sem2007_unsup_FS} we present the unsupervised evaluation results for our models as well as for some other systems. We include the F-Score measure as performance metric. Three baselines are included. The best one, one cluster per word, or \textit{1c1word} was not beaten during the competition. The second was a random assignation of senses to each instance. Finally, the third and easiest to beat baseline assigned one cluster per instance {1c1instance}. 

In this table, as in the rest of the tables presented in this section, the columns show the results for all the words, for the nouns exclusively as well as for the verbs. The final column indicates the number of induced clusters per system. It is important to consider this value as the unsupervised metrics are biased towards systems with less number of induced clusters and thus to the 1c1word baseline.

We can appreciate that both our methods surpass the baselines and the system described before \textit{UoY(2007)}. The best system of the competition, \textit{UBC-AS} used also co-occurrence graphs and applied a random-walk based clustering algorithm over the vertices' similarity matrix. Still, our system induced a larger amount of senses, while retaining a competitive F-Score value. We also note that in this evaluation DEP, the system using only co-occurrent dependency relations outperformed the lexical co-occurrence only system LEX.

\begin{table}[!htb]
\centering
\caption{Supervised Recall (SR) on the Semeval-2007 test set}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{SR (\%)} & \textbf{all} & \textbf{nouns} & \textbf{verbs} & \textbf{\#cl} \\ \midrule
I2R & 81.6 & 86.8 & 75.7 & 3.08 \\
LEX & 79.4 & 82.5 & 75.9 & 4.26 \\
DEP & 79.1 & 81.5 & 76.4 & 3.27\\
MFS & 78.7 & 80.9 & 76.2 & 1 \\
UoY(2007) & 77.7 & 81.6 & 73.3 & 9.28 \\ \bottomrule
\end{tabular}

\label{tab:sem2007_sup_recall}
\end{table}


Moving onto the supervised results for Semeval-2007, in Table \ref{tab:sem2007_sup_recall} we show the results obtained concerning the Recall performance metric.  In this table we include the competing system \textit{I2R} \cite{Niu2007}, based on an Information Bottleneck based clustering algorithm, which obtains the best results according to all the words and nouns. Both our systems DEP and LEX beat the baseline of assigning the most frequent sense to an instance (\textit{MFS}). More interestingly, \textit{DEP} was able to beat the MFS verb baseline, something that was not achieved during the competition. As was the case before, our systems beat UoY(2007).




As a way of determining where does both systems complement each other, in Figure \ref{fig:nouns_fs} and Figures  \ref{fig:verbs_fs2} and \ref{fig:verbs_fs3} we show the unsupervised F-Score value for nouns and verbs respectively (we split the verbs in two figures for visibility). We can see that, as the previous result tables indicated, DEP did better overall. Nonetheless, and what is most interesting in these figures, is that there are certain words (both nouns and verbs) that obtain better scores using LEX instead of DEP and vice versa. For example, the nouns \textit{area}, \textit{future}, and \textit{state} are better treated by LEX, according to this measure, even if by a small margin. On the other hand, with respect to the verbs, the differences between performance are more important. Again, the LEX system does better while finding senses and assigning them to the verbs \textit{avoid}, \textit{fix}, and \textit{work}. %This information will be useful during the design of hybridization techniques between feature of our hypergraph structure.

 




 \begin{figure}[!htb]
\centering
\includegraphics[width=1\linewidth]{images/Chapitre5/tex_img_files/nouns_fs.pdf}
\caption{Unsupervised F-Score results for the nouns of the Semeval-2007 test set.}
\label{fig:nouns_fs}
\end{figure}

 \begin{figure}[!htb]
\centering
\includegraphics[width=1\linewidth]{images/Chapitre5/tex_img_files/verbs_fs_2.pdf}
\caption{Unsupervised F-Score results for the first half of verbs of the Semeval-2007 test set.}
\label{fig:verbs_fs2}
\end{figure}

 \begin{figure}[!htb]
\centering
\includegraphics[width=1\linewidth]{images/Chapitre5/tex_img_files/verbs_fs_3.pdf}
\caption{Unsupervised F-Score results for the second half of verbs of the Semeval-2007 test set.}
\label{fig:verbs_fs3}
\end{figure}

%todo add phrase that says that syntactic is not as good as seen in the paper where they compare a lot of distributional models... 	


%todo split verbs?
\subparagraph{Semeval-2010}
In Semeval-2010, two unsupervised evaluation metrics are introduced: \textit{V-Measure} and \textit{paired F-Score}. Briefly, the V-Measure assesses the quality of a clustering solution by measuring the degree to which each cluster consists of instances principally belonging to a single gold standard class, or homogeneity; and to completeness, the level with which each gold standard class consists of instances primarily assigned to a single cluster.

The paired F-Score evaluation transforms the clustering problem into a classification one. Two instance pairs sets are generated, the first one coming from the system induced clusters, by including pairs of the instances found in each cluster. The second set of instance pairs is built from the gold standard classes. It contains pairs of the instances found in each class. We can then define \textit{Precision} and \textit{Recall}. Precision is computed as the ratio of the number of common instance pairs between the two sets divided by the total number of pairs in the clustering solution. Recall is the count of common instance pairs between the two sets divided by the total number of pairs in the gold standard. The paired F-Score is the harmonic mean of both quantities.

Concerning the supervised evaluation, it follows explicitly that of Semeval-2007, using recall as the main performance measure as in  \cite{Semeval2010,VandeCruys2011,pedersen2010duluth}.

\begin{table}[]
\centering
\caption{Unsupervised V-Measure (VM) on the Semeval 2010 test set}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{VM (\%)} & \textbf{all} & \textbf{nouns} & \textbf{verbs} & \textbf{\#cl} \\ \midrule
{Hermit} & 16.2 & 16.7 & 15.6 & 10.78 \\
NMF$_{lib}$&11.8&13.5&9.4&4.80\\
LEX & 11.6 & 8.8 & 11.9 & 10.5 \\
Random & 4.4 & 4.2 & 4.6 & 4.00 \\
DEP & 3.5 & 3.9 & 2.8 & 2.75 \\
MFS & 0.0 & 0.0 & 0.0 & 1.00 \\ \bottomrule
\end{tabular}

\label{tab:sem2010_VM}
\end{table}

In Table \ref{tab:sem2010_VM} we present our systems compared to the baseline and other methods using during Semeval 2010. V-Measure is a metric well-known for favoring systems producing a higher number of clusters \cite{VandeCruys2011,pedersen2010duluth}. Thus, it is considered not a very reliable metric. We have included it to have a global insight about the performance of our method. We remark that only LEX performed better than both baselines, random assignation of senses (Random) and using the most frequent sense (MFS). Also, we note that we included only the best performing systems, in this case \textit{NMF$_{lib}$} and  \textit{Hermit}. The former  did not participate on the competition but was developed later. We include it henceforth to illustrate how systems variate from one position to another depending the metric used to assess the performance. The latter was the best method on this metric from the task challenge. It is important to notice that other systems exist between Hermit and LEX. They were not included for the sake of clarity.

\begin{table}[]
\centering
\caption{Unsupervised Paired F-Score (FS) for the Semeval 2010 test set}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{FS (\%)} & \textbf{all} & \textbf{nouns} & \textbf{verbs} & \textbf{\#cl} \\ \midrule
MFS & 63.5 & 57.0 & 72.4 & 1.00 \\
Duluth-WSI-SVD-Gap & 63.3 & 57.0 & 72.4 & 1.02 \\
DEP & 53.6 & 50.1 & 58.7 & 2.75 \\
NMF$_{lib}$&45.3&42.2&49.8&5.42\\
LEX & 38.4 & 46.7 & 28.5 & 10.5 \\
Random & 31.9 & 30.4 & 34.1 & 4.00 \\ \bottomrule
\end{tabular}

\label{tab:sem2010_FS}
\end{table}

The second unsupervised measure, Paired F-Score, can be seen in Table \ref{tab:sem2010_FS}. In this case both systems presented performed better than the random baseline. Any system presented was able to beat the MFS baseline. We note that DEP does much better compared to LEX concerning verbs, namely 58\% vs. 28\% F-Score. Still, the results are low considering the best results of the competition, 63\% from Duluth, although again, it generates a number of senses very similar to the MFS baseline. Both our systems induce a considerable amount of clusters while keeping a descent F-Score.



\begin{table}[tb]
\centering
\caption{Supervised recall (SR) for Semeval 2010 test set (80\% mapping, 20\% evaluation)}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{SR (\%)} & \textbf{all} & \textbf{nouns} & \textbf{verbs} \\ \midrule
NMF$_{lib}$&62.6&57.3&70.2\\
UoY(2010) & 62.4 & 59.4 & 66.8 \\

LEX & 59.8 & 55.8 & 67.4 \\
DEP & 59.3 & 53.9 & 67.2 \\
MFS & 58.7 & 53.2 & 66.6 \\
Random & 57.3 & 51.5 & 65.7 \\ \bottomrule

\end{tabular}


\label{tab:sem2010_SR}
\end{table}


Finally, we in Table \ref{tab:sem2010_SR} we show the supervised Recall results of Semeval-2010. The best performing algorithm shown is NMF$_{lib}$. During the competition, UoY(2010) was the best method. It is a graph-based algorithm which shares the name with the UoY  system presented in Semeval-2007, but it is a different  approach. 

Concerning our systems, in this evaluation they seem to perform the best, or in a comparable level, to the top methods.  We find that in general our systems seem to perform better on the Semeval-2007 dataset. Discovering the reason could shed light into improving the performance on the Semeval-2010 test set. Given the results, it seems like a combination of features (syntactic plus lexical) in  a single algorithm could yield better results. 

As a side note, it is argued that supervised Recall is not a very robust metric as the supervised method within tends to converge towards the most frequent sense. In this regard, several researchers \cite{VandeCruys2011,pedersen2010duluth} have voiced their concerns about the quality of the current WSI/WSD evaluation metrics as well as the need of new, more robust techniques to properly evaluate these systems.



%\paragraph{Conclusion}

\section{Conclusion}
\label{chap6:conclusion}
In this chapter we addressed two NLP tasks from two different points of view: on the one hand, we computed several representation spaces using fusion operations in order to enrich and densify otherwise sparse and independent features. The matrices generated were used to train both supervised and unsupervised models to solve named entity recognition and word sense induction and disambiguation.

On the other hand, we proposed a model that leverages the inner structure of the hypergraph network to group words that belong to a shared sense. This approach was used to solve word sense induction.

More specifically, concerning the first part, we presented  a comparative study of multimedia fusion techniques applied to named entity recognition.  We also tested hybrid fusion recombinations in order to complement the information contained in the single representation matrices. In order to accomplish this goal, we built upon basic fusion techniques such as early and late fusion, as well as cross media fusion to transfer quality information from one set of features to another. 

%We found that by taking a strong feature, in our case lexical context, $\mlex$, and enriching it with the output of rather complex fusion combinations, we can improve the performance of the tasks addressed. The enrichment has to give more relevance to the strong feature matrix, by selecting the right parameters. 

We analyzed the results to understand how the enrichment of features improved the performance. We found that at each fusion step, a different type of NER tag is benefited. We studied what features where driving the decision towards the correct class and found that while the enriched features are not the most prominent  in the decision function, they play an important role by tipping said decision towards the correct label and away from the wrong one. 

Concerning fusion enrichment and WSI/WSD, we found that the fusion operations also improve the results of the task, although not as clearly as in NER. The metrics used to measure the performance on this ask does not allow a clear understanding on the behavior of the model employed. While we want to avoid converging to the trivial one sense per word solution, we know that words do not have numerous senses. In that sense, the results obtained stay reasonably away from the trivial solution while not producing many senses as other approaches.

While there is an improvement using fusion techniques, we do note that they  enlarge the feature space, especially early fusion, which is used frequently. This may imply the need of larger quantities of memory and longer execution time. In that sense, as future work, more intelligent ways of finding the most appropriate fusion must be researched. This is indeed one of our future work paths: determining an optimal fusion path from single features to a high degree fusion recombination. Coupled with this, the automatic determination of the parameters is still ongoing research in the multimedia fusion community. Consequently, we believe that efficiently determining both parameters and fusion combinations is the general domain of our future work. Another route we would like to explore is testing these techniques on other tasks and with datasets from different domains, in order to assert its effectiveness.


% WSD network
Concerning our proposed network-based method, we show how using the inner links within the hypergraph structure we can group words that represent senses and then assign them to target words. Our method distinguishes from similar works in two main aspects: the definition of similarity used, the reduced number of parameters that are needed, the use of diverse types of contexts to solve the task. 	We show that our method beats said similar approaches. Also, we discovered the behavior of syntactic contexts in comparison to lexical contexts at word-level. Indeed, lexical contexts seem to perform better. This is in line with other works on distributional representations \cite{kiela2014systematic}.



%\minitoc

%\subsubsection{Introduction}
