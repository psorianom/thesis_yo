\chapter{Conclusions and Future Work}
\label{chap:conclusions}
%\minitoc
\section{Conclusion}
Linguistic Networks are useful methods to understand the nature of our language. In the literature, they are generally used to comprehend either the dynamics of words and other textual units within language, and to solve practical NLP tasks. Nonetheless, no mater the objective, they are usually based on the distributional hypothesis, that is, words will be found in similar contexts if they tend to be semantically related.

Distributional models are based on several parameters, such as the size of the context to be used, their linguistic type (either syntactic, lexical, etc.), the weight that affects each context-word co-occurrence, as well as determining how the semantic relatedness is computed. Indeed, most of the linguistic networks in the literature deal with a single type of contexts, either lexical or syntactical.

On the other hand, text data representations, described through contexts in a distributional framework, are sparse by nature: the large majority of the entries in a co-occurrence matrix are zero. This translates to greatly sparse features' matrices which represent problems for knowledge discovery methods as they do not have much information about words because each one of them is has a very low number of features. We considered the lack of heterogeneity and data sparsity two open challenges in textual representations.

To treat these concerns, on this thesis we proposed three contributions. The first and second entail a fusion enriched linguistic network. The third is practical: a method based on graph structure to find groups of related words. The third one, a method to generate denser matrix representations by combining different feature spaces.

The linguistic model we proposed unifies language networks by means of a hypergraph structure. We consider three different types of co-occurrence contexts in order to represent three distinct levels of semantic relatedness. These contexts are based on lexical and syntactic co-occurrences, which are unified with a hypergraph. This union yields words that are related by any of the three contexts and thus creating more links among words. 

These heterogeneous features are important as they represent relations between words from different points of view. Nonetheless, they tend to generate sparse representations, as is common with text representations. In this sense, we proposed  as second contribution the use of fusion techniques to combine these features while reducing the sparsity of the representation space. 

Lastly, the third contribution entails a network-based method that leverages the structure of the hypergraph to find communities of words using contexts independently. These groups are found based on the intuition that words tend to group together around a single hub word which represents, broadly, the general semantic topic of these words.


In order to evaluate the methods and intuitions proposed, we performed experiments on two semantic tasks: WSI/WSD and NER.

Concerning WSI/WSD, we tested our hypergraph-based model over the Semeval 2007 corpus. Using the free-scale presumption we found communities of words describing senses by using sentence-level lexical contexts and raw frequencies to weight the co-occurrences. Jaccard similarity was chosen to measure the relatedness among words. These parameters were defined experimentally. Also, we found that contrary to what we expected initially, the contexts defined by syntactic-based co-occurrences perform worse than lexical contexts. Additionally, we analyzed the differences of the two contexts in terms of performance for each word in the Semeval 2007 test corpus. In general, it seems that verbs are better off with syntactical contexts while nouns are best represented by lexical contexts. 

With regard to our fusion technique method, we tested it over both WSI/WSD and NER tasks. We created new representation matrices that showed improvement in performance. In order to get to these improvements, which are consistent in the whole ensemble of datasets tested, we performed a high level of fusion aggregation. Not unlike relevant literature. Once again, lexical, and ad-hoc, features, proved more useful that syntactic features. We estimate this is due to the fact that syntactic features require larger corpora to actually populate the relations between words using dependency functions. Our experiments show that reducing the sparsity by combining heterogeneous features can ameliorate over using independent features and over the trivial feature concatenation. For all our experiments, while our results can be regarded as "baseline" performances, we do stay in the same ball-park of similar task-tailored methods.
% We note that in contrast to NER, in WSI/WSD the best systems consisted of single feature enriched spaces. We argue that the senses are correlated 





\section{Future Work}
The work we present still has several research paths to be explored.
The hypergraph model itself could utilize different contexts, going further than syntactic or lexical contexts, for example using morphological or even phonological contexts for words or other utterances. Even more, the constituent-based contexts are surely open for improvement: trying more literature approaches or devising intelligent ways to leverage the information provided by this syntactic parse.

%The choice of weights and similarity measures is also open for exploration. While in my research experience, no great improvement comes from changing from one to another, it is still interesting to understand in which task what parameters are the most appropriate. This is still an open research question in the domain.

Concerning fusion techniques, a more principled way to determine what type of context with what type of fusion operation would indeed reduce the need for exploring the whole space of possibilities. Finally,  comparing said methods with other well-established dimension reduction approaches would be interesting to understand the trade-offs of lower performance versus dimension reduction, while focusing on not-so-large corpora. Indeed, if the new wave of distributional representations, or word embeddings, has a shortcoming is that empirically it does not perform as well on smaller corpus. This may represent an avenue of opportunity to methods such as feature fusion functions.


Regarding the network-based algorithm for WSI/WSD, a deeper errors' analysis would deep a larger glimpse on the behavior of nouns and verbs according to the context. Understanding what is the syntactic or lexical difference among contexts, which induce the good or bad performance of each type of feature could make the system more flexible to other text domains. Also, the hypergraph could be better leveraged by using hypergraph-specific methods, mainly through spectral analysis.

Lastly, we built a enriched hypergraph resource based on a very large corpus such as Wikipedia. The relations between words contained within could be leveraged to generate more powerful representations as the one created in this work. Indeed, we explored this avenue but was put aside given the size of a matrix extracted from a very large corpus. Furthermore, a comparison with other distributional representations may signal other advantages of our proposed model. Specially for smaller corpora. 

