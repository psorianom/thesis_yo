\chapter{Conclusions and Future Work}
\label{chap:conclusions}
%\minitoc
\section{Conclusion}
Linguistic Networks are useful methods to understand the nature of our language. In the literature, they are generally used to comprehend either the dynamics of words and other textual units within language, and to solve practical NLP tasks. Nonetheless, no mater the objective, they are usually based on the distributional hypothesis, that is, words will be found in similar contexts if they tend to be semantically related.

Distributional models are based on several parameters, such as the size of the context to be used, their linguistic type (either syntactic, lexical, etc.), the weight that affects each context-word co-occurrence, as well as determining how the semantic relatedness is computed. Indeed, most of the linguistic networks in the literature deal with a single type of contexts, either lexical or syntactical.

On the other hand, text data representations, described through contexts in a distributional framework, are sparse by nature: the large majority of the entries in a co-occurrence matrix are zero. This translates to greatly sparse features' matrices which represent problems for knowledge discovery methods as they do not have much information about words because each one of them is has a very low number of features. We considered the lack of heterogeneity and data sparsity two open challenges in textual representations.

To treat these concerns, on this thesis we proposed three contributions. The first and second entail a fusion enriched linguistic network,  which entails denser text representations by combining heterogeneous feature spaces. The second is a method based on graph structure to find groups of related words.

The linguistic model we proposed unifies language networks by means of a hypergraph structure. We consider three different types of co-occurrence contexts in order to represent three distinct levels of semantic relatedness. These contexts are based on lexical and syntactic co-occurrences, which are unified with a hypergraph. This union yields words that are related by any of the three contexts and thus creating more links among words. 

These heterogeneous features are important as they represent relations between words from different points of view. Nonetheless, they tend to generate sparse representations, as is common with text representations. In this sense, we proposed  as second contribution the use of fusion techniques to combine these features while reducing the sparsity of the representation space. 

Lastly, the third contribution entails a network-based method that leverages the structure of the hypergraph to find communities of words using contexts independently. These groups are found based on the intuition that words tend to group together around a single hub word which represents, broadly, the general semantic topic of these words.


In order to evaluate the methods and intuitions proposed, we performed experiments on two semantic tasks: WSI/WSD and NER.



With regard to our fusion techniques, we tested them over both WSI/WSD and NER tasks. Particularly, in NER, we created new representation matrices that showed overall improvement in performance. In order to get to these improvements, which are consistent in the whole ensemble of datasets tested, we consistently performed a high level of fusion aggregation. Once again, lexical and task-standard features, proved more useful that syntactic features. We estimate this is due to the fact that syntactic features require larger corpora to actually populate the relations between words using dependency functions. Our experiments show that reducing the sparsity by combining heterogeneous features can ameliorate over using independent features and over the trivial feature concatenation. For all our experiments, while our results can be regarded as "baseline" performances, we do stay in the same ball-park of similar task-tailored methods.
% We note that in contrast to NER, in WSI/WSD the best systems consisted of single feature enriched spaces. We argue that the senses are correlated 

Concerning our graph-based model, we tested it on the WSI/WSD task, over the Semeval 2007 corpus. Using the free-scale presumption we found communities of words describing senses by using sentence-level lexical contexts and raw frequencies to weight the co-occurrences. Jaccard similarity was chosen to measure the relatedness among words. These parameters were defined experimentally. Also, we found that contrary to what we expected initially, the contexts defined by syntactic-based co-occurrences perform worse than lexical contexts. The fusion operators produced representation spaces that improved over using single features, as in NER experiments. Nonetheless, the heterogeneity aspect of our proposed linguistic network is not particularly leveraged by the proposed method. Heterogeneous networks seem not to allow the retrieval of important hubs that represent senses. Finally, we analyzed the differences of the two contexts in terms of performance for each word in the Semeval 2007 test corpus. In general, it seems that verbs are better off with syntactical contexts while nouns are best represented by lexical contexts. 

Finally, the proposed hypergraph, through its fusion representations, generate large matrices that need to be correctly manipulated in order to solve NLP tasks. To address this challenge, we use simple solutions as simple as word filtering to more complex approaches that computationally deal with large, sparse, and dense, spaces, such as parallelization and out-of-core computing methods\footnote{Algorithms that only keep in memory the required parts of a matrix during computations, keeping the rest on the hard drive}. Several other techniques may be used, and were tried, such as dimension reduction via random projections or hash-valued  representation spaces (commonly known as the hash trick). The downside is, in our experience, a considerable loss in performance.


\section{Future Work}
The work we present still has several research paths to be explored.
The hypergraph model itself could utilize different contexts, going further than syntactic or lexical contexts, for example using morphological or even phonological contexts for words or other utterances. Even more, the constituent-based contexts are surely open for improvement: trying more literature approaches or devising intelligent ways to leverage the information provided by this syntactic parse.  Regarding the computationally implemented resource, which follows the proposed model guidelines, it would be interesting to leverage the information within by means of key-valued queries. The extracted information could be helpful to discover, for example, which and how many nouns participate in adjective modifier dependency relation, or exploring what are the must recurring type of noun phrases in corpus to better adequate a NLP system.

%The choice of weights and similarity measures is also open for exploration. While in my research experience, no great improvement comes from changing from one to another, it is still interesting to understand in which task what parameters are the most appropriate. This is still an open research question in the domain.

Concerning fusion techniques, a more principled way to determine what type of context with what type of fusion operation would indeed reduce the need for exploring the whole space of possibilities. In the same sense, in the NER experiments, finding which fusion operators work best for each of the classes tested (location, organization, person, miscellaneous) in order to better exploit them in a single fusion operator. On the other hand, for WSI/WSD experiments, a more in-deep exploration of the senses found by each fusion operator may give us a layered overview of the textual properties according to each type of feature employed. Coupled with this exploration, a larger analysis on the contributions of each one of the parameters used in the fusion operators ($\alpha, \beta, \gamma$) would allow us to properly weight each feature type to each experimental setting.

Comparing fusion methods with other well-established dimension reduction approaches would be interesting to understand the trade-offs of lower performance versus dimension reduction, while focusing on not-so-large corpora. Indeed, if the new wave of distributional representations, or word embeddings, has a shortcoming is that empirically it does not perform as well on smaller corpus. This may represent an avenue of opportunity to methods such as feature fusion functions.


Regarding the network-based algorithm for WSI/WSD, a deeper errors' analysis would deep a larger glimpse on the behavior of nouns and verbs according to the context. Understanding what is the syntactic or lexical difference among contexts, which induce the good or bad performance of each type of feature could make the system more flexible to other text domains. Also, the hypergraph could be better leveraged by using hypergraph-specific methods, mainly through spectral analysis.

Lastly, we built a enriched hypergraph resource based on a very large corpus such as Wikipedia. The relations between words contained within could be leveraged to generate more powerful representations as the one created in this work. Indeed, we explored this avenue but was put aside given the size of a matrix extracted from a very large corpus. Furthermore, a comparison with other distributional representations may signal other advantages of our proposed model. Specially for smaller corpora. 

