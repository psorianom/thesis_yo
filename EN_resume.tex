\documentclass[a4paper,11pt,twoside]{article}
\include{formatAndDefs_FR}

\begin{document}
%\chapter*{Résumé}
\section{Introduction}
\subsection{Contexte}

Making sense of texts plays a vital role on the evolution of general artificial intelligence. Given the constantly-growing generation of textual data, there is the need of computational systems able to extract useful information from large quantities of textual collections  to facilitate our day-to-day activities to find useful latent information hidden behind these large quantities of data. 
Indeed, making computers learn is the general objective of artificial intelligence research \cite{Sugiyama2015}. Coming from this multi-disciplinary area, Natural Language Processing (NLP) is the domain that aims to make machines understand our language \cite{JurafskyM09}.

Solutions to NLP tasks generally follow three steps to achieve their respective goals \cite{mining12Book,JurafskyM09}. These steps are: first, in preprocessing, an input corpus is "normalized" so that it will be easier to treat by the machine in the subsequent steps. Secondly, in feature representation, numerous features are extracted from the preprocessed text. Thirdly, in {Knowledge Discovery},  a machine learning technique is used to  learn a model able to provide an interesting insight within the existing data as well as on new future instances. The output of said system is usually the model or the language knowledge that reveals an interesting piece of information contained in the input corpus. 


\subsection{Problématiques et contributions}
There are several research challenges that arise from the choices taken in each one of the steps comprising the NLP system's flow.  In this thesis, we focus on three challenges arising in both the feature representation and knowledge discovery phases. These challenges are: (1) modeling, extracting, and storing different types of linguistic features from raw text, (2) dealing with the sparsity inherent to text data features and also successfully combining them to get better representations, and (3)  finding	 relations  between words and then leveraging them in order to discover their latent relatedness and be able to solve NLP tasks.


In order to address said challenges, we propose three contributions:
\begin{itemize}
\item a hypergraph network-based model to hold heterogeneous linguistic data %as well as a tool to extract said data from unstructured text.
\item a method to combine heterogeneous representations coming from the hypergraph model, while at the same time alleviating the sparsity problem, common while dealing with text features.
\item a network-based algorithm to discover semantic relatedness between linked words.
\end{itemize}


These contributions are tested and evaluated using two different NLP semantic tasks: Word Sense Induction and Disambiguation, and Named Entity Recognition. We attack these tasks as they are central building blocks of more intricate text analysis systems.

\section{Background}

\subsection{Distributional Hypothesis}
The work we present in this thesis is prominently based on the distributional hypothesis (DH). This is also the case for the large majority of semantic approaches in NLP today. This context-analysis insight is usually credited primarily to \cite{harris1954}. The hypothesis is simple yet powerful:  it states that the similarity of meaning correlates with the similarity of the word's context distribution. It follows that the meaning of a word can be determined by the set of contexts in which it occurs in a corpus. 
%
In this work we will exclusively focus on those two contexts: lexical co-occurrence and syntactic co-occurrence contexts. Lexical co-occurrences consist on those  words that co-occur with a given word in a predetermined neighborhood. Syntactic contexts are based on the analysis (or parsing) of text in order to obtain sense from them.

\subsection{Representation Models}
 The Vector Space Model (VSM) consists in representing textual units in a multi-dimensional space. The textual units represented are not constrained to words themselves. We may describe co-occurrent features for documents, phrases, paragraphs, or other types \cite{manning1999foundations}. A matrix is used as the structure that holds each object and its context features. Distance metrics (or similarity measures) are used over the vectors of the matrix to determine a level of dissimilarity or similarity between these objects.
 Other type of representation models, based on graph structures are commonly used in the literature. Indeed, network based models have been studied deeply during the last years in the NLP field  \cite{Mihalcea2011}. While we can represent a graph as a matrix, and thus as a vector space model, graphs are useful representation formalism that can be applied to a large set of linguistic characteristics, from the relation between words in a text or between the features that describe them. Indeed, language being a dynamic complex system, networks provide an adequate model to represent and study the structure and evolution of linguistic systems \cite{Choudhury2009}.
In this thesis we base our linguistic model proposition on a graph-based structure.

\subsection{Data Sparsity}
Without regards to their type, network-based structures are ultimately transformed into matrices before being treated computationally. Therefore, given that we are still modeling language (words), graphs suffer from sparsity just as vector space models. Indeed, data sparsity is an issue that affects the performance of knowledge discovery approaches \cite{mining12Book,PerinetH15} applied to textual data.

 A sparse data matrix has most of its entries equal to zero. Thus, the majority of the words (rows) in the corpus are described by very few contexts (columns). This is a significant problem as on the knowledge discovery phase of any NLP system we aim to train a learning model that will eventually predict, classify, group our words in one way ot another. If the words are represented by a limited number of contexts, the learning algorithms will not be able to generalize properly. 
%\subsection{Conclusion}

Whether its vector based or graph-based, a textual, explicit, and distributional representation will be sparse. There are too many words in a text and its assured that, while they could occur in other texts, they will not occur in a single text. This becomes an important problem with NLP systems: words are described by only a small set of features. In the following describe our two first propositions which address the issue of using heterogeneous information to represent a term and alleviating the data sparsity that comes with such types of textual representations.


\section{Modèle linguistique basé sur des hypergraphs enrichi par la fusion}
The first two contributions of this thesis are contained in a fusion enriched hypergraph linguistic model proposition. The model  consists on two components which address two research questions each: the issue of making sparsity less severe and leveraging different types of features  by using a single feature representation space. 

The model we present here entails three  important characteristics: firstly, the possibility to leverage different types of information.  Secondly, as the words will be linked together, there is an inner structure that will emerge from the model and which we exploit in our experiments. Thirdly, given that we treat unstructured text data, the relations (or features)  between words are sparse, this is alleviated by combining features via fusion techniques.  The three of them are addressed with our propositions. 

Our network is based on the distributional hypothesis, as described in the previous chapter.  As co-occurrence features, we select both lexical and syntactic contexts, indeed creating a linguistic resource that hold both types of information in order to get a complementary insight of words' relations. 


In the literature, regarding WSD approaches, we see that the use of a lexical knowledge base, such as Wordnet\footnote{\url{https://wordnet.princeton.edu}}, is pervasive in this task. On the other hand, WSI, while being a more flexible approach (language and word-usage independent, does not require human-made bases)  for solving WSD, its results are tightly linked to the quality of the clustering algorithm used. 
% 
%We refer to  linguistic-network modelization as the type of linguistic information and the means in which it is stored within a language network.

 With respect to the networks' modelization, we find that few approaches deal with syntactic attributes. We believe that finding semantic similarities can be improved by adding syntactic information not only  while using dependency relations but also by leveraging the constituency tree of each word. Moreover, using syntactic data along with semantic and/or lexical co-occurrences takes us into the heterogeneous network domain which has not been addressed in most of the approaches covered.  


Taking into account the described opportunities of research, in the following section we propose a  hypergraph modelization of a linguistic network that aims to solve some limitations stated above. 



\subsection{Proposed Model: Fusion Enriched Hypergraph Linguistic Network}

As stated before, our model consists on two parts (and two contributions). The first one, an hypergraph model that holds different types of linguistic relations extracted from a corpus. And the second one, the combination of linguistic features in order to generate a less sparse, enriched representation. 

Our model is  based on the use of a hypergraph. Its single most important difference with regular graphs, is being able to relate more than two vertices at the same type, which allows for a better characterization of interactions within a set of individual elements (in our case, words) \cite{heintz2014beyond}. Indeed, our hypergraph modelization initially integrates four types of relations between tokens: sentence co-occurrence, part-of-speech tags, words' constituents data and dependency relations in a single linguistic structure. These relationships were chosen because it is relatively easy to obtain them for high-resource languages. These features can be seen as building blocks for NLP models. In any case, our goal is to arrive to more complex annotations (e.g., named entities) from the selected features and relations. We decided to keep a lexical context at sentence level, so that it may complement the distributional semantic information provided by the dependency functions context as well as the phrase-constituency syntactic context. In short, we  aim to cover three levels of possible semantic relatedness via  three levels (in terms of the size of the neighborhood of a target word) of distributional co-occurrences: a short range with dependency functions, a medium range with phrase constituency membership, and a longer range with sentence lexical co-occurrences. The intuition is that when solving NLP tasks, having direct access to these three semantic spaces will help to determine a more appropriate meaning's relation between words. 

The second part of our proposed method deals with the fusion of textual features. Namely, we combine the features that describe terms into a single representation space. This new space aims to address two issues that arise while working with textual data: effectively using information coming from different linguistic levels (e.g., lexical, syntactic, semantic) while alleviating the sparsity typical of textual representations. In the multimodal fusion literature we can discern two main common types of techniques: early fusion and late fusion. A third and fourth type of fusion methods, cross-media fusion and hybrid fusion are also employed in multimedia analysis tasks. 

These four fusion operators naturally address the issue of dealing with heterogeneous data as they all mix one way or another the feature columns from each of two representations. Regarding alleviating sparsity, the intuition is that by combining matrices either by summing or element-wise multiplying them, the resulting matrix will have a denser structure. For example, by summing two matrices with the same shape, such as two term-term similarity matrices, we  obtain a resulting matrix that contains the similarities of both feature spaces. In the same sense, when multiplying two matrices we combine them while also obtaining a denser output matrix. Nonetheless, both sum and multiplication result depends evidently on the nature of the matrices employed. 
%\subsection{Proof of Concept: Wikipedia-based Corpus as an Enriched Hypergraph}

In order to materialize our proposed linguistic model we implemented a procedure that takes a corpus as input and outputs the linguistic resource we introduced in the previous section. We based our process on the online encyclopedia Wikipedia\footnote{\url{https://en.wikipedia.org}} which has been used as a source of valuable data as well as a common background corpus to solve diverse NLP/TM related tasks. 

%chap 4
\section{Applications à la reconnaissance d'entités nommées et à la désambiguïsation du sens du mot}
\subsection{Introduction}
In this application's chapter we set to solve two natural language processing tasks using as data source corpora in the form of our proposed model. We address the tasks of Named Entity Recognition (NER) and Word Sense Induction and Disambiguation (WSI/WSD). We employ both a fusion enriched and a raw hypergraph network based on benchmark corpora to validate the utility of our proposals.
\subsection{First Application: Named Entity Recognition and Word Sense Disambiguation}
 NER goal is to automatically discover, within a text, mentions that belong to a well-defined semantic category. The classic task of NER involves detecting, within a text, entities of type Location (LOC), Organization (ORG), Person (PER), Miscellaneous (MISC), or if the term is not an even an entity, assigning them a (O) label. The task is of great importance for more complex NLP systems, e.g, relation extraction, opinion mining \cite{nadeau2007survey}.  Generally, the common solution to NER involves training a supervised machine learning algorithm with large quantities of annotated text \cite{mining12Book}. As is usual with other NLP tasks, NER  requires textual features to represent words in order to determine their role within a phrase. We propose to build representations based on our fusion enriched hypergraph model. For the main experiments, we chose a structured perceptron learning algorithm because of its performance and its lower training time. 

We experiment with the four levels of fusion on three different datasets. The representation matrices for NER come from lexical context features $\mlex$, syntactical context features or other task-standard features.  On the other hand, experiments on WSI/WSD exclusively employ lexical and syntactic matrices. Our main goal is to compare the efficiency of the primary  fusion techniques applied to   named entity recognition. Then, we empirically determine a fusion combination operator able to leverage the complementarity of the features used.
We discover that using a recombination of several fusion operations, a so-called hybrid approach, we  improve over using the features individually and also over the use of the trivial early fusion operator. This indicates that the single feature matrix, enriched with other combined features, is enough to improve the results of said baselines. In general, in our experiments with NER, we see that the added enriched features are not the most important for the learning algorithm while taking a decision, nonetheless, they  provide the extra information needed to push the model towards the correct prediction, by enriching the features through cross and late fusion and by providing more descriptors for each word and consequently reducing the sparsity of the representation matrices.

Once we found a set of fusion operations that work reasonably well with NER, we experiment with another task, word sense induction and disambiguation, to confirm the usefulness of using fusion enriched representations to train better models.
\subsection{Second Application: Word Sense Induction and Disambiguation}
\subsubsection{Fusion Enriched Representations}
Having learned the best fusion configuration from the NER task, in these experiments we set to test if the improvements achieved can be transfered into another NLP task, namely Word Sensed Induction and Disambiguation (WSI/WSD). As preprocessing, we simply remove stopwords and tokens with less than three letters. The features we extracted from the tested corpora with the same tools as in the previous task.
Word Sense Induction and Disambiguation entails two closely related tasks. WSI aims to automatically discover the set of possible senses for a target word given a text corpus containing several occurrences of said target word. Meanwhile, WSD takes a set of possible senses and determines the most appropriate sense for each instance of the target word according to the instance's context. WSI is usually approached as an unsupervised learning task, i.e., a cluster method is applied to the words occurring in the instances of a target word. The groups found are interpreted as the senses of the target word. The WSD task is usually solved with knowledge-based approaches, or more recently, with supervised models which require annotated data. It can be also solved reasonably well by comparing the words surrounding each target word and the words belonging to the induced senses (or clusters) found during the WSI step, as we do in this section.

We employ spectral clustering on the input matrices in order to automatically discover senses (a cluster is considered a sense).  Regarding sense disambiguation, we trivially assign senses to the target word instances according to the number of common words in each cluster and the context words of the target word. In other words, for each test instance of a target word, we select the cluster (sense) with the maximum number of shared words with the current instance context.

In order to determine the best performing operations, that stray away from the trivial baselines, we propose a measure to help us identify those systems that perform the best. According to this metric, we find the combination of fusion operators is the best performing combination of features. Indeed, by transferring quality lexical similarities into its same feature matrix, we obtain more useful relations than by using any syntactic data. While the same operators that outperformed the rest in NER are not as adequate in this WSI/WSD experiment, we see that most of the feature combination techniques improve over the baselines of the single features and early fusion operations. 
\subsubsection{Leveraging the Linguistic Network Structure}

We leverage the relations that exist within the network to identify words that, together with their neighborhood, represent a sense. Thus, we propose a network-based algorithm to solve word sense induction. 
Our method  is inspired on previous approaches from both \cite{2004.Veronis} and \cite{2007.Klapaftis.UoY}. In Hyperlex,  the graph-based  method presented  in \cite{2004.Veronis}, the main intuition is that co-occurrence networks have small-world properties and thus it is possible to detect and isolate important heavily-connected nodes, called "hubs". The idea is that these hubs, and their connected nodes, represent a sense themselves. 
We apply our method to a well-known WSI/WSD dataset. We also use feature spaces produced with fusion operators.

Using our network-based approach, we obtained results that surpassed those from the similar methods while being more flexible in terms of use of parameters. Again, we found that using fusion operators yield better results compared to using single features or early fusion. While our fusion systems beat these baselines, the systems that perform the best do not employ heterogeneous data to do so. Indeed, the best systems that combine the two possible types of features lag behind the best fusion spaces. 

% chap 5
\section{Conclusions et travaux futurs}
\subsection{Conclusion}
Linguistic Networks are useful methods to understand the nature of our language. In the literature, they are generally used to comprehend either the dynamics of words and other textual units within language, and to solve practical NLP tasks. Nonetheless, no mater the objective, they are usually based on the distributional hypothesis, that is, words will be found in similar contexts if they tend to be semantically related. On the other hand, text data representations, described through contexts in a distributional framework, are sparse by nature: the large majority of the entries in a co-occurrence matrix are zero. To treat these concerns, on this thesis we proposed three contributions. The first and second entail a fusion enriched linguistic network,  which entails denser text representations by combining heterogeneous feature spaces. The second is a method based on graph structure to find groups of related words.

With regard to our fusion techniques, we tested them over both WSI/WSD and NER tasks. Particularly, in NER, we created new representation matrices that showed overall improvement in performance. In order to get to these improvements, which are consistent in the whole ensemble of datasets tested, we performed a high level of fusion aggregation. Concerning our graph-based model, we tested it on the WSI/WSD task, over the Semeval 2007 corpus. Using the free-scale presumption we found communities of words describing senses by using sentence-level lexical contexts and raw frequencies to weight the co-occurrences. The fusion operators produced representation spaces that improved over using single features, as in NER experiments.

Finally, the proposed hypergraph, through its fusion representations, generate large matrices that need to be correctly manipulated in order to solve NLP tasks. To address this challenge, we use simple solutions as simple as word filtering to more complex approaches that computationally deal with large, sparse, and dense, spaces, such as parallelization and out-of-core computing methods\footnote{Algorithms that only keep in memory the required parts of a matrix during computations, keeping the rest on the hard drive}.
\subsection{Travaux futurs}
Concerning fusion techniques, a more principled way to determine what type of context with what type of fusion operation would indeed reduce the need for exploring the whole space of possibilities. Finally,  comparing said methods with other well-established dimension reduction approaches would be interesting to understand the trade-offs of lower performance versus dimension reduction, while focusing on not-so-large corpora. Indeed, if the new wave of distributional representations, or word embeddings, has a shortcoming is that empirically it does not perform as well on smaller corpus. This may represent an avenue of opportunity to methods such as feature fusion functions.

Regarding the network-based algorithm for WSI/WSD, a deeper errors' analysis would deep a larger glimpse on the behavior of nouns and verbs according to the context. Understanding what is the syntactic or lexical difference among contexts, which induce the good or bad performance of each type of feature could make the system more flexible to other text domains. Also, the hypergraph could be better leveraged by using hypergraph-specific methods, mainly through spectral analysis.
%Fusion Enriched Hypergraph Linguistic Model
\bibliographystyle{ThesisStyle}
\bibliography{Thesis_f}

\end{document}